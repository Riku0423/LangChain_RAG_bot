
Dify取扱説明書
Welcome to Dify!
Dify is an open-source large language model (LLM) application development platform. It combines the concepts of Backend-as-a-Service and LLMOps to enable developers to quickly build production-grade generative AI applications. Even non-technical personnel can participate in the definition and data operations of AI applications.
By integrating the key technology stacks required for building LLM applications, including support for hundreds of models, an intuitive Prompt orchestration interface, high-quality RAG engines, and a flexible Agent framework, while providing a set of easy-to-use interfaces and APIs, Dify saves developers a lot of time reinventing the wheel, allowing them to focus on innovation and business needs.
Why Use Dify?
You can think of libraries like LangChain as toolboxes with hammers, nails, etc. In comparison, Dify provides a more production-ready, complete solution - think of Dify as a scaffolding system with refined engineering design and software testing.
Importantly, Dify is open source, co-created by a professional full-time team and community. You can self-deploy capabilities similar to Assistants API and GPTs based on any model, maintaining full control over your data with flexible security, all on an easy-to-use interface.
Our community users summarize their evaluation of Dify's products as simple, restrained, and rapid iteration.
- Lu Yu, Dify.AI CEO
We hope the above information and this guide can help you understand this product. We believe Dify is made for you.
What Can Dify Do?
The name Dify comes from Define + Modify, referring to defining and continuously improving your AI applications. It's made for you.
Startups - Quickly turn your AI ideas into reality, accelerating both success and failure. In the real world, dozens of teams have already built MVPs to get funding or win customer orders through Dify.
Integrate LLMs into existing businesses - Enhance capabilities of current apps by introducing LLMs. Access Dify’s RESTful APIs to decouple Prompts from business logic. Use Dify’s management interface to track data, costs and usage while continuously improving performance.
Enterprise LLM infrastructure - Some banks and internet companies are deploying Dify as an internal LLM gateway, accelerating the adoption of GenAI technologies while enabling centralized governance.
Explore LLM capabilities - Even as a tech enthusiast, you can easily practice Prompt engineering and Agent technologies through Dify. Over 60,000 developers have built their first app on Dify even before GPTs came out.
Next Steps
Read Quick Start for an overview of Dify’s application building workflow.
Learn how to self-deploy Dify to your servers and integrate open source models.
Understand Dify’s specifications and roadmap.
Star us on GitHub and read our Contributor Guidelines.

Technical Spec
For those already familiar with LLM application tech stacks, this document serves as a shortcut to understand Dify's unique advantages
We adopt transparent policies around product specifications to ensure decisions are made based on complete understanding. Such transparency not only benefits your technical selection, but also promotes deeper comprehension within the community for active contributions.
Project Basics
Established
March 2023
Open Source License
Apache License 2.0 with commercial licensing
Official R&D Team
Over 10 full-time employees
Community Contributors
Over 120 people
Backend Technology
Python/Flask/PostgreSQL
Frontend Technology
Next.js
Codebase Size
Over 130,000 lines
Release Frequency
Average once per week

Technical Features
LLM Inference Engines
Dify Runtime (LangChain removed since v0.4)
Commercial Models Supported
10+, including OpenAI and Anthropic
Onboard new mainstream models within 48 hours
MaaS Vendor Supported
7, Hugging Face, Replicate, AWS Bedrock, NVIDIA, GroqCloud, together.ai,, OpenRouter
Local Model Inference Runtimes Supported
6, Xoribits (recommended), OpenLLM, LocalAI, ChatGLM,Ollama, NVIDIA TIS
OpenAI Interface Standard Model Integration Supported
∞
Multimodal Capabilities
ASR Models
Rich-text models up to GPT-4V specs
Built-in App Types
Text generation, Conversational, Agent, Workflow, Group(Q2 2024)
Prompt-as-a-Service Orchestration
Visual orchestration interface widely praised, modify Prompts and preview effects in one place.

Orchestration Modes
Simple orchestration
Assistant orchestration
Flow orchestration
Multi-Agent orchestration(Q2 2024)
Prompt Variable Types
String
Radio enum
External API
File (Q2 2024)
Agentic Workflow Features
Industry-leading visual workflow orchestration interface, live-editing node debugging, modular DSL, and native code runtime, designed for building more complex, reliable, and stable LLM applications.

Supported Nodes
LLM
Knowledge Retrieval
Question Classifier
IF/ELSE
CODE
Template
HTTP Request
Tool
RAG Features
Industry-first visual knowledge base management interface, supporting snippet previews and recall testing.
Indexing Methods
Keywords
Text vectors
LLM-assisted question-snippet model
Retrieval Methods
Keywords
Text similarity matching
Hybrid Search
N choose 1
Multi-path recall
Recall Optimization
Re-rank models
ETL Capabilities
Automated cleaning for TXT, Markdown, PDF, HTML, DOC, CSV formats. Unstructured service enables maximum support.
Sync Notion docs as knowledge bases.
Vector Databases Supported
Qdrant (recommended), Weaviate, Zilliz
Agent Technologies
ReAct, Function Call.

Tooling Support
Invoke OpenAI Plugin standard tools
Directly load OpenAPI Specification APIs as tools
Built-in Tools
30+ tools(As of Q1 2024)
Logging
Supported, annotations based on logs
Annotation Reply
Based on human-annotated Q&As, used for similarity-based replies. Exportable as data format for model fine-tuning.
Content Moderation
OpenAI Moderation or external APIs
Team Collaboration
Workspaces, multi-member management
API Specs
RESTful, most features covered
Deployment Methods
Docker, Helm

Model Providers
Dify supports the below model providers out-of-box:
Provider
LLM
Embedding
Rerank
OpenAI
✔️ (🛠️) (👓)
✔️


Anthropic
✔️




Azure OpenAI
✔️ (🛠️) (👓)
✔️


Google
✔️ (👓)




Cohere
✔️
✔️
✔️
Bedrock
✔️




together.ai
✔️




Ollama
✔️
✔️


Replicate
✔️
✔️


Hugging Face
✔️
✔️


Zhipu AI
✔️ (🛠️) (👓)
✔️


Baichuan
✔️
✔️


Spark
✔️




Minimax
✔️ (🛠️)
✔️


Tongyi
✔️




Wenxin
✔️




Jina


✔️
✔️
ChatGLM
✔️ (🛠️)




Xinference
✔️ (🛠️) (👓)
✔️
✔️
OpenLLM
✔️
✔️


LocalAI
✔️
✔️


OpenAI API-Compatible
✔️
✔️



where (🛠️) ︎ denotes Function Calling and (👓) denotes support for vision.

This table is continuously updated. We also keep track of model providers requested by community members here. If you'd like to see a model provider not listed above, please consider contributing by making a PR. To learn more, check out our Contributing Guide.

Using Dify Cloud
Note: Dify is currently in the Beta testing phase. If there are inconsistencies between the documentation and the product, please refer to the actual product experience.
Dify offers a cloud service for everyone, so you can use the full functionality of LangGenius without deploying it yourself. To use the cloud version of LangGenius, you need to have a GitHub or Google account.
Log in to Dify Cloud and create a new Workspace or join an existing one
Configure your model provider or use our hosted model provider
You can create an application now!
Currently, we don't have a pricing plan. If you like this LLMOps product, please introduce it to your friends😄.

Install (Self hosted)
The Dify Self hosted Edition, which is the open-source on GitHub, can be deployed in one of the following two ways:
Docker Compose Deployment
Local Source Code Start
Contributing
To ensure proper review, all code contributions - including those from contributors with direct commit access - must be submitted via pull requests and approved by the core development team prior to being merged.
We welcome all pull requests! If you'd like to help, check out the Contribution Guide for more information on how to get started.

Docker Compose Deployment
Prerequisites
Operating System
Software
Explanation
macOS 10.14 or later
Docker Desktop
Set the Docker virtual machine (VM) to use a minimum of 2 virtual CPUs (vCPUs) and 8 GB of initial memory. Otherwise, the installation may fail. For more information, please refer to the Docker Desktop installation guide for Mac.
Linux platforms
Docker 19.03 or later
Docker Compose 1.25.1 or later
Please refer to the Docker installation guide and the Docker Compose installation guide for more information on how to install Docker and Docker Compose, respectively.
Windows with WSL 2 enabled
Docker Desktop
We recommend storing the source code and other data that is bound to Linux containers in the Linux file system rather than the Windows file system. For more information, please refer to the Docker Desktop installation guide for using the WSL 2 backend on Windows.

Clone Dify
Clone the Dify source code to your local machine:
Copy
git clone https://github.com/langgenius/dify.git
Start Dify
Navigate to the docker directory in the Dify source code and execute the following command to start Dify:
Copy
cd dify/docker

docker compose up -d
If your system has Docker Compose V2 installed instead of V1, use docker compose instead of docker-compose. Check if this is the case by running $ docker compose version. Read more information here.
Deployment Results:
Copy
[+] Running 7/7

✔ Container docker-web-1       Started                                                                                                                                                                                       1.0s 

✔ Container docker-redis-1     Started                                                                                                                                                                                       1.1s 

✔ Container docker-weaviate-1  Started                                                                                                                                                                                       0.9s 

✔ Container docker-db-1        Started                                                                                                                                                                                       0.0s 

✔ Container docker-worker-1    Started                                                                                                                                                                                       0.7s 

✔ Container docker-api-1       Started                                                                                                                                                                                       0.8s 

✔ Container docker-nginx-1     Started
Finally, check if all containers are running successfully:
Copy
docker compose ps
This includes 3 business services: api / worker / web, and 4 underlying components: weaviate / db / redis / nginx.
Copy
NAME                IMAGE                              COMMAND                  SERVICE             CREATED             STATUS              PORTS

docker-api-1        langgenius/dify-api:0.3.2          "/entrypoint.sh"         api                 4 seconds ago       Up 2 seconds        80/tcp, 5001/tcp

docker-db-1         postgres:15-alpine                 "docker-entrypoint.s…"   db                  4 seconds ago       Up 2 seconds        0.0.0.0:5432->5432/tcp

docker-nginx-1      nginx:latest                       "/docker-entrypoint.…"   nginx               4 seconds ago       Up 2 seconds        0.0.0.0:80->80/tcp

docker-redis-1      redis:6-alpine                     "docker-entrypoint.s…"   redis               4 seconds ago       Up 3 seconds        6379/tcp

docker-weaviate-1   semitechnologies/weaviate:1.18.4   "/bin/weaviate --hos…"   weaviate            4 seconds ago       Up 3 seconds        

docker-web-1        langgenius/dify-web:0.3.2          "/entrypoint.sh"         web                 4 seconds ago       Up 3 seconds        80/tcp, 3000/tcp

docker-worker-1     langgenius/dify-api:0.3.2          "/entrypoint.sh"         worker              4 seconds ago       Up 2 seconds        80/tcp, 5001/tcp
Upgrade Dify
Enter the docker directory of the dify source code and execute the following commands:
Copy
cd dify/docker

git pull origin main

docker compose down

docker compose pull

docker compose up -d
Access Dify
Finally, access http://localhost/install to use the deployed Dify.

Local Source Code Start
Prerequisites
Operating System
Software
Explanation
macOS 10.14 or later
Docker Desktop
Set the Docker virtual machine (VM) to use a minimum of 2 virtual CPUs (vCPUs) and 8 GB of initial memory. Otherwise, the installation may fail. For more information, please refer to the Docker Desktop installation guide for Mac.
Linux platforms
Docker 19.03 or later
Docker Compose 1.25.1 or later
Please refer to the Docker installation guide and the Docker Compose installation guide for more information on how to install Docker and Docker Compose, respectively.
Windows with WSL 2 enabled
Docker Desktop


We recommend storing the source code and other data that is bound to Linux containers in the Linux file system rather than the Windows file system. For more information, please refer to the Docker Desktop installation guide for using the WSL 2 backend on Windows.

If you need to use OpenAI TTS, FFmpeg must be installed on the system for it to function properly. For more details, refer to: Link.
Clone Dify
Copy
git clone https://github.com/langgenius/dify.git
Before enabling business services, we need to first deploy PostgresSQL / Redis / Weaviate (if not locally available). We can start them with the following commands:
Copy
cd docker

docker compose -f docker-compose.middleware.yaml up -d

Server Deployment
API Interface Service
Worker Asynchronous Queue Consumption Service
Installation of the basic environment:
Server startup requires Python 3.10.x. It is recommended to use Anaconda for quick installation of the Python environment, which already includes the pip package management tool.
To create a Python 3.10 environment named "dify," you can use the following command:
Copy
conda create --name dify python=3.10
To switch to the "dify" Python environment, use the following command:
Copy
conda activate dify
Follow these steps :
Navigate to the "api" directory:
Copy
cd api
Copy the environment variable configuration file:
Copy
cp .env.example .env
Generate a random secret key and replace the value of SECRET_KEY in the .env file:
Copy
openssl rand -base64 42


sed -i 's/SECRET_KEY=.*/SECRET_KEY=<your_value>/' .env
Install the required dependencies:
Copy
pip install -r requirements.txt
Perform the database migration
Perform database migration to the latest version:
Copy
flask db upgrade
Start the API server:
Copy
flask run --host 0.0.0.0 --port=5001 --debug
output：
Copy
* Debug mode: on


INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.


* Running on all addresses (0.0.0.0)


* Running on http://127.0.0.1:5001


INFO:werkzeug:Press CTRL+C to quit


INFO:werkzeug: * Restarting with stat


WARNING:werkzeug: * Debugger is active!


INFO:werkzeug: * Debugger PIN: 695-801-919
start the Worker service
To consume asynchronous tasks from the queue, such as dataset file import and dataset document updates, follow these steps to start the Worker service on Linux or macOS:
Copy
celery -A app.celery worker -P gevent -c 1 -Q dataset,generation,mail --loglevel INFO
If you are using a Windows system to start the Worker service, please use the following command instead:
Copy
celery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail --loglevel INFO
output:
Copy
-------------- celery@TAKATOST.lan v5.2.7 (dawn-chorus)


--- ***** ----- 


-- ******* ---- macOS-10.16-x86_64-i386-64bit 2023-07-31 12:58:08


- *** --- * --- 


- ** ---------- [config]


- ** ---------- .> app:         app:0x7fb568572a10


- ** ---------- .> transport:   redis://:**@localhost:6379/1


- ** ---------- .> results:     postgresql://postgres:**@localhost:5432/dify


- *** --- * --- .> concurrency: 1 (gevent)


-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)


--- ***** ----- 


-------------- [queues]


               .> dataset          exchange=dataset(direct) key=dataset


               .> generation       exchange=generation(direct) key=generation


               .> mail             exchange=mail(direct) key=mail





[tasks]


 . tasks.add_document_to_index_task.add_document_to_index_task


 . tasks.clean_dataset_task.clean_dataset_task


 . tasks.clean_document_task.clean_document_task


 . tasks.clean_notion_document_task.clean_notion_document_task


 . tasks.create_segment_to_index_task.create_segment_to_index_task


 . tasks.deal_dataset_vector_index_task.deal_dataset_vector_index_task


 . tasks.document_indexing_sync_task.document_indexing_sync_task


 . tasks.document_indexing_task.document_indexing_task


 . tasks.document_indexing_update_task.document_indexing_update_task


 . tasks.enable_segment_to_index_task.enable_segment_to_index_task


 . tasks.generate_conversation_summary_task.generate_conversation_summary_task


 . tasks.mail_invite_member_task.send_invite_member_mail_task


 . tasks.remove_document_from_index_task.remove_document_from_index_task


 . tasks.remove_segment_from_index_task.remove_segment_from_index_task


 . tasks.update_segment_index_task.update_segment_index_task


 . tasks.update_segment_keyword_index_task.update_segment_keyword_index_task





[2023-07-31 12:58:08,831: INFO/MainProcess] Connected to redis://:**@localhost:6379/1


[2023-07-31 12:58:08,840: INFO/MainProcess] mingle: searching for neighbors


[2023-07-31 12:58:09,873: INFO/MainProcess] mingle: all alone


[2023-07-31 12:58:09,886: INFO/MainProcess] pidbox: Connected to redis://:**@localhost:6379/1.


[2023-07-31 12:58:09,890: INFO/MainProcess] celery@TAKATOST.lan ready.

Deploy the frontend page
Start the web frontend client page service
Installation of the basic environment:
To start the web frontend service, you will need Node.js v18.x (LTS) and NPM version 8.x.x or Yarn.
Install NodeJS + NPM
Please visit https://nodejs.org/en/download and choose the installation package for your respective operating system that is v18.x or higher. It is recommended to download the stable version, which includes NPM by default.
Follow these steps :
Enter the web directory
Copy
cd web
Install the dependencies.
Copy
npm install
Configure the environment variables. Create a file named .env.local in the current directory and copy the contents from .env.example. Modify the values of these environment variables according to your requirements:
Copy
# For production release, change this to PRODUCTION


NEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT


# The deployment edition, SELF_HOSTED or CLOUD


NEXT_PUBLIC_EDITION=SELF_HOSTED


# The base URL of console application, refers to the Console base URL of WEB service if console domain is


# different from api or web app domain.


# example: http://cloud.dify.ai/console/api


NEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api


# The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from


# console or api domain.


# example: http://udify.app/api


NEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api





# SENTRY


NEXT_PUBLIC_SENTRY_DSN=


NEXT_PUBLIC_SENTRY_ORG=


NEXT_PUBLIC_SENTRY_PROJECT=
Build the code
Copy
npm run build
Start the web service:
Copy
npm run start


# or


yarn start


# or


pnpm start
After successful startup, the terminal will output the following information：
Copy
ready - started server on 0.0.0.0:3000, url: http://localhost:3000

warn  - You have enabled experimental feature (appDir) in next.config.js.

warn  - Experimental features are not covered by semver, and may cause unexpected or broken application behavior. Use at your own risk.

info  - Thank you for testing `appDir` please leave your feedback at https://nextjs.link/app-feedback
Access Dify
Finally, access http://127.0.0.1:3000 to use the locally deployed Dify.

Start the frontend Docker container separately
When developing the backend separately, you may only need to start the backend service from source code without building and launching the frontend locally. In this case, you can directly start the frontend service by pulling the Docker image and running the container. Here are the specific steps:
Pull the Docker image for the frontend service from DockerHub:
Copy
docker run -it -p 3000:3000 -e CONSOLE_URL=http://127.0.0.1:5001 -e APP_URL=http://127.0.0.1:5001 langgenius/dify-web:latest
Build Docker Image from Source Code
Build the frontend image：
Copy
cd web && docker build . -t dify-web
Start the frontend image
Copy
docker run -it -p 3000:3000 -e CONSOLE_URL=http://127.0.0.1:5001 -e APP_URL=http://127.0.0.1:5001 dify-web
When the console domain and web app domain are different, you can set the CONSOLE_URL and APP_URL separately.
To access it locally, you can visit http://127.0.0.1:3000.

Environments
Common Variables
CONSOLE_API_URL
The backend URL of the console API, used to concatenate the authorization callback. If empty, it is the same domain. Example: https://api.console.dify.ai
CONSOLE_WEB_URL
The front-end URL of the console web, used to concatenate some front-end addresses and for CORS configuration use. If empty, it is the same domain. Example: https://console.dify.ai
SERVICE_API_URL
Service API Url, used to display Service API Base Url to the front-end. If empty, it is the same domain. Example: https://api.dify.ai
APP_API_URL
WebApp API backend Url, used to declare the back-end URL for the front-end API. If empty, it is the same domain. Example: https://app.dify.ai
APP_WEB_URL
WebApp Url, used to display WebAPP API Base Url to the front-end. If empty, it is the same domain. Example: https://api.app.dify.ai
FILES_URL
File preview or download URL prefix, used to display the file preview or download URL to the front-end or as a multi-modal model input; In order to prevent others from forging, the image preview URL is signed and has a 5-minute expiration time.
Server
MODE
Startup mode, only available when starting with docker, not effective when starting from source code.
api
Start API Server.
worker
Start asynchronous queue worker.
DEBUG
Debug mode, default is false. It is recommended to turn on this configuration for local development to prevent some problems caused by monkey patch.
FLASK_DEBUG
Flask debug mode, it can output trace information at the interface when turned on, which is convenient for debugging.
SECRET_KEY
A key used to securely sign session cookies and encrypt sensitive information in the database.
This variable needs to be set when starting for the first time.
You can use openssl rand -base64 42 to generate a strong key.
DEPLOY_ENV
Deployment environment.
PRODUCTION (default)
Production environment.
TESTING
Testing environment. There will be a distinct color label on the front-end page, indicating that this environment is a testing environment.
LOG_LEVEL
Log output level, default is INFO.
It is recommended to set it to ERROR for production.
MIGRATION_ENABLED
When set to true, the database migration will be automatically executed when the container starts, only available when starting with docker, not effective when starting from source code.
You need to manually execute flask db upgrade in the api directory when starting from source code.
CHECK_UPDATE_URL
Whether to enable the version check policy. If set to false, https://updates.dify.ai will not be called for version check.
Since the version interface based on CloudFlare Worker cannot be directly accessed in China at present, setting this variable to empty can shield this interface call.
OPENAI_API_BASE
Used to change the OpenAI base address, default is https://api.openai.com/v1.
When OpenAI cannot be accessed in China, replace it with a domestic mirror address, or when a local model provides OpenAI compatible API, it can be replaced.
Container Startup Related Configuration
Only effective when starting with docker image or docker-compose.
DIFY_BIND_ADDRESS
API service binding address, default: 0.0.0.0, i.e., all addresses can be accessed.
DIFY_PORT
API service binding port number, default 5001.
SERVER_WORKER_AMOUNT
The number of API server workers, i.e., the number of gevent workers. Formula: number of cpu cores x 2 + 1
Reference: https://docs.gunicorn.org/en/stable/design.html#how-many-workers
SERVER_WORKER_CLASS
Defaults to gevent. If using windows, it can be switched to sync or solo.
GUNICORN_TIMEOUT
Request handling timeout. The default is 200, it is recommended to set it to 360 to support a longer sse connection time.
CELERY_WORKER_CLASS
Similar to SERVER_WORKER_CLASS. Default is gevent. If using windows, it can be switched to sync or solo.
CELERY_WORKER_AMOUNT
The number of Celery workers. The default is 1, and can be set as needed.
Database Configuration
The database uses PostgreSQL. Please use the public schema.
DB_USERNAME: username
DB_PASSWORD: password
DB_HOST: database host
DB_PORT: database port number, default is 5432
DB_DATABASE: database name
SQLALCHEMY_POOL_SIZE: The size of the database connection pool. The default is 30 connections, which can be appropriately increased.
SQLALCHEMY_POOL_RECYCLE: Database connection pool recycling time, the default is 3600 seconds.
SQLALCHEMY_ECHO: Whether to print SQL, default is false.
Redis Configuration
This Redis configuration is used for caching and for pub/sub during conversation.
REDIS_HOST: Redis host
REDIS_PORT: Redis port, default is 6379
REDIS_DB: Redis Database, default is 0. Please use a different Database from Session Redis and Celery Broker.
REDIS_USERNAME: Redis username, default is empty
REDIS_PASSWORD: Redis password, default is empty. It is strongly recommended to set a password.
REDIS_USE_SSL: Whether to use SSL protocol for connection, default is false
Celery Configuration
CELERY_BROKER_URL
Format as follows:
Copy
redis://<redis_username>:<redis_password>@<redis_host>:<redis_port>/<redis_database>
Example: redis://:difyai123456@redis:6379/1
BROKER_USE_SSL
If set to true, use SSL protocol for connection, default is false
CORS Configuration
Used to set the front-end cross-domain access policy.
CONSOLE_CORS_ALLOW_ORIGINS
Console CORS cross-domain policy, default is *, that is, all domains can access.
WEB_API_CORS_ALLOW_ORIGINS
WebAPP CORS cross-domain policy, default is *, that is, all domains can access.
File Storage Configuration
Used to store uploaded data set files, team/tenant encryption keys, and other files.
STORAGE_TYPE
Type of storage facility
local (default)
Local file storage, if this option is selected, the following STORAGE_LOCAL_PATH configuration needs to be set.
s3
S3 object storage, if this option is selected, the following S3_ prefixed configurations need to be set.
azure-blob
Azure Blob object storage, if this option is selected, the following AZURE_BLOB_ prefixed configurations need to be set.
STORAGE_LOCAL_PATH
Default is storage, that is, it is stored in the storage directory of the current directory.
If you are deploying with docker or docker-compose, be sure to mount the /app/api/storage directory in both containers to the same local directory, otherwise, you may encounter file not found errors.
S3_ENDPOINT: S3 endpoint address
S3_BUCKET_NAME: S3 bucket name
S3_ACCESS_KEY: S3 Access Key
S3_SECRET_KEY: S3 Secret Key
S3_REGION: S3 region information, such as: us-east-1
AZURE_BLOB_ACCOUNT_NAME: your-account-name eg, 'difyai'
AZURE_BLOB_ACCOUNT_KEY: your-account-key eg, 'difyai'
AZURE_BLOB_CONTAINER_NAME: your-container-name eg, 'difyai-container'
AZURE_BLOB_ACCOUNT_URL: 'https://<your_account_name>.blob.core.windows.net'
Vector Database Configuration
VECTOR_STORE
Available enumeration types include：
weaviate
qdrant
milvus
zilliz (share the same configuration as milvus)
pinecone (not yet open)
WEAVIATE_ENDPOINT
Weaviate endpoint address, such as: http://weaviate:8080.
WEAVIATE_API_KEY
The api-key credential used to connect to Weaviate.
WEAVIATE_BATCH_SIZE
The number of index Objects created in batches in Weaviate, default is 100.
Refer to this document: https://weaviate.io/developers/weaviate/manage-data/import#how-to-set-batch-parameters
WEAVIATE_GRPC_ENABLED
Whether to use the gRPC method to interact with Weaviate, performance will greatly increase when enabled, may not be usable locally, default is true.
QDRANT_URL
Qdrant endpoint address, such as: https://your-qdrant-cluster-url.qdrant.tech/
QDRANT_API_KEY
The api-key credential used to connect to Qdrant.
PINECONE_API_KEY
The api-key credential used to connect to Pinecone.
PINECONE_ENVIRONMENT
The environment where Pinecone is located, such as: us-east4-gcp
MILVUS_HOST
Milvus host configuration.
MILVUS_PORT
Milvus port configuration.
MILVUS_USER
Milvus user configuration, default is empty.
MILVUS_PASSWORD
Milvus password configuration, default is empty.
MILVUS_SECURE
Whether Milvus uses SSL connection, default is false.
Knowledge Configuration
UPLOAD_FILE_SIZE_LIMIT:
Upload file size limit, default 15M.
UPLOAD_FILE_BATCH_LIMIT
The maximum number of files that can be uploaded at a time, default 5.
ETL_TYPE
Available enumeration types include:
dify
Dify's proprietary file extraction scheme
Unstructured
Unstructured.io file extraction scheme
UNSTRUCTURED_API_URL
Unstructured API path, needs to be configured when ETL_TYPE is Unstructured.
For example: http://unstructured:8000/general/v0/general
Multi-modal Configuration
MULTIMODAL_SEND_IMAGE_FORMAT
The format of the image sent when the multi-modal model is input, the default is base64, optional url. The delay of the call in url mode will be lower than that in base64 mode. It is generally recommended to use the more compatible base64 mode. If configured as url, you need to configure FILES_URL as an externally accessible address so that the multi-modal model can access the image.
UPLOAD_IMAGE_FILE_SIZE_LIMIT
Upload image file size limit, default 10M.
Sentry Configuration
Used for application monitoring and error log tracking.
SENTRY_DSN
Sentry DSN address, default is empty, when empty, all monitoring information is not reported to Sentry.
SENTRY_TRACES_SAMPLE_RATE
The reporting ratio of Sentry events, if it is 0.01, it is 1%.
SENTRY_PROFILES_SAMPLE_RATE
The reporting ratio of Sentry profiles, if it is 0.01, it is 1%.
Notion Integration Configuration
Notion integration configuration variables can be obtained by applying for Notion integration: https://www.notion.so/my-integrations
NOTION_INTEGRATION_TYPE: Configure as "public" or "internal". Since Notion's OAuth redirect URL only supports HTTPS, if deploying locally, please use Notion's internal integration.
NOTION_CLIENT_SECRET: Notion OAuth client secret (used for public integration type)
NOTION_CLIENT_ID: OAuth client ID (used for public integration type)
NOTION_INTERNAL_SECRET: Notion internal integration secret. If the value of NOTION_INTEGRATION_TYPE is "internal", you need to configure this variable.
Mail related configuration
MAIL_TYPE
resend
MAIL_DEFAULT_SEND_FROM
The sender's email name, such as: no-reply no-reply@dify.ai, not mandatory.
RESEND_API_KEY
API-Key for the Resend email provider, can be obtained from API-Key.
smtp
SMTP_SERVER
SMTP server address
SMTP_PORT
SMTP server port number
SMTP_USERNAME
SMTP username
SMTP_PASSWORD
SMTP password
SMTP_USE_TLS
Whether to use TLS, default is false
MAIL_DEFAULT_SEND_FROM
The sender's email name, such as: no-reply no-reply@dify.ai, not mandatory.
Others
INVITE_EXPIRY_HOURS: Member invitation link valid time (hours), Default: 72.

Web Frontend
SENTRY_DSN
Sentry DSN address, default is empty, when empty, all monitoring information is not reported to Sentry.
Deprecated
CONSOLE_URL
⚠️ Modified in 0.3.8, will be deprecated in 0.4.9, replaced by: CONSOLE_API_URL and CONSOLE_WEB_URL.
Console URL, used to concatenate the authorization callback, console front-end address, and CORS configuration use. If empty, it is the same domain. Example: https://console.dify.ai.
API_URL
⚠️ Modified in 0.3.8, will be deprecated in 0.4.9, replaced by SERVICE_API_URL.
API URL, used to display Service API Base URL to the front-end. If empty, it is the same domain. Example: https://api.dify.ai
APP_URL
⚠️ Modified in 0.3.8, will be deprecated in 0.4.9, replaced by APP_API_URL and APP_WEB_URL.
WebApp Url, used to display WebAPP API Base Url to the front-end. If empty, it is the same domain. Example: https://api.app.dify.ai
Session Configuration
⚠️ This configuration is no longer valid since v0.3.24.
Only used by the API service for interface identity verification.
SESSION_TYPE：
Session component type
redis (default)
If you choose this, you need to set the environment variables starting with SESSION_REDIS_ below.
sqlalchemy
If you choose this, the current database connection will be used and the sessions table will be used to read and write session records.
SESSION_REDIS_HOST: Redis host
SESSION_REDIS_PORT: Redis port, default is 6379
SESSION_REDIS_DB: Redis Database, default is 0. Please use a different Database from Redis and Celery Broker.
SESSION_REDIS_USERNAME: Redis username, default is empty
SESSION_REDIS_PASSWORD: Redis password, default is empty. It is strongly recommended to set a password.
SESSION_REDIS_USE_SSL: Whether to use SSL protocol for connection, default is false
Cookie Policy Configuration
⚠️ This configuration is no longer valid since v0.3.24.
Used to set the browser policy for session cookies used for identity verification.
COOKIE_HTTPONLY
Cookie HttpOnly configuration, default is true.
COOKIE_SAMESITE
Cookie SameSite configuration, default is Lax.
COOKIE_SECURE
Cookie Secure configuration, default is false.
FAQ
1. How to reset the password if the local deployment initialization fails with an incorrect password?
If deployed using docker compose, you can execute the following command to reset the password: docker exec -it docker-api-1 flask reset-password Enter the account email and twice new passwords, and it will be reset.
2. How to resolve File not found error in the log when deploying locally?
Copy
ERROR:root:Unknown Error in completion

Traceback (most recent call last):

 File "/www/wwwroot/dify/dify/api/libs/rsa.py", line 45, in decrypt

   private_key = storage.load(filepath)

 File "/www/wwwroot/dify/dify/api/extensions/ext_storage.py", line 65, in load

   raise FileNotFoundError("File not found")

FileNotFoundError: File not found
This error may be caused by switching deployment methods, or deleting the api/storage/privkeys file, which is used to encrypt large model keys and can not be reversed if lost. You can reset the encryption public and private keys with the following command:
Docker compose deployment
Copy
docker exec -it docker-api-1 flask reset-encrypt-key-pair
Source code startup
Enter the api directory
Copy
flask reset-encrypt-key-pair
Follow the prompts to reset.
3. Unable to log in when installing later, and then login is successful but subsequent interfaces prompt 401?
This may be due to switching the domain name/website, causing cross-domain between front-end and server-side. Cross-domain and identity involve two configuration items:
CORS cross-domain configuration
CONSOLE_CORS_ALLOW_ORIGINS Console CORS cross-domain policy, default to *, which allows access from all domain names. WEB_API_CORS_ALLOW_ORIGINS WebAPP CORS cross-domain strategy, default to *, which allows access from all domain names.
4. After starting, the page keeps loading and checking the request prompts CORS error?
This may be because the domain name/URL has been switched, resulting in cross-domain between the front end and the back end. Please change all the following configuration items in docker-compose.yml to the new domain name: CONSOLE_API_URL: The backend URL of the console API. CONSOLE_WEB_URL: The front-end URL of the console web. SERVICE_API_URL: Service API Url APP_API_URL: WebApp API backend Url. APP_WEB_URL: WebApp Url.
For more information, please check out: Environments
5. How to upgrade version after deployment?
If you start up through images, please pull the latest images to complete the upgrade. If you start up through source code, please pull the latest code and then start up to complete the upgrade.
When deploying and updating local source code, you need to enter the API directory and execute the following command to migrate the database structure to the latest version:
flask db upgrade
6.How to configure the environment variables when use Notion import
Q: What is the Notion's Integration configuration address?
A: https://www.notion.so/my-integrations
Q: Which environment variables need to be configured？
A: Please set below configuration when doing the privatized deployment
NOTION_INTEGRATION_TYPE : The value should configrate as (public/internal). Since the Redirect address of Notion’s Oauth only supports https, if it is deployed locally, please use Notion’s internal integration
NOTION_CLIENT_SECRET : Notion OAuth client secret (userd for public integration type)
NOTION_CLIENT_ID : OAuth client ID (userd for public integration type)
NOTION_INTERNAL_SECRET : Notion Internal Integration Secret, If the value of NOTION_INTEGRATION_TYPE is internal ,you need to configure this variable.
7. How to change the name of the space in the local deployment version?
Modify in the tenants table in the database.
8. Where can I modify the domain name for accessing the application?
Find the configuration domain name APP_WEB_URL in docker_compose.yaml.
9. If database migration is required, what things need to be backed up?
The database, configured storage, and vector database data need to be backed up. If deployed in Docker Compose mode, all data content in the dify/docker/volumes directory can be directly backed up.
10. Why is Docker deploying Dify and starting OpenLLM locally using 127.0.0.1, but unable to access the local port?
127.0.0.1 is the internal address of the container, and the server address configured by Dify requires the host LAN IP address.
11. How to solve the size and quantity limitations for uploading knowledge documents in the local deployment version？
You can refer to the official website environment variable description document to configure:
Environments
12. How does the local deployment edition invite members through email?
Local deployment edition, members can be invited through email. After entering the email invitation, the page displays the invitation link, copies the invitation link, and forwards it to users. Your team members can open the link and log in to your space by setting a password through email login.
13. How to solve listen tcp4 0.0.0.0:80: bind: address already in use?
This is because the port is occupied. You can use the netstat -tunlp | grep 80 command to view the process that occupies the port, and then kill the process. For example, the apache and nginx processes occupy the port, you can use the service apache2 stop and service nginx stop commands to stop the process.
14. What to do if this error occurs in text-to-speech?
Copy
[openai] Error: ffmpeg is not installed
Since OpenAI TTS has implemented audio stream segmentation, ffmpeg needs to be installed for normal use when deploying the source code. Here are the detailed steps:
Windows:
Visit the FFmpeg official website and download the precompiled Windows shared library.
Download and unzip the FFmpeg folder, which will generate a folder similar to "ffmpeg-20200715-51db0a4-win64-static".
Move the unzipped folder to a location of your choice, for example, C:\Program Files.
Add the absolute path of the FFmpeg bin directory to the system environment variables.
Open the command prompt and enter "ffmpeg -version" to see if the FFmpeg version information is displayed, indicating successful installation.
Ubuntu:
Open the terminal.
Enter the following commands to install FFmpeg: sudo apt-get update, then enter sudo apt-get install ffmpeg.
Enter "ffmpeg -version" to check if it has been successfully installed.
CentOS:
First, you need to enable the EPEL repository. In the terminal, enter: sudo yum install epel-release
Then, enter: sudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm
Update the yum package, enter: sudo yum update
Finally, install FFmpeg, enter: sudo yum install ffmpeg ffmpeg-devel
Enter "ffmpeg -version" to check if it has been successfully installed.
Mac OS X:
Open the terminal.
If you haven't installed Homebrew yet, you can install it by entering the following command in the terminal: /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
Install FFmpeg with Homebrew, enter: brew install ffmpeg
Enter "ffmpeg -version" to check if it has been successfully installed.
15. Migrate Vector Database to Qdrant or Milvus
If you want to migrate the vector database from weaviate to qdrant or milvus, you need to migrate the data in the vector database. The following is the migration method:
Step:
If you are starting from local source code, modify the environment variable in the .env file to the vector database you want to migrate to. etc: VECTOR_STORE=qdrant
If you are starting from docker-compose, modify the environment variable in the docker-compose.yaml file to the vector database you want to migrate to, both api and worker are all needed. etc:
Copy
# The type of vector store to use. Supported values are `weaviate`, `qdrant`, `milvus`.

VECTOR_STORE: weaviate
run the below command in your terminal or docker container
Copy
flask vdb-migrarte # or docker exec -it docker-api-1 flask vdb-migrarte
16. Why is SYS_ADMIN permission needed?
Why does the sandbox service need SYS_ADMIN permission?
The sandbox service is based on Seccomp for sandbox isolation, but also, Docker is based on Seccomp for resource isolation. In Docker, Linux Seccomp BPF is disabled by default, which prevents the use of Seccomp in containers, so SYS_ADMIN permission is required to enable Seccomp.
How does the sandbox service ensure security?
As for the security of the sandbox service, we disabled all file system, network, IPC, PID, user, mount, UTS, and system access capabilities of all processes in the sandbox to ensure that malicious code is not executed. At the same time, we also isolate the files and network in the container to ensure that even if the code is executed, it cannot harm the system.

What is LLMOps?
LLMOps (Large Language Model Operations) is a comprehensive set of practices and processes that cover the development, deployment, maintenance, and optimization of large language models (such as the GPT series). The goal of LLMOps is to ensure the efficient, scalable, and secure use of these powerful AI models to build and run real-world applications. It involves aspects such as model training, deployment, monitoring, updating, security, and compliance.
The table below illustrates the differences in various stages of AI application development before and after using Dify:
Steps
Before
After
Save time
Developing Frontend & Backend for Applications
Integrating and encapsulating LLM capabilities requires a lot of time to develop front-end applications.
Directly use Dify' backend services to develop based on a WebApp scaffold.
-80%
Prompt Engineering
Can only be done by calling APIs or Playground.
Debug based on the user's input data.
-25%
Data Preparation and Embedding
Writing code to implement long text data processing and embedding.
Upload text or bind data sources to the platform.
-80%
Application Logging and Analysis
Writing code to record logs and accessing databases to view them.
The platform provides real-time logging and analysis.
-70%
Data Analysis and Fine-Tuning
Technical personnel manage data and create fine-tuning queues.
Non-technical personnel can collaborate and adjust the model visually.
-60%
AI Plugin Development and Integration
Writing code to create and integrate AI plugins.
The platform provides visual tools for creating and integrating plugins.
-50%

Before using an LLMOps platform like Dify, the process of developing applications based on LLMs can be cumbersome and time-consuming. Developers need to handle tasks at each stage on their own, which can lead to inefficiencies, difficulties in scaling, and security issues. Here is the development process before using an LLMOps platform:
Data Preparation: Manually collect and preprocess data, which may involve complex data cleaning and annotation work, requiring a significant amount of code.
Prompt Engineering: Developers can only write and debug Prompts through API calls or Playgrounds, lacking real-time feedback and visual debugging.
Embedding and Context Management: Manually handling the embedding and storage of long contexts, which can be difficult to optimize and scale, requiring a fair amount of programming work and familiarity with model embedding and vector databases.
Application Monitoring and Maintenance: Manually collect and analyze performance data, possibly unable to detect and address issues in real-time, and may even lack log records.
Model Fine-tuning: Independently manage the fine-tuning data preparation and training process, which can lead to inefficiencies and require more code.
System and Operations: Technical personnel involvement or cost required for developing a management backend, increasing development and maintenance costs, and lacking support for collaboration and non-technical users.
With the introduction of an LLMOps platform like Dify, the process of developing applications based on LLMs becomes more efficient, scalable, and secure. Here are the advantages of developing LLM applications using Dify:
Data Preparation: The platform provides data collection and preprocessing tools, simplifying data cleaning and annotation tasks, and minimizing or even eliminating coding work.
Prompt Engineering: WYSIWYG Prompt editing and debugging, allowing real-time optimization and adjustments based on user input data.
Embedding and Context Management: Automatically handling the embedding, storage, and management of long contexts, improving efficiency and scalability without the need for extensive coding.
Application Monitoring and Maintenance: Real-time monitoring of performance data, quickly identifying and addressing issues, ensuring the stable operation of applications, and providing complete log records.
Model Fine-tuning: The platform offers one-click fine-tuning functionality based on previously annotated real-use data, improving model performance and reducing coding work.
System and Operations: User-friendly interface accessible to non-technical users, supporting collaboration among multiple team members, and reducing development and maintenance costs. Compared to traditional development methods, Dify offers more transparent and easy-to-monitor application management, allowing team members to better understand the application's operation.
Additionally, Dify will provide AI plugin development and integration features, enabling developers to easily create and deploy LLM-based plugins for various applications, further enhancing development efficiency and application value.
Dify is an easy-to-use LLMOps platform designed to empower more people to create sustainable, AI-native applications. With visual orchestration for various application types, Dify offers out-of-the-box, ready-to-use applications that can also serve as Backend-as-a-Service APIs. Unify your development process with one API for plugins and knowledge integration, and streamline your operations using a single interface for prompt engineering, visual analytics, and continuous improvement.


Creating Dify Apps

Quickstart
In Dify, an "application" refers to a real-world scenario application built on large language models such as GPT. By creating an application, you can apply intelligent AI technology to specific needs. It encompasses both the engineering paradigms for developing AI applications and the specific deliverables.
In short, an application delivers to developers:
A user-friendly, encapsulated LLM API that can be called directly by backend or frontend applications with token authentication
A ready-to-use, beautiful, and hosted Web App that you can develop further using the Web App templates
A set of easy-to-use interfaces for Prompt Engineering, context management, log analysis, and annotation
You can choose one or all of them to support your AI application development.
Application Types
Dify offers two types of applications: text generation and conversational. More application paradigms may appear in the future (we should keep up-to-date), and the ultimate goal of Dify is to cover more than 80% of typical LLM application scenarios. The differences between text generation and conversational applications are shown in the table below:


Text Generator
Chat App
WebApp Interface
Form + Results
Chat style
API Endpoint
completion-messages
chat-messages
Interaction Mode
One question and one answer
Multi-turn dialogue
Streaming results return
Supported
Supported
Context Preservation
Current time
Continuous
User input form
Supported
Supported
Knowledge&Plugins
Supported
Supported
AI opening remarks
Not supported
Supported
Scenario example
Translation, judgment, indexing
Chat or everything

Steps to Create an Application
After logging in as an administrator in Dify, go to the main navigation application page Click "Create New Application" Choose a conversational or text generation application and give it a name (modifiable later)

Create a new App
We provide some templates in the application creation interface, and you can click to create from a template in the popup when creating an application. These templates will provide inspiration and reference for the application you want to develop.
Creating from a Configuration File
If you have obtained a template from the community or someone else, you can click to create from an application configuration file. Uploading the file will load most of the settings from the other party's application (but not the knowledge at present).
Your Application
If you are using it for the first time, you will be prompted to enter your OpenAI API key. A properly functioning LLM key is a prerequisite for using Dify. If you don't have one yet, please apply for one.

Enter your OpenAI API Key
After creating an application or selecting an existing one, you will arrive at an application overview page showing the application's profile. You can directly access your WebApp or check the API status here, as well as enable or disable them.
Statistics show the usage, active user count, and LLM call consumption of the application over a period of time—enabling you to continually improve the cost-effectiveness of application operations. We will gradually provide more useful visualization capabilities; please let us know what you want.
Total Messages: Daily AI interactions count; prompt engineering/debugging excluded.
Active Users: Unique users engaging in Q&A with AI; prompt engineering/debugging excluded.
Avg. Session Interactions: Continuous user-AI communication count; for conversation-based apps.
User Satisfaction Rate: Likes per 1,000 messages; indicates satisfaction with AI answers.
Avg. Response Time: Time (ms) for AI to process/respond; for text-based apps.
Token Usage: Daily language model token usage; for cost control.
What's Next
Try your WebApp
Take a tour of the Configuration, Development, and Logs pages on the left
Try configuring an application using a reference case
If you have the ability to develop frontend applications, please consult the API documentation
Overview
Web applications are for application consumers. When an application developer creates an application in Dify, he will get a corresponding web application. Users of the web application can use it without logging in. The web application is adapted to different sizes of devices: PC, tablet and mobile.
The content of the web application is consistent with the configuration published by the application. When the configuration of the application is modified and the "Publish" button is clicked on the prompt word layout page of the application to publish, the content of the web application will also be updated according to the configuration of the current application.
We can enable and disable access to the web application on the application overview page, and modify the site information of the web application:
Icon
Name
Application Description
Interface Language
Copyright Information
Privacy Policy Link
The functional performance of the web application depends on whether the developer enables this function when compiling the application, for example:
Conversation remarks
Variables filled in before the conversation
Follow-up
Speech to text
More answers like this (Text Generation apps)
...
In the following chapters, we will introduce the two types of web applications separately:
Text Generator
Conversational
Setting Prompts
Master the use of Dify for orchestrating applications and practicing Prompt Engineering, and build high-value AI applications with the two built-in application types.
The core concept of Dify is the declarative definition of AI applications. Everything including Prompts, context, plugins, etc. can be described in a YAML file (which is why it is called Dify). It ultimately presents a single API or out-of-the-box WebApp.
At the same time, Dify provides an easy-to-use Prompt orchestration interface where developers can visually orchestrate various application features based on Prompts. Doesn't it sound simple?
For both simple and complex AI applications, good Prompts can effectively improve the quality of model output, reduce error rates, and meet the needs of specific scenarios. Dify currently provides two common application forms: conversational and text generator. This section will guide you through visually orchestrating AI applications.
Application Orchestration Steps
Determine application scenarios and functional requirements
Design and test Prompts and model parameters
Orchestrate Prompts
Publish the application
Observe and continuously iterate
Hands-on Practice
TODO
The Differences between Application Types
Text generation and conversation applications in Dify have slight differences in prompt orchestration. Conversation applications require incorporating "conversation lifecycle" to meet more complex user scenarios and context management needs.
Prompt Engineering has developed into a field with tremendous potential, worthy of continuous exploration. Please continue reading to learn about the orchestration guidelines for both types of applications.
Extended Reading
Learn Prompting
ChatGPT Prompt Engineering for Developers
Awesome ChatGPT Prompts
Agent Assistant
Definition
An Agent Assistant can leverage the reasoning abilities of large language models (LLMs). It independently sets goals, simplifies complex tasks, operates tools, and refines processes to complete tasks autonomously.
Usage Instructions
To facilitate quick learning and use, application templates for the Agent Assistant are available in the 'Explore' section. You can integrate these templates into your workspace. The new Dify 'Studio' also allows the creation of a custom Agent Assistant to suit individual requirements. This assistant can assist in analyzing financial reports, composing reports, designing logos, and organizing travel plans.

Explore-Agent Assistant Application Template
After entering 'Studio-Assistant', you can begin orchestrating by choosing the Agent Assistant.

Studio-Create Agent Assistant
The task completion ability of the Agent Assistant depends on the inference capabilities of the model selected. We recommend using a more powerful model series like GPT-4 when employing Agent Assistant to achieve more stable task completion results.

Selecting the Reasoning Model for Agent Assistant
You can write prompts for the Agent Assistant in 'Instructions'. To achieve optimal results, you can clearly define its task objectives, workflow, resources, and limitations in the instructions.

Orchestrating Prompts for Agent Assistant
Adding Tools for the Agent Assistant
In the "Context" section, you can incorporate knowledge base tools that the Agent Assistant can utilize for information retrieval. This will assist in providing it with external background knowledge.
In the "Tools" section, you are able to add tools that are required for use. These tools can enhance the capabilities of LLMs, such as internet searches, scientific computations, or image creation, thereby enriching the LLM's ability to interact with the real world. Dify offers two types of tools: built-in tools and custom tools.
You have the option to directly use built-in tools in Dify, or you can easily import custom API tools (currently supporting OpenAPI/Swagger and OpenAI Plugin standards).

Adding Tools for the Assistant
The tool allows you to create more powerful AI applications on Dify. For example, you can orchestrate suitable tools for Agent Assistant, enabling it to complete complex tasks through reasoning, step decomposition, and tool invocation. Additionally, the tool facilitates the integration of your application with other systems or services, allowing interaction with the external environment, such as code execution and access to exclusive information sources.
Agent Settings
On Dify, two inference modes are provided for Agent Assistant: Function Calling and ReAct. Models like GPT-3.5 and GPT-4 that support Function Calling have demonstrated better and more stable performance. For model series that do not support Function Calling, we have implemented the ReAct inference framework to achieve similar effects.
In the Agent settings, you can modify the iteration limit of the Agent.

Function Calling Mode

ReAct Mode
Configuring the Conversation Opener
You can set up a conversation opener and initial questions for your Agent Assistant. The configured conversation opener will be displayed at the beginning of each user's first interaction, showcasing the types of tasks the Agent can perform, along with examples of questions that can be asked.

Configuring the Conversation Opener and Initial Questions
Debugging and Preview
After orchestrating your Agent Assistant, you have the option to debug and preview it before publishing it as an application. This allows you to assess the effectiveness of the agent in completing tasks.

Debugging and Preview
Application Publish

Publishing the Application as a Webapp

Chat App
Conversation applications use a one-question-one-answer mode to have a continuous conversation with the user.
Applicable scenarios
Conversation applications can be used in fields such as customer service, online education, healthcare, financial services, etc. These applications can help organizations improve work efficiency, reduce labor costs, and provide a better user experience.
How to compose
Conversation applications supports: prompts, variables, context, opening remarks, and suggestions for the next question.
Here, we use a interviewer application as an example to introduce the way to compose a conversation applications.
Step 1 Create an application
Click the "Create Application" button on the homepage to create an application. Fill in the application name, and select "Chat App" as the application type.

Create Application
Step 2: Compose the Application
After the application is successfully created, it will automatically redirect to the application overview page. Click on the left-hand menu: “Prompt Eng.” to compose the application.

2.1 Fill in Prompts
Prompts are used to give a series of instructions and constraints to the AI response. Form variables can be inserted, such as {{input}}. The value of variables in the prompts will be replaced with the value filled in by the user.
The prompt we are filling in here is:
I want you to be the interviewer for the {{jobName}} position. I will be the candidate, and you will ask me interview questions for the position of {{jobName}} developer. I hope you will only answer as the interviewer. Don't write all the questions at once. I wish for you to only interview me. Ask me questions and wait for my answers. Don't write explanations. Ask me one by one like an interviewer and wait for my answer.
When I am ready, you can start asking questions.

For a better experience, we will add an opening dialogue: "Hello, {{name}}. I'm your interviewer, Bob. Are you ready?"
To add the opening dialogue, click the "Add Feature" button in the upper left corner, and enable the "Conversation remarkers" feature:

And then edit the opening remarks:

2.2 Adding Context
If an application wants to generate content based on private contextual conversations, it can use our knowledge feature. Click the "Add" button in the context to add a knowledge base.

2.3 Debugging
We fill in the user input on the right side and debug the input content.

If the results are not satisfactory, you can adjust the prompts and model parameters. Click on the model name in the upper right corner to set the parameters of the model:

We support the GPT-4 model.
2.4 Publish
After debugging the application, click the "Publish" button in the upper right corner to save the current settings.
Share Application
On the overview page, you can find the sharing address of the application. Click the "Preview" button to preview the shared application. Click the "Share" button to get the sharing link address. Click the "Settings" button to set the shared application information.

If you want to customize the application that you share, you can Fork our open source WebApp template. Based on the template, you can modify the application to meet your specific needs and style requirements.

Text Generator
Text generation applications are applications that can automatically generate high-quality text based on prompts provided by users. They can generate various types of text, such as article summaries, translations, etc.
Applicable scenarios
Text generation applications are suitable for scenarios that require a large amount of text creation, such as news media, advertising, SEO, marketing, etc. They can provide efficient and fast text generation services for these industries, reduce labor costs, and improve production efficiency.
How to compose
Text generation applications supports: prefix prompt words, variables, context, and generating more similar content.
Here, we use a translation application as an example to introduce the way to compose a text generation applications.
Step 1: Create the application
Click the "Create Application" button on the homepage to create an application. Fill in the application name, and select "Text Generator" as the application type.

Create Application
Step 2: Compose the Application
After the application is successfully created, it will automatically redirect to the application overview page. Click on the left-hand menu: “Prompt Eng.” to compose the application.

2.1 Fill in Prefix Prompts
Prompts are used to give a series of instructions and constraints to the AI response. Form variables can be inserted, such as {{input}}. The value of variables in the prompts will be replaced with the value filled in by the user.
The prompt we are filling in here is: Translate the content to: {{language}}. The content is as follows:

2.2 Adding Context
If the application wants to generate content based on private contextual conversations, our knowledge feature can be used. Click the "Add" button in the context to add a knowledge base.

2.3 Adding Future: Generate more like this
Generating more like this allows you to generate multiple texts at once, which you can edit and continue generating from. Click on the "Add Future" button in the upper left corner to enable this feature.

2.4 Debugging
We debug on the right side by entering variables and querying content. Click the "Run" button to view the results of the operation.

If the results are not satisfactory, you can adjust the prompts and model parameters. Click on the model name in the upper right corner to set the parameters of the model:

2.5 Publish
After debugging the application, click the "Publish" button in the upper right corner to save the current settings.
Share Application
You can find the sharing address of the application on the overview page. Click the "Preview" button to preview the shared application. Click the "Share" button to obtain the sharing link address. Click the "Settings" button to set the information of the shared application.

If you want to customize the application shared outside, you can Fork our open source WebApp template. Based on the template, you can modify the application to meet your specific situation and style requirements.

FAQ
1. How to choose a basic model?
gpt-3.5-turbo •gpt-3.5-turbo is an upgraded version of the gpt-3 model series. It is more powerful than gpt-3 and can handle more complex tasks. It has significant improvements in understanding long text and cross-document reasoning. Gpt-3.5 turbo can generate more coherent and persuasive text. It also has great improvements in summarization, translation and creative writing. Good at: Long text understanding, cross-document reasoning, summary, translation, creative writing
gpt-4 •gpt-4 is the latest and most powerful Transformer language model. It has nearly 200 billion pre-trained parameters, making it state-of-the-art on all language tasks, especially those requiring deep understanding and generation of long, complex responses. Gpt-4 can handle all aspects of human language, including understanding abstract concepts and cross-page reasoning. Gpt-4 is the first true general language understanding system that can handle any natural language processing task in the field of artificial intelligence. Good at: *All NLP tasks, language understanding, long text generation, cross-document reasoning, understanding abstract concepts*Please refer to: https://platform.openai.com/docs/models/overview
2. Why is it recommended to set max_tokens smaller?
Because in natural language processing, longer text outputs usually require longer computation time and more computing resources. Therefore, limiting the length of the output text can reduce the computational cost and time to some extent. For example, set: max_tokens=500, which means that only the first 500 tokens of the output text are considered, and the part exceeding this length will be discarded. The purpose of doing so is to ensure that the length of the output text does not exceed the acceptable range of the LLM, while making full use of computing resources to improve the efficiency of the model. On the other hand, more often limiting max_tokens can increase the length of the prompt, such as the limit of gpt-3.5-turbo is 4097 tokens, if you set max_tokens=4000, then only 97 tokens are left for the prompt, and an error will be reported if exceeded.
3. How to split long text data in the knowledge reasonably?
In some natural language processing applications, text is often split into paragraphs or sentences for better processing and understanding of semantic and structural information in the text. The minimum splitting unit depends on the specific task and technical implementation. For example:
• For text classification tasks, text is usually split into sentences or paragraphs.
• For machine translation tasks, entire sentences or paragraphs need to be used as splitting units.
Finally, experiments and evaluations are still needed to determine the most suitable embedding technology and splitting unit. The performance of different technologies and splitting units can be compared on the test set to select the optimal scheme.
4. What distance function did we use when getting knowledge segmentation?
We use cosine similarity. The choice of distance function is usually irrelevant. OpenAI embeddings are normalized to length 1, which means:
•Using the dot product to calculate cosine similarity can be slightly faster
•Cosine similarity and Euclidean distance will lead to the same ranking
After the embedding vectors are normalized to length 1, calculating the cosine similarity between two vectors can be simplified to their dot product. Because the normalized vectors have a length of 1, the result of the dot product is equal to the result of the cosine similarity.
Since the dot product calculation is faster than other similarity metrics (such as Euclidean distance), using normalized vectors for dot product calculation can slightly improve computational efficiency.
5. When filling in the OpenAI key, the error "Validation failed: You exceeded your current quota, please check your plan and billing details" occurs. What is causing this error?
This error indicates that the OpenAI key account balance has been used up. Please top up the OpenAI account at openai.com. Refer to OpenAI for details on their plans and billing.
6. When using OpenAI's key for dialogue in the application, there is an error prompt as follows. What is the cause?
Error 1：
Copy
The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application
Error 1：
Copy
Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.
Please check if the official interface call rate limit has been reached. Please refer to the official documentation for details.
7. After local deployment, Explore-Chat returns an error "Unrecognized request argument supplied: functions". How can this be resolved?
First, please check that the frontend and backend versions are up-to-date and consistent with each other. This error can also occur if an Azure OpenAI key is being used without successfully deploying the model. Verify that the Azure OpenAI resource has a deployed model - the gpt-3.5-turbo model version must be 0613 or later, as earlier versions do not support the function calling capabilities required by Explore-Chat.
8. When switching models in the app, the following error is encountered:
Copy
Anthropic: Error code: 400 - f'error': f'type': "invalid request error, 'message': 'temperature: range: -1 or 0..1)
This error occurs because each model has different valid ranges for its parameters. Make sure to configure the parameter value according to the allowed range for the current model.
9. How to solve the following error prompt?
Copy
Query or prefix prompt is too long, you can reduce the preix prompt, or shrink the max token, or switch to a llm with a larger token limit size
You can lower the value of "Max token" in the parameter settings of the Prompt Eng.
10. What are the default models in Dify, and can open-source LLMs be used?
A: The default models can be configured under Settings - Model Provider. Currently supported text generation LLMs include OpenAI, Azure OpenAl, Anthropic, etc. At the same time, open-source LLMs hosted on Hugging Face, Replicate, xinference, etc. can also be integrated.
11. The knowledge in Community Edition gets stuck in "Queued" when Q&A segmentation mode is enabled.
Please check if the rate limit has been reached for the Embedding model API key used.
12. The error "Invalid token" appears when using the app.
There are two potential solutions if the error "Invalid token" appears:
Clear the browser cache (cookies, session storage, and local storage) or the app cache on mobile. Then, revisit the app.
Regenerate the app URL and access the app again with the new URL. This should resolve the "Invalid token" error.
13. What are the size limits for uploading knowledge documents?
The maximum size for a single document upload is currently 15MB. There is also a limit of 100 total documents. These limits can be adjusted if you are using a local deployment. Refer to the documentation for details on changing the limits.
14. Why does Claude still consume OpenAI credits when using the Claude model?
The Claude model does not have its own embedding model. Therefore, the embedding process and other dialog generation like next question suggestions default to using OpenAI keys. This means OpenAI credits are still consumed. You can set different default inference and embedding models under Settings > Model Provider.
15. Is there any way to control the greater use of knowledge data rather than the model's own generation capabilities?
Whether to use a knowledge base is related to the description of the knowledge. Please write the knowledge description clearly as much as possible. Please refer to the documentation for details.
16. How to better segment the uploaded knowledge document in Excel?
Set the header in the first row, and display the content in each subsequent row. Do not have any additional header settings or complex formatted table content.
17. I have already purchased ChatGPT plus, why can't I still use GPT4 in Dify?
ChatGPT Plus and OpenAI's GPT-4 model API are two separate products with separate pricing. The model APIs have their own pricing structure, see OpenAI's pricing documentation for details. To get access to the GPT-4 model API, you need to pay for a billing cycle - simply having a payment method on file and access to GPT-3.5 via ChatGPT Plus is not sufficient. Please refer to OpenAI's official documentation for complete details on gaining access to GPT-4.
18. How to add other embedding models?
Dify supports using the listed providers as an Embedding model provider, simply select the Embedding type in the configuration box.
Azure
LocalAI
MiniMax
OpenAI
Replicate
XInference
19. How can I set my own created app as an app template?
The ability to set your own created app as a template is currently not supported. The existing templates are provided by Dify officially for cloud version users' reference. If you are using the cloud version, you can add apps to your workspace or customize them to make your own after modifications. If you are using the community version and need to create more app templates for your team, you may consult our business team to obtain paid technical support: business@dify.ai
Use Cases
Notion AI Assistant Based on Your Own Notes
Intro​
Notion is a powerful tool for managing knowledge. Its flexibility and extensibility make it an excellent personal knowledge library and shared workspace. Many people use it to store their knowledge and work in collaboration with others, facilitating the exchange of ideas and the creation of new knowledge.
However, this knowledge remains static, as users must search for the information they need and read through it to find the answers they're seeking. This process is neither particularly efficient nor intelligent.
Have you ever dreamed of having an AI assistant based on your Notion library? This assistant would not only assist you in reviewing your knowledge base, but also engage in the communication like a seasoned butler, even answering other people's questions as if you were the master of your personal Notion library.
How to Make Your Notion AI Assistant Come True?​
Now, you can make this dream come true through Dify. Dify is an open-source LLMOps (Large Language Models Ops) platform.
Large Language Models like ChatGPT and Claude, have been using their impressive abilities to reshape the world. Their powerful learning aptitude primarily attributable to robust training data. Luckily, they've evolved to be sufficiently intelligent to learn from the content you provide, thus making the process of ideating from your personal Notion library, a reality.
Without Dify, you might need to acquaint yourself with langchain, an abstraction that streamlines the process of assembling these pieces.
How to Use Dify to Build Your Personal Notion AI Assistant?​
The process to train a Notion AI assistant is relatively straightforward. Just follow these steps:
Login to Dify.
Create a new datasets.
Connect with Notion and your datasets.
Start training.
Create your own AI application.
1. Login to dify​
Click here to login to Dify. You can conveniently log in using your GitHub or Google account.
If you are using GitHub account to login, how about getting this project a star? It really help us a lot!

login-1
2. Create new datasets​​
Click the Knowledge button on the top side bar, followed by the Create Knowledge button.

login-2
3. Connect with Notion and Your Knowledge​
Select "Sync from Notion" and then click the "Connect" button..

connect-with-notion-1
Afterward, you'll be redirected to the Notion login page. Log in with your Notion account.

Check the permissions needed by Dify, and then click the "Select pages" button.

Select the pages you want to synchronize with Dify, and press the "Allow access" button.

4. Start training​
Specifying the pages for AI need to study, enabling it to comprehend the content within this section of Notion. Then click the "next" button.

train-1
We suggest selecting the "Automatic" and "High Quality" options to train your AI assistant. Then click the "Save & Process" button.

train-2
Enjoy your coffee while waiting for the training process to complete.

train-3
5. Create Your AI application​
You must create an AI application and link it with the knowledge you've recently created.
Return to the dashboard, and click the "Create new APP" button. It's recommended to use the Chat App directly.

create-app-1
Select the "Prompt Eng." and link your notion datasets in the "context".

create-app-2
I recommend adding a 'Pre Prompt' to your AI application. Just like spells are essential to Harry Potter, similarly, certain tools or features can greatly enhance the ability of AI application.
For example, if your Notion notes focus on problem-solving in software development, could write in one of the prompts:
I want you to act as an IT Expert in my Notion workspace, using your knowledge of computer science, network infrastructure, Notion notes, and IT security to solve the problems.

It's recommended to initially enable the AI to actively furnish the users with a starter sentence, providing a clue as to what they can ask. Furthermore, activating the 'Speech to Text' feature can allow users to interact with your AI assistant using their voice.

Finally, Click the "Publish" button on the top right of the page. Now you can click the public URL in the "Overview" section to converse with your personalized AI assistant!

create-app-4
Utilizing API to Integrate With Your Project
Each AI application baked by Dify can be accessed via its API. This method allows developers to tap directly into the robust characteristics of large language models (LLMs) within frontend applications, delivering a true "Backend-as-a-Service" (BaaS) experience.
With effortless API integration, you can conveniently invoke your Notion AI application without the need for intricate configurations.
Click the "API Reference" button on the page of Overview page. You can refer to it as your App's API document.

using-api-1
1. Generate API Secret Key​
For sercurity reason, it's recommened to create new API secret key to access your AI application.

using-api-2
2. Retrieve Conversation ID​
After chatting with your AI application, you can retrieve the session ID from the "Logs & Ann." pages.

using-api-3
3. Invoke API​
You can run the example request code on the API document to invoke your AI application in terminal.
Remember to replace YOUR SECRET KEY and conversation_id on your code.
You can input empty conversation_id at the first time, and replace it after you receive response contained conversation_id.
Copy
curl --location --request POST 'https://api.dify.ai/v1/chat-messages' \

--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \

--header 'Content-Type: application/json' \

--data-raw '{

   "inputs": {},

   "query": "eh",

   "response_mode": "streaming",

   "conversation_id": "",

   "user": "abc-123"

}'
Sending request in terminal and you will get a successful response.

using-api-4
If you want to continue this chat, go to replace the conversation_id of the request code to the conversation_id you get from the response.
And you can check all the conversation history on the "Logs & Ann." page.

using-api-5
Sync with notion periodically​
If your Notion's pages have updated, you can sync with Dify periodically to keep your AI assistant up-to-date. Your AI assistant will learn from the new content.

create-app-5
Summary​
In this tutorial, we have learned not only how to import Your Notion data into Dify, but also know how to use the API to integrate it with your project.
Dify is a user-friendly LLMOps platform targeted to empower more individuals to create sustainable, AI-native applications. With visual orchestration designed for various application types, Dify offers ready-to-use applications that can assist you in utilizing data to craft your distinctive AI assistant. Do not hesitate to contact us if you have any inquiries.


AI ChatBot with Business Data
AI-powered customer service may be a standard feature for every business website, and it is becoming easier to implement with higher levels of customization. The following content will guide you on how to create an AI-powered customer service for your website in just a few minutes using Dify.
Prerequisite
Register or Deploy Dify.AI
Dify is an open source product which you can find on GitHub and deploy it to your local or company intranet. Meanwhile, it provides a cloud SaaS version, access Didy.AI to register and use it.
Apply for API key from OpenAI and other model providers.
Dify provides free message call usage quotas for OpenAI GPT series (200 times) and Antropic Claude (1000 times) AI models, which require tokens to be consumed. Before you run out, you need to apply for your own API key through the official channel of the model provider. You can enter the key in Dify's "Settings" - "Model Provider".
Upload your product documentation or knowledge base.
If you want to build an AI Chatbot based on the company's existing knowledge base and product documents, then you need to upload as many product-related documents as possible to Dify's knowledge. Dify helps you complete segmentation and cleaning of the data. The Dify knowledge supports two indexing modes: high quality and economical. We recommend using the high quality mode, which consumes tokens but provides higher accuracy.
Create a new knowledge base
upload your business data (support batch uploading multiple texts)
select the cleaning method
Click [Save and Process], and it will take only a few seconds to complete the processing.

Create an AI application and give it instructions
Create a conversational app on the [Build App] page. Then start setting up the prompt and its front-end user experience interactions.
Give the AI instruction: Click on the "Pre Prompt" on the left to edit your Prompt, so that it can play the role of customer service and communicate with users. You can specify its tone, style, and limit it to answer or not answer certain questions.
Let AI possess your business knowledge: add the target knowledge you just uploaded in the [context].
Set up the opening remarks: click "Add Feature" to turn on the feature. The purpose is to add an opening line for AI applications, so that when the user opens the customer service window, it will greet the user first and increase affinity.
Set up the "Next Question Suggestion": turn on this feature to "Add Feature". The purpose is to give users a direction for their next question after they have asked one.
Choose a suitable model and adjust the parameters: different models can be selected in the upper right corner of the page. The performance and token price consumed by different models are different. In this example, we use the GPT3.5 model.
In this case, we assign a role to the AI:
Pre prompt：You are Bob, the AI customer service for Dify, specializing in answering questions about Dify's products, team, or LLMOps for users.Please note, refuse to answer when users ask "inappropriate questions", i.e., content beyond the scope of this document.
Opening remarks：Hey {{username}}, I'm Bob☀️, the first AI member of Dify. You can discuss with me any questions related to Dify products, team, and even LLMOps.

Debug the performance of AI Chatbot and publish.
After completing the setup, you can send messages to it on the right side of the current page to debug whether its performance meets expectations. Then click "Publish". And then you get an AI chatbot.

Embed AI Chatbot application into your front-end page.
This step is to embed the prepared AI chatbot into your official website . Click [Overview] -> [Embedded], select the script tag method, and copy the script code into the <head> or <body> tag of your website. If you are not a technical person, you can ask the developer responsible for the official website to paste and update the page.

Paste the copied code into the target location on your website.

Update your official website and you can get an AI intelligent customer service with your business data. Try it out to see the effect.

Above is an example of how to embed Dify into the official website through the AI chatbot Bob of Dify official website. Of course, you can also use more features provided by Dify to enhance the performance of the chatbot, such as adding some variable settings, so that users can fill in necessary judgment information before interaction, such as name, specific product used and so on.
Welcome to explore in Dify together!

Midjourney Prompt Bot
via @op7418 on Twitter
I recently tried out a natural language programming tool called Dify, developed by @goocarlos. It allows someone without coding knowledge to create a web application just by writing prompts. It even generates the API for you, making it easy to deploy your application on your preferred platform.
The application I created using Dify took me only 20 minutes, and the results were impressive. Without Dify, it might have taken me much longer to achieve the same outcome. The specific functionality of the application is to generate Midjourney prompts based on short input topics, assisting users in quickly filling in common Midjourney commands. In this tutorial, I will walk you through the process of creating this application to familiarize you with the platform.
Dify offers two types of applications: conversational applications similar to ChatGPT, which involve multi-turn dialogue, and text generation applications that directly generate text content with the click of a button. Since we want to create a Midjoureny prompt bot, we'll choose the text generator.
You can access Dify here: https://dify.ai/

Once you've created your application, the dashboard page will display some data monitoring and application settings. Click on "Prompt Engineering" on the left, which is the main working page.

On this page, the left side is for prompt settings and other functions, while the right side provides real-time previews and usage of your created content. The prefix prompts are the triggers that the user inputs after each content, and they instruct the GPT model how to process the user's input information.

Take a look at my prefix prompt structure: the first part instructs GPT to output a description of a photo in the following structure. The second structure serves as the template for generating the prompt, mainly consisting of elements like 'Color photo of the theme,' 'Intricate patterns,' 'Stark contrasts,' 'Environmental description,' 'Camera model,' 'Lens focal length description related to the input content,' 'Composition description relative to the input content,' and 'The names of four master photographers.' This constitutes the main content of the prompt. In theory, you can now save this to the preview area on the right, input the theme you want to generate, and the corresponding prompt will be generated.

You may have noticed the "{{proportion}}" and "{{version}}" at the end. These are variables used to pass user-selected information. On the right side, users are required to choose image proportions and model versions, and these two variables help carry that information to the end of the prompt. Let's see how to set them up.

Our goal is to fill in the user's selected information at the end of the prompt, making it easy for users to copy without having to rewrite or memorize these commands. For this, we use the variable function.
Variables allow us to dynamically incorporate the user's form-filled or selected content into the prompt. For example, I've created two variables: one represents the image proportion, and the other represents the model version. Click the "Add" button to create the variables.

After creation, you'll need to fill in the variable key and field name. The variable key should be in English. The optional setting means the field will be non-mandatory when the user fills it. Next, click "Settings" in the action bar to set the variable content.

Variables can be of two types: text variables, where users manually input content, and select options where users select from given choices. Since we want to avoid manual commands, we'll choose the dropdown option and add the required choices.

Now, let's use the variables. We need to enclose the variable key within double curly brackets {} and add it to the prefix prompt. Since we want the GPT to output the user-selected content as is, we'll include the phrase "Producing the following English photo description based on user input" in the prompt.

However, there's still a chance that GPT might modify our variable content. To address this, we can lower the diversity in the model selection on the right, reducing the temperature and making it less likely to alter our variable content. You can check the tooltips for other parameters' meanings.

With these steps, your application is now complete. After testing and ensuring there are no issues with the output, click the "Publish" button in the upper right corner to release your application. You and users can access your application through the publicly available URL. You can also customize the application name, introduction, icon, and other details in the settings.

That's how you create a simple AI application using Dify. You can also deploy your application on other platforms or modify its UI using the generated API. Additionally, Dify supports uploading your own data, such as building a customer service bot to assist with product-related queries. This concludes the tutorial, and a special thanks to @goocarlos for creating such a fantastic product.

Launching Dify Apps
Quickstart
One of the benefits of creating AI applications with Dify is that you can launch a user-friendly Web application in just a few minutes, based on your Prompt orchestration.
If you are using the self-hosted open-source version, the application will run on your server.
If you are using the cloud version, the application will be hosted on udify.app.
Launch WebApp
In the application overview page, you can find a card for the AI site (WebApp). Simply enable WebApp access to get a shareable link for your users.

Share your WebApp
We provide a sleek WebApp interface for both of the following applications:
Text Generation (go to preview)
Conversational (go to preview)
Configure your WebApp
Click the settings button on the WebApp card to configure some options for the AI site. These will be visible to the end users:
Icon
Name
Application Description
Interface Language
Copyright Information
Privacy Policy Link
Embed your WebApp
Dify supports embedding your AI application into your business website. With this capability, you can create AI customer service and business knowledge Q&A applications with business data on your official website within minutes. Click the embed button on the WebApp card, copy the embed code, and paste it into the desired location on your website.
For iframe tag:
Copy the iframe code and paste it into the tags (such as <div>, <section>, etc.) on your website used to display the AI application.
For script tag:
Copy the script code and paste it into the <head> or <body> tags on your website.

For example, if you paste the script code into the section of your official website, you will get an AI chatbot on your website:

Developing with APIs
Dify offers a "Backend-as-a-Service" API, providing numerous benefits to AI application developers. This approach enables developers to access the powerful capabilities of large language models (LLMs) directly in frontend applications without the complexities of backend architecture and deployment processes.
Benefits of using Dify API
Allow frontend apps to securely access LLM capabilities without backend development
Design applications visually with real-time updates across all clients
Well-encapsulated original LLM APIs
Effortlessly switch between LLM providers and centrally manage API keys
Operate applications visually, including log analysis, annotation, and user activity observation
Continuously provide more tools, plugins, and knowledge
How to use
Choose an application, and find the API Access in the left-side navigation of the Apps section. On this page, you can view the API documentation provided by Dify and manage credentials for accessing the API.

API document
You can create multiple access credentials for an application to deliver to different users or developers. This means that API users can use the AI capabilities provided by the application developer, but the underlying Prompt engineering, knowledge, and tool capabilities are encapsulated.
In best practices, API keys should be called through the backend, rather than being directly exposed in plaintext within frontend code or requests. This helps prevent your application from being abused or attacked.
For example, if you're a developer in a consulting company, you can offer AI capabilities based on the company's private database to end-users or developers, without exposing your data and AI logic design. This ensures a secure and sustainable service delivery that meets business objectives.
Text-generation application
These applications are used to generate high-quality text, such as articles, summaries, translations, etc., by calling the completion-messages API and sending user input to obtain generated text results. The model parameters and prompt templates used for generating text depend on the developer's settings in the Dify Prompt Arrangement page.
You can find the API documentation and example requests for this application in Applications -> Access API.
For example, here is a sample call an API for text generation:
Copy
curl --location --request POST 'https://api.dify.ai/v1/completion-messages' \

--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \

--header 'Content-Type: application/json' \

--data-raw '{

   "inputs": {},

   "response_mode": "streaming",

   "user": "abc-123"

}'
Conversational applications
Suitable for most scenarios, conversational applications engage in continuous dialogue with users in a question-and-answer format. To start a conversation, call the chat-messages API and maintain the session by continuously passing in the returned conversation_id.
You can find the API documentation and example requests for this application in Applications -> Access API.
For example, here is a sample call an API for chat-messages:
Copy
curl --location --request POST 'https://api.dify.ai/v1/chat-messages' \

--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \

--header 'Content-Type: application/json' \

--data-raw '{

   "inputs": {},

   "query": "eh",

   "response_mode": "streaming",

   "conversation_id": "1c7e55fb-1ba2-4e10-81b5-30addcea2276"

   "user": "abc-123"

}'
FAQ
1. What is a Bearer Token?
Bearer authentication (also called token authentication) is an HTTP authentication scheme that involves security tokens called bearer tokens. The name “Bearer authentication” can be understood as “give access to the bearer of this token.” The bearer token is a cryptic string, usually generated by the server in response to a login request. The client must send this token in the Authorization header when making requests to protected resources:
Copy
Authorization: Bearer <token>
The Bearer authentication scheme was originally created as part of OAuth 2.0 in RFC 6750, but is sometimes also used on its own. Similarly to Basic authentication, Bearer authentication should only be used over HTTPS (SSL).

Using Dify Apps
Text Generator
The text generation application is an application that automatically generates high-quality text according to the prompts provided by the user. It can generate various types of text, such as article summaries, translations, etc.
Text generation applications support the following features:
Run it once.
Run in batches.
Save the run results.
Generate more similar results.
Let's introduce them separately.
Run it once
Enter the query content, click the run button, and the result will be generated on the right, as shown in the following figure:

In the generated results section, click the "Copy" button to copy the content to the clipboard. Click the "Save" button to save the content. You can see the saved content in the "Saved" tab. You can also "like" and "dislike" the generated content.
Run in batches
Sometimes, we need to run an application many times. For example: There is a web application that can generate articles based on topics. Now we want to generate 100 articles on different topics. Then this task has to be done 100 times, which is very troublesome. Also, you have to wait for one task to complete before starting the next one.
In the above scenario, the batch operation function is used, which is convenient to operate (enter the theme into a csv file, only need to be executed once), and also saves the generation time (multiple tasks run at the same time). The usage is as follows:
Step 1 Enter the batch run page
Click the "Run Batch" tab to enter the batch run page.

Step 2 Download the template and fill in the content
Click the Download Template button to download the template. Edit the template, fill in the content, and save as a .csv file.

Step 3 Upload the file and run

If you need to export the generated content, you can click the download "button" in the upper right corner to export as a csv file.
Note: The encoding of the uploaded csv file must be Unicode encoding. Otherwise, the result will fail. Solution: When exporting to a csv file with Excel, WPS, etc., select Unicode for encoding.
Save run results
Click the "Save" button below the generated results to save the running results. In the "Saved" tab, you can see all saved content.

Generate more similar results
If the "more similar" function is turned on when applying the arrangement. Clicking the "more similar" button in the web application generates content similar to the current result. As shown below:

Chat App
Conversational applications use a question-and-answer model to maintain a dialogue with the user. Conversational applications support the following capabilities (please confirm that the following functions are enabled when the application is programmed):
Variables to fill in before the dialog. Create, pin, and delete conversations.
Conversation remarks.
Follow-up.
Speech to text.
Citations and Attributions
Variables filled in before the dialog
If you have the requirement to fill in variables when you apply the layout, you need to fill in the information according to the prompts before entering the dialog window:

Fill in the necessary content and click the "Start Chat" button to start chatting.

Move to the AI's answer, you can copy the content of the conversation, and give the answer "like" and "dislike".

Conversation creation, pinning and deletion
Click the "New Conversation" button to start a new conversation. Move to a session, and the session can be "pinned" and "deleted".

Conversation remarks
If the "Conversation remarks" function is enabled when the application is programmed, the AI application will automatically initiate the first sentence of the dialogue when creating a new dialogue:

Follow-up
If the "Follow-up" function is enabled during the application arrangement, the system will automatically generate 3 related question suggestions after the dialogue:

Speech to text
If the "Speech to Text" function is enabled during application programming, you will see the voice input icon in the input box on the web application side, click the icon to convert the voice input into text:
Please make sure that the device environment you are using is authorized to use the microphone.

Citations and Attributions
If the "Quotations and Attribution" feature is enabled during the application arrangement, the dialogue returns will automatically show the quoted knowledge document sources.
Further Chat App Settings
Chat in explore is a conversational application used to explore the boundaries of Dify's capabilities.
When we talk to large natural language models, we often encounter situations where the answers are outdated or invalid. This is due to the old training data of the large model and the lack of networking capabilities. Based on the large model, Chat uses agents to capabilities and some tools endow the large model with the ability of online real-time query.

Chat supports the use of plugins and knowledge.
Use plugins
LLM(Large language model)cannot be networked and invoke external tools. But this cannot meet the actual usage scenarios, such as:
When we want to know the weather today, we need to be connected to the Internet.
When we want to summarize the content of a web page, we need to use an external tool: read the content of the web page.
The above problem can be solved by using the agent mode: when the LLM cannot answer the user's question, it will try to use the existing plugins to answer the question.
In Dify, we use different proxy strategies for different models. The proxy strategy used by OpenAI's model is GPT function call. Another model used is ReACT. The current test experience is that the effect of GPT function call is better. To know more, you can read the link below:
Function calling and other API updates
ReAct: Synergizing Reasoning and Acting in Language Models
Currently we support the following plugins:
Google Search. The plugin searches Google for answers.
Web Reader. The plugin reads the content of linked web pages.
Wikipedia. The plugin searches Wikipedia for answers.
We can choose the plugins needed for this conversation before the conversation starts.

If you use the Google search plugin, you need to configure the SerpAPI key.

Configured entry:

Use knowledge
Chat supports knowledge. After selecting the knowledge, the questions asked by the user are related to the content of the data set, and the model will find the answer from the data set.
We can select the knowledge needed for this conversation before the conversation starts.

The process of thinking
The thinking process refers to the process of the model using plugins and knowledge. We can see the thought process in each answer.
Prompting Expert Mode
Currently, the orchestration for creating apps in Dify is set to Basic Mode by default. This is ideal for non-tech-savvy individuals who want to quickly make an app. For example, if you want to create a corporate knowledge-base ChatBot or an article summary Generator, you can use the Basic Mode to design Pre-prompt words, add Query, integrate Context, and other straightforward steps to launch a complete app. For more head to 👉 Text Generator and Chat App.
💡However, you surely want to design prompts in a more customized manner if you're a developer who has conducted in-depth research on prompts, then you should opt for the Expert Mode. In this mode, you are granted permission to customize comprehensive prompts rather than using the pre-packaged prompts from Dify. You can modify the built-in prompts, rearrange the placement of Context and History , set necessary parameters, and more. If you're familiar with the OpenAI's Playground, you can get up to speed with this mode more quickly.

Well, before you try the new mode, you should be aware of some essential elements in it:
Complete
When choosing a model, if you see "COMPLETE" on the right side of the model name, it indicates a Text completion model e.g. 
This type of model accepts a freeform text string and generates a text completion, attempting to match any context or pattern you provide. For example, if you write the prompt As René Descartes said, "I think, therefore", it's highly likely that the model will return "I am." as the completion.\
Chat
When choosing a model, if you see "CHAT" on the right side of the model name, it indicates a Chat completions model e.g. 
This type of model takes a list of messages as input and returns a message generated by the model as output. It consists of three message types: SYSTEM, USER, and ASSISTANT.
SYSTEM
System messages help guide the behavior of the AI assistant. For example, you can alter the personality of the AI assistant or provide specific instructions on how it should perform throughout the conversation. System messages are optional. Without system messages, the AI assistant might behave like it's using generic messages such as "you are a helpful assistant."
USER
User messages provide requests or comments for the AI assistant to respond to.
ASSISTANT
Assistant messages store previous assistant responses, but they can also be written by you to provide examples of desired behavior.\
Stop_Sequences
Stop_Sequences refers to specific words, phrases, or characters used to send a signal to LLM to stop generating text.\
Blocks


When users input a query, the app processes the query as search criteria for the knowledge. The organized results from the search then replace the variable Context, allowing the LLM to reference the content for its response.


The query content is only available in the Text completion models of conversational applications. The content entered by the user during the conversation will replace this variable, initiating a new turn of dialogue.


The conversation history is only available in the Text completion model of conversational applications. When engaging in multiple conversations in dialogue applications, Dify will assemble and concatenate the historical dialogue records according to built-in rules and replace the 'Conversation History' variable. The Human and Assistant prefixes can be modified by clicking on the ... after "Conversation History".\
Prompt Template
In this mode, before formal orchestration, an initial template is provided in the prompt box. We can directly modify this template to have more customized requirements for LLM. Different types of applications have variations in different modes.
For more head to 👉 Prompt Template

Comparison of the two modes
Comparison Dimension
Basic Mode
Expert Mode
Visibility of Built-in Prompts
Invisible
Visible
Automatic Design
Available
Disabled
Variable Insertion
Available
Available
Block Validation
Disabled
Available
SYSTEM / USER / ASSISTANT
Invisible
Visible
Context parameter settings
Available
Available
PROMPT LOG
Available
Available
Stop_Sequences
Disabled
Available

Operation Guide
1. How to enter the Expert Mode
After creating an application, you can switch to the Expert Mode on the prompt design page.

Access to the Expert Mode
After modifying the prompts in the Expert Mode and publishing the application, you will not be able to revert back to the Basic Mode.
2. Modify Context parameters
In both two modes, you can modify the parameters for the inserting context, which includes TopK and Score Threshold.

Context parameters
Please note that only after uploading the context, the built-in prompts containing  will be displayed on the prompt design page.
TopK: The value is an integer from 1 to 10.
It is used to filter the text fragments with the highest similarity to the user's query. The system will also dynamically adjust the number of fragments based on the context window size of the selected model. The default system value is 2. This value is recommended to be set between 2 and 5, because we expect to get answers that match the embedded context more closely.\
Score Threshold: The value is a floating-point number from 0 to 1, with two decimal places.
It is used to set the similarity threshold for text segment selection, i.e., it only recalls text chunks that exceed the set score. By default, the system turns this setting off, meaning there's no filtering based on the similarity value of the recalled text chunks. When activated, the default value is 0.7. We recommend keeping this setting deactivated by default. If you have more stringent reply requirements, you can set a higher value, though it's not advisable to set it excessively high.
3. Stop_Sequences
We do not expect the LLM to generate excessive content. Therefore, it's necessary to set specific words, phrases, or characters to signal the LLM to stop generating text. The default setting is Human: .For example, if you've written the Few-Shot below:
Copy
Human1: What color is the sky?



Assistant1: The sky is blue.



Human1: What color is the fire?



Assistant1: The fire is red.



Human1: What color is the soil?



Assistant1:
Then, in the model parameters, you need to locate Stop_Sequences and input Human1:. Do not forget to press the Tab key. In this way, the LLM will only respond with one sentence when replying instead of generating any extra dialogues:
Copy
The soil is yellow.
Because LLM stops generating content before the next Human1:.
4.Use "/" to insert Variables and Blocks
You can enter "/" in the text editor to quickly bring up Blocks to insert into the prompt.

shortcut "/"
Except for Variables, other Blocks cannot be inserted repeatedly. In different applications and models, the Blocks that can be inserted will vary based on different prompt template structures.
5. Input Pre-prompt
The system's initial Prompt Template provides the necessary parameters and requirements for LLM's response. For more head to 👉 Prompt Template.
The core of the early-stage development by developers is the Pre-prompt for the conversation. It requires designing built-in prompts, with the suggested insertion position below:
Copy
When answer to user:

- If you don't know, just say that you don't know.

- If you don't know when you are not sure, ask for clarification.

Avoid mentioning that you obtained the information from the context.

And answer according to the language of the user's question.



You are a customer service assistant for Apple Inc., and you can provide iPhone consultation services to users.

When answering, you need to list the detailed specifications of the iPhone, and you must output this information in a vertical {{Format}} table. If the list is too long, it should be transposed.

You are allowed to take a long time to think to generate a more reasonable output.

Note: You currently have knowledge of only a subset of iPhone models, not all of them.
You can also customize and modify the initial prompt template. For example, if you want LLM's responses to be in English, you can modify the built-in prompts as follows:
Copy
When answer to user:

- If you don't know, just say that you don't know.

- If you don't know when you are not sure, ask for clarification.

Avoid mentioning that you obtained the information from the context.

And answer according to the language English.
6. Check the Prompt Log
During debugging, you can not only check the user's input and LLM's responses but also view the complete prompts by clicking on the icon in the upper-left corner of the send message button. This makes it convenient for developers to confirm whether the input Variable content, Context, Chat History, and Query content meet expectations.
6.1 Access to the Prompt Log
In the debugging preview interface, after engaging in a conversation with the AI, simply move the mouse pointer over any user session, and you will see the "Log" icon button in the upper-left corner. Click on it to view the Prompt Log.

Access to the Prompt Log
In the Prompt Log, we can clearly see:
Complete built-in prompts.
Relevant text snippets referenced in the current session.
Historical session records.

Prompt Log
From the log, we can view the complete prompts that have been assembled by the system and sent to LLM, and we can continuously improve prompt input based on debugging results.
6.2 Trace Debugging History
In the initial application's main interface, you can find "Logs & Ann." in the left-side navigation bar. Clicking on it will allow you to view the complete logs. In the "Logs & Ann." main interface, you can click on any conversation log entry. In the right-side dialog box that appears, simply move the mouse pointer over the conversation and then click the "Log" button to check the Prompt Log.
For more head to 👉 Logs & Annotations .
Prompt Template
In order to meet the more customized requirements of developers for LLM, Dify has fully opened the built-in complete prompts in the Expert Mode and provided initial templates in the composition interface. Below are four initial templates for reference:
1. Using Chat models to build Conversational apps
SYSTEM
Copy
Use the following context as your learned knowledge, inside <context></context> XML tags.



<context>

{{#context#}}

</context>



When answer to user:

- If you don't know, just say that you don't know.

- If you don't know when you are not sure, ask for clarification.

Avoid mentioning that you obtained the information from the context.

And answer according to the language of the user's question.

{{pre_prompt}}
USER
Copy
{{Query}} //Enter the Query variables here.
ASSITANT
Copy
""
Prompt Structure：
Context
Pre-prompt
Query
2. Using Chat models to build Text Generator apps
SYSTEM
Copy
Use the following context as your learned knowledge, inside <context></context> XML tags.



<context>

{{#context#}}

</context>



When answer to user:

- If you don't know, just say that you don't know.

- If you don't know when you are not sure, ask for clarification.

Avoid mentioning that you obtained the information from the context.

And answer according to the language of the user's question.

{{pre_prompt}}
USER
Copy
{{Query}} //Enter the Query variables here.
ASSITANT
Copy
""
Prompt Structure：
Context
Pre-prompt
Query
3. Using Complete models to build Conversational apps
Copy
Use the following context as your learned knowledge, inside <context></context> XML tags.



<context>

{{#context#}}

</context>



When answer to user:

- If you don't know, just say that you don't know.

- If you don't know when you are not sure, ask for clarification.

Avoid mentioning that you obtained the information from the context.

And answer according to the language of the user's question.



{{pre_prompt}}



Here is the chat histories between human and assistant, inside <histories></histories> XML tags.



<histories>

{{#histories#}}

</histories>





Human: {{#query#}}



Assistant:
Prompt Structure：
Context
Pre-prompt
History
Query
4. Using Complete models to build Text Generator apps
Copy
Use the following context as your learned knowledge, inside <context></context> XML tags.



<context>

{{#context#}}

</context>



When answer to user:

- If you don't know, just say that you don't know.

- If you don't know when you are not sure, ask for clarification.

Avoid mentioning that you obtained the information from the context.

And answer according to the language of the user's question.



{{pre_prompt}}

{{query}}
Prompt Structure：
Context
Pre-prompt
Query
Dify has collaborated with some model providers for joint deep optimization of system prompts, and the initial templates for some models may differ from the examples provided above.
Parameter Definitions
Context: Used to insert related text from the knowledge as context into the complete prompts.
Pre-prompt: Pre-prompts arranged in the Basic Mode are inserted into the complete prompts.
History: When building a chat application using text generation models, the system inserts the user's conversation history as context into the complete prompts. Since some models may respond differently to role prefixes, you can also modify the role prefix name in the conversation history settings, for example, changing the name "Assistant" to "AI".
Query: The query content represents variable values used to insert questions that users input during the chat.
Workflow
Introduce
Introduce
Workflow reduces system complexity by breaking complex tasks into smaller steps (nodes), reducing dependence on prompt word technology and model inference capabilities, enhancing the performance of LLM applications for complex tasks, and improving system explainability, stability, and fault tolerance. Dify workflows are divided into two types based on application scenarios:
Chatflow: For conversational scenarios, including customer service, semantic search, and other conversational applications that require multi-step logic in building responses.
Workflow: For automation and batch processing scenarios, suitable for high-quality translation, data analysis, content creation, email automation, etc.
To address the complexity of user intent recognition in natural language inputs, Chatflow provides problem understanding nodes, such as question classification, question rewriting, sub-question splitting, etc. In addition, it will also provide LLM with the ability to interact with the external environment, i.e., tool invocation capability, such as online search, mathematical calculation, weather query, drawing, etc.

To solve complex business logic in automation and batch processing scenarios, Workflow provides a wealth of logic nodes, such as code nodes, IF/ELSE nodes, merge nodes, template conversion nodes, etc. In addition, it will also provide the ability to trigger by time and event, facilitating the construction of automated processes.

Common Cases
Customer Service By integrating LLM into your customer service system, you can automate the answering of common questions, reducing the workload of the support team. LLM can understand the context and intent of customer queries and generate helpful and accurate responses in real-time.
Content Generation Whether you need to create blog posts, product descriptions, or marketing materials, LLM can assist you by generating high-quality content. Just provide an outline or topic, and LLM will use its extensive knowledge base to produce engaging, informative, and well-structured content.
Task Automation Can be integrated with various task management systems, such as Trello, Slack, Lark, to automate project and task management. By using natural language processing, LLM can understand and interpret user inputs, create tasks, update statuses, and assign priorities without manual intervention.
Data Analysis and Reporting Can be used to analyze large datasets and generate reports or summaries. By providing relevant information to LLM, it can identify trends, patterns, and insights, transforming raw data into actionable intelligence. This is especially valuable for businesses that wish to make data-driven decisions.
Email Automation LLM can be used to draft emails, social media updates, and other forms of communication. By providing a brief outline or key points, LLM can generate a well-structured, coherent, and contextually relevant message. This can save a significant amount of time and ensure your responses are clear and professional.
How to Start
Start building from a blank workflow or use system templates to help you start.
Familiarize yourself with basic operations, including creating nodes on the canvas, connecting and configuring nodes, debugging workflows, viewing run history, etc.
Save and publish a workflow.
Run the published application or call the workflow through an API.
Key Concept
Node
Nodes are the key components of a workflow. By connecting nodes with different functionalities, a series of operations within the workflow are executed. Nodes are categorized by type:
Basic Nodes：Start, End, Answer, LLM, Knowledge Retrieval, Applications (coming soon)
Question Understand：Quesition Classifier，Question Rewriting (coming soon), Sub-question Splitting (coming soon)
Logic Processing：IF/ELSE, Merge (coming soon), Loop (coming soon)
Transformation：Code, Template， Variable Assigner, Function Extraction (coming soon)
Others：HTTP Request
Tools：Built-in Tools, Custom Tools
Variables
Variables are crucial for linking the input and output of nodes within a workflow, facilitating the implementation of complex processing logic throughout the process.
Workflows need to define input variables for initiating execution or conversation.
Nodes require input variables for initiation; for instance, the input variable for a question classifier typically consists of the user's question.
Variables referenced within a node can only be those from preceding process nodes to ensure coherence and avoid duplication.
To prevent variable name duplication, node names must be unique.
The output variables of a node are fixed by the system and are not subject to modification.
Differences between Chatflow and Workflow
Application Scenario Differences
Chatflow: Targets conversational scenarios and represents an advanced orchestration mode for Chatbot application types.
Workflow: Geared towards automation and batch processing scenarios.
Differences in Nodes
Node
Chatflow
Workflow
Start
Utilizes system-built variables for user input and file uploads
Utilizes system-built variables for file uploads
End
Not support End node


Uses an End node to output structured text at the conclusion of execution, which is not designed for mid-process output.
Answer
The Answer node is used for streaming output or fixed text replies and can be utilized mid-process.
Not support Answer node
LLM
Memory is automatically enabled to store and pass on the history of multi-turn dialogues.
Not support Memory configuration


Question Classifier
Memory is automatically enabled to store and pass on the history of multi-turn dialogues.
Not Support Memory configuration

Application Entry Division
Chatflow Entry:

Workflow Entry:

Nodes
Start
Defining initial parameters for a workflow process initiation allows for customization at the start node, where you input variables to kick-start the workflow. Every workflow necessitates a start node, acting as the entry point and foundation for the workflow's execution path.

Within the "Start" node, you can define input variables of four types:
Text: For short, simple text inputs like names, identifiers, or any other concise data.
Paragraph: Supports longer text entries, suitable for descriptions, detailed queries, or any extensive textual data.
Dropdown Options: Allows the selection from a predefined list of options, enabling users to choose from a set of predetermined values.
Number: For numeric inputs, whether integers or decimals, to be used in calculations, quantities, identifiers, etc.

Once the configuration is completed, the workflow's execution will prompt for the values of the variables defined in the start node. This step ensures that the workflow has all the necessary information to proceed with its designated processes.

Tip: In Chatflow, the start node provides system-built variables: sys.query and sys.files. sys.query is utilized for user question input in conversational applications, enabling the system to process and respond to user queries. sys.files is used for file uploads within the conversation, such as uploading an image to understand its content. This requires the integration of image understanding models or tools designed for processing image inputs, allowing the workflow to interpret and act upon the uploaded files effectively.
End
Defining the Final Output Content of a Workflow Process. Every workflow needs at least one "End" node to output the final result after full execution.
The "End" node serves as the termination point of the process, beyond which no further nodes can be added. In workflow applications, execution results are only output when the process reaches the "End" node. If the process involves conditional branching, multiple "End" nodes must be defined.
Single-Path Execution Example:

Multi-Path Execution Example:

Answer
Answer
Defining Reply Content in a Chatflow Process. In a text editor, you have the flexibility to determine the reply format. This includes crafting a fixed block of text, utilizing output variables from preceding steps as the reply content, or merging custom text with variables for the response.
Answer node can be seamlessly integrated at any point to dynamically deliver content into the dialogue responses. This setup supports a live-editing configuration mode, allowing for both text and image content to be arranged together. The configurations include:
Outputting the reply content from a Language Model (LLM) node.
Outputting generated images.
Outputting plain text.
Example 1: Output plain text.

Example 2: Output image and LLM reply.


LLM
Invoking a Large Language Model for Question Answering or Natural Language Processing. Within an LLM node, you can select an appropriate model, compose prompts, set the context referenced in the prompts, configure memory settings, and adjust the memory window size.

Configuring an LLM node primarily involves two steps:
Selecting a model
Composing system prompts
Model Configuration
Before selecting a model suitable for your task, you must complete the model configuration in "System Settings—Model Provider". The specific configuration method can be referenced in the model configuration instructions. After selecting a model, you can configure its parameters.

Write Prompts
Within an LLM node, you can customize the model input prompts. If you choose a conversational model, you can customize the content of system prompts, user messages, and assistant messages.
For instance, in a knowledge base Q&A scenario, after linking the "Result" variable from the knowledge base retrieval node in "Context", inserting the "Context" special variable in the prompts will use the text retrieved from the knowledge base as the context background information for the model input.

In the prompt editor, you can bring up the variable insertion menu by typing "/" or "{" to insert special variable blocks or variables from preceding flow nodes into the prompts as context content.

If you opt for a completion model, the system provides preset prompt templates for conversational applications. You can customize the content of the prompts and insert special variable blocks like "Conversation History" and "Context" at appropriate positions by typing "/" or "{", enabling richer conversational functionalities.

Memory Toggle Settings
In conversational applications (Chatflow), the LLM node defaults to enabling system memory settings. In multi-turn dialogues, the system stores historical dialogue messages and passes them into the model. In workflow applications (Workflow), system memory is turned off by default, and no memory setting options are provided.
Memory Window Settings
If the memory window setting is off, the system dynamically passes historical dialogue messages according to the model's context window. With the memory window setting on, you can configure the number of historical dialogue messages to pass based on your needs.
Dialogue Role Name Settings
Due to differences in model training phases, different models adhere to role name commands to varying degrees, such as Human/Assistant, Human/AI, 人类/助手, etc. To adapt to the prompt response effects of multiple models, the system allows setting dialogue role names, modifying the role prefix in conversation history.
Knowledge Retrieval

The Knowledge Base Retrieval Node is designed to query text content related to user questions from the Dify Knowledge Base, which can then be used as context for subsequent answers by the Large Language Model (LLM).

Configuring the Knowledge Base Retrieval Node involves three main steps:
Selecting the Query Variable
Choosing the Knowledge Base for Query
Configuring the Retrieval Strategy
Selecting the Query Variable
In knowledge base retrieval scenarios, the query variable typically represents the user's input question. In the "Start" node of conversational applications, the system pre-sets "sys.query" as the user input variable. This variable can be used to query the knowledge base for text segments most closely related to the user's question.
Choosing the Knowledge Base for Query
Within the knowledge base retrieval node, you can add an existing knowledge base from Dify. For instructions on creating a knowledge base within Dify, please refer to the knowledge base help documentation.
Configuring the Retrieval Strategy
It's possible to modify the indexing strategy and retrieval mode for an individual knowledge base within the node. For a detailed explanation of these settings, refer to the knowledge base help documentation.

Dify offers two recall strategies for different knowledge base retrieval scenarios: "N-choose-1 Recall" and "Multi-way Recall". In the N-choose-1 mode, knowledge base queries are executed through function calling, requiring the selection of a system reasoning model. In the multi-way recall mode, a Rerank model needs to be configured for result re-ranking. For a detailed explanation of these two recall strategies, refer to the retrieval mode explanation in the help documentation.

Question Classifier
Question Classifier node defines the categorization conditions for user queries, enabling the LLM to dictate the progression of the dialogue based on these categorizations. As illustrated in a typical customer service robot scenario, the question classifier can serve as a preliminary step to knowledge base retrieval, identifying user intent. Classifying user intent before retrieval can significantly enhance the recall efficiency of the knowledge base.

Configuring the Question Classifier Node involves three main components:
Selecting the Input Variable
Configuring the Inference Model
Writing the Classification Method
Selecting the Input Variable In conversational customer scenarios, you can use the user input variable from the "Start Node" (sys.query) as the input for the question classifier. In automated/batch processing scenarios, customer feedback or email content can be utilized as input variables.
Configuring the Inference Model The question classifier relies on the natural language processing capabilities of the LLM to categorize text. You will need to configure an inference model for the classifier. Before configuring this model, you might need to complete the model setup in "System Settings - Model Provider". The specific configuration method can be found in the model configuration instructions. After selecting a suitable model, you can configure its parameters.
Writing Classification Conditions You can manually add multiple classifications by composing keywords or descriptive sentences that fit each classification. Based on the descriptions of these conditions, the question classifier can route the dialogue to the appropriate process path according to the semantics of the user's input.
IF/ELSE
The IF/ELSE Node allows you to split a workflow into two branches based on if/else conditions. In this node, you can set one or more IF conditions. When the IF condition(s) are met, the workflow proceeds to the next step under the "IS TRUE" branch. If the IF condition(s) are not met, the workflow triggers the next step under the "IS FALSE" branch.

Code
Navigation
Introduction
Use Cases
Local Deployment
Security Policy
Introduction
The code node supports the execution of Python / NodeJS code to perform data transformations within workflows. It simplifies your workflows, suitable for Arithmetic, JSON transform, text processing, and more scenarios.
This node significantly enhances developers' flexibility, allowing them to embed custom Python or Javascript scripts in their workflows and manipulate variables in ways that preset nodes cannot achieve. Through configuration options, you can specify the required input and output variables and write the corresponding execution code:

Configuration
If you need to use variables from other nodes within the code node, you need to define the variable names in input variables and reference these variables, see Variable Reference for details.
Use Cases
With the code node, you can perform the following common operations:
Structured Data Processing
In workflows, it's often necessary to deal with unstructured data processing, such as parsing, extracting, and transforming JSON strings. A typical example is data processing in the HTTP node, where data might be nested within multiple layers of JSON objects, and we need to extract certain fields. The code node can help you accomplish these tasks. Here's a simple example that extracts the data.name field from a JSON string returned by an HTTP node:
Copy
def main(http_response: str) -> str:

   import json

   data = json.loads(http_response)

   return {

       # do not forget to declare 'result' in the output variables

       'result': data['data']['name']

   }
Mathematical Calculations
When complex mathematical calculations are needed in workflows, the code node can also be used. For example, to calculate a complex mathematical formula or perform some statistical analysis on the data. Here is a simple example that calculates the variance of a list:
Copy
def main(x: list) -> float:

   return {

       # do not forget to declare 'result' in the output variables

       'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)

   }
Data Concatenation
Sometimes, you may need to concatenate multiple data sources, such as multiple knowledge retrievals, data searches, API calls, etc. The code node can help you integrate these data sources. Here's a simple example that merges data from two knowledge bases:
Copy
def main(knowledge1: list, knowledge2: list) -> list:

   return {

       # do not forget to declare 'result' in the output variables

       'result': knowledge1 + knowledge2

   }
Local Deployment
If you are a user deploying locally, you need to start a sandbox service, which ensures that malicious code is not executed. Also, launching this service requires Docker, and you can find specific information about the Sandbox service here. You can also directly start the service using docker-compose
Copy
docker-compose -f docker-compose.middleware.yaml up -d
Security Policy
The execution environment is sandboxed for both Python and Javascript, meaning that certain functionalities that require extensive system resources or pose security risks are not available. This includes, but is not limited to, direct file system access, network calls, and operating system-level commands.
Template
Template lets you dynamically format and combine variables from previous nodes into a single text-based output using Jinja2, a powerful templating syntax for Python. It's useful for combining data from multiple sources into a specific structure required by subsequent nodes. The simple example below shows how to assemble an article by piecing together various previous outputs:

Beyond naive use cases, you can create more complex templates as per Jinja's documentation for a variety of tasks. Here's one template that structures retrieved chunks and their relevant metadata from a knowledge retrieval node into a formatted markdown:
Copy
{% for item in chunks %}

### Chunk {{ loop.index }}. 

### Similarity: {{ item.metadata.score | default('N/A') }}



#### {{ item.title }}



##### Content

{{ item.content | replace('\n', '\n\n') }}



---

{% endfor %}

This template node can then be used within a Chatflow to return intermediate outputs to the end user, before a LLM response is initiated.
The Answer node in a Chatflow is non-terminal. It can be inserted anywhere to output responses at multiple points within the flow.
Variable Assigner
The Variable Assigner node serves as a hub for collecting branch outputs within the workflow, ensuring that regardless of which branch is taken, the output can be referenced by a single variable. The output can subsequently be manipulated by nodes downstream.

Variable Assigner supports multiple types of output variables including String,Number, Object, and Array. Given the specified output type, you may add input variables from the dropdown list of variables to the node. The list of variables is derived from previous branch outputs and autofiltered based on the specified type.

Variable Assigner gives a single output variable of the specified type for downstream use.
HTTP Request
HTTP Request node lets you craft and dispatch HTTP requests to specified endpoints, enabling a wide range of integrations and data exchanges with external services. The node supports all common HTTP request methods, and lets you fully customize over the URL, headers, query parameters, body content, and authorization details of the request.

A really handy feature with HTTP request is the ability to dynamically construct the request by inserting variables in different fields. For instance, in a customer support scenario, variables such as username or customer ID can be used to personalize automated responses sent via a POST request, or retrieve individual-specific information related to the customer.The HTTP request returns body, status_code, headers, and files as outputs. If the response includes files of MIME types (currently limited to images), the node automatically saves these as files for downstream use.
Tools
Within a workflow, Dify provides both built-in and customizable tools. Before utilizing these tools, you need to "authorize" them. If the built-in tools do not meet your requirements, you can create custom tools within "Dify—Tools".

Configuring a tool node generally involves two steps:
Authorizing the Tool/Creating Custom Tools
Configuring Tool Inputs and Parameters
For guidance on creating custom tools and configuring them, please refer to the tool configuration instructions.
Preview&Run
Preview&Run
Dify Workflow offers a comprehensive set of execution and debugging features. In conversational applications, clicking "Preview" enters debugging mode.

In workflow applications, clicking "Run" enters debugging mode.

Once in debugging mode, you can debug the configured workflow using the interface on the right side of the screen.

Step Test
Workflow supports step-by-step debugging of nodes, where you can repetitively test whether the execution of the current node meets expectations.

After running a step test, you can review the execution status, input/output, and metadata information.

Log
Log
Clicking "View Log—Details" allows you to see a comprehensive overview of the run, including information on input/output, metadata, and more, in the details section.

Clicking "View Log—Trace" enables you to review the input/output, token consumption, runtime duration, etc., of each node throughout the complete execution process of the workflow.

Checklist
Checklist
Before entering debug mode, you can check the checklist to see if there are any nodes with incomplete configurations or that have not been connected.

History
History
In the "Run History," you can view the run results and log information from the historical debugging of the current workflow.

Publish
After completing debugging, clicking "Publish" in the upper right corner allows you to save and quickly release the workflow as different types of applications.

Conversational applications can be published as:
Run App
Embed into Site
Access API Reference
Workflow applications can be published as:
Run App
Batch Run App
Access API Reference
You can also click "Restore" to preview the last published version of the application. Confirming the restore will use the last published workflow version to overwrite the current workflow version.
Export/Import
You can export/import application templates as YAML-format DSL (Domain Specific Language) files within the studio to share applications with your team members.
To import a DSL file in the studio application list:

To export a DSL file from the studio application list:

To export a DSL file from the workflow orchestration page:

RAG (Retrieval Augmented Generation)
The concept of the RAG
The RAG architecture, with vector retrieval at its core, has become the leading technological framework for addressing two major challenges of large models: acquiring the latest external knowledge and mitigating issues of generating hallucinations. This architecture has been widely implemented in numerous practical application scenarios.
Developers can utilize this technology to cost-effectively build AI-powered customer service bots, corporate knowledge bases, AI search engines, etc. These systems interact with various forms of organized knowledge through natural language input. A representative example of a RAG application is as follows:
In the diagram below, when a user asks, "Who is the President of the United States?", the system doesn't directly relay the question to the large model for an answer. Instead, it first conducts a vector search in a knowledge base (like Wikipedia, as shown in the diagram) for the user's query. It finds relevant content through semantic similarity matching (for instance, "Biden is the current 46th President of the United States…"), and then provides the user's question along with the found knowledge to the large model. This enables the model to have sufficient and complete knowledge to answer the question, thereby yielding a more reliable response.

Basic Architecture of RAG
Why is this necessary?
We can liken a large model to a super-expert, knowledgeable in various human domains. However, this expert has its limitations; for example, it doesn't know your personal situation, as such information is private and not publicly available on the internet, and therefore, it hasn't had the opportunity to learn it beforehand.
When you want to hire this super-expert as your family financial advisor, you need to allow them to review your investment records, household expenses, and other relevant data before they can respond to your inquiries. This enables them to provide professional advice tailored to your personal circumstances.
This is what the RAG system does: it helps the large model temporarily acquire external knowledge it doesn't possess, allowing it to search for answers before responding to a question.
Based on this example, it's evident that the most critical aspect of the RAG system is the retrieval of external knowledge. The expert's ability to provide professional financial advice depends on accurately finding the necessary information. If the expert retrieves information unrelated to financial investments, like a family weight loss plan, even the most capable expert would be ineffective.
Hybrid Search
Why is Hybrid Search Necessary?
The mainstream method in the RAG retrieval phase is Vector Search, which is based on semantic relevance matching. The technical principle involves initially dividing documents from external knowledge bases into semantically complete paragraphs or sentences, and then converting them through a process known as embedding into a series of numerical expressions (multidimensional vectors) that computers can understand. The user's query undergoes a similar conversion.
Computers can detect subtle semantic correlations between user queries and sentences. For example, the semantic relevance between "a cat chases a mouse" and "a kitten hunting a mouse" is higher than between "a cat chases a mouse" and "I like to eat ham." After identifying the text with the highest relevance, the RAG system provides it as context alongside the user's query to the large model, aiding in answering the question.
Besides complex semantic text searches, Vector Search has other advantages:
Understanding of similar semantics (e.g., mouse/mousetrap/cheese, Google/Bing/search engine)
Multilingual comprehension (e.g., matching Chinese input with English content)
Multimodal understanding (supports matching text, images, audio, and video)
Fault tolerance (handles spelling mistakes, vague descriptions)
However, Vector Search might underperform in certain scenarios, like:
Searching names of people or objects (e.g., Elon Musk, iPhone 15)
Searching acronyms or short phrases (e.g., RAG, RLHF)
Searching IDs (e.g., gpt-3.5-turbo, titan-xlarge-v1.01)
These limitations are precisely where traditional keyword search excels, being adept at:
Precise matching (e.g., product names, personal names, product numbers)
Matching a small number of characters (vector search performs poorly with few characters, but users often input just a few keywords)
Matching low-frequency vocabulary (which often carries more significant meanings, like in “Do you want to go for a coffee with me?”, words like “drink” and “coffee” carry more weight than “you”, “want”, “me”)
In most text search scenarios, it's crucial to ensure that the most relevant results appear in the candidates. Vector and keyword searches each have their strengths in the search domain. Hybrid Search combines the advantages of both techniques while compensating for their respective shortcomings.
In Hybrid Search, vector and keyword indices are pre-established in the database. Upon user query input, the system searches for the most relevant text in documents using both search methods.

Hybrid Search
"Hybrid Search" doesn't have a definitive definition; this article exemplifies it as a combination of Vector Search and Keyword Search. However, the term can also apply to other combinations of search algorithms. For instance, we could combine knowledge graph technology, used for retrieving entity relationships, with Vector Search.
Different search systems each excel at uncovering various subtle connections within texts (paragraphs, sentences, words), including precise relationships, semantic relationships, thematic relationships, structural relationships, entity relationships, temporal relationships, and event relationships. It's safe to say that no single search mode is suitable for all scenarios. Hybrid Search, by integrating multiple search systems, achieves a complementarity among various search technologies.
Vector Search
Definition: Vector Search involves generating query embeddings and then searching for text chunks that most closely match these embeddings in terms of vector representation.

Settings for Vector Search
TopK: This setting is used to filter text chunks that have the highest similarity to the user's query. The system also dynamically adjusts the number of chunks based on the context window size of the selected model. The default value for this setting is 3.
Score Threshold: This setting is used to establish a similarity threshold for the selection of text chunks. It means that only text chunks exceeding the set score are recalled. By default, this setting is turned off, meaning that the system does not filter the similarity values of the recalled text chunks. When activated, the default value is set to 0.5.
Rerank Model: After configuring the Rerank model's API key on the "Model Provider" page, you can enable the "Rerank Model" in the search settings. The system then performs a semantic re-ranking of the document results that have been recalled after semantic search, optimizing the order of these results. Once the Rerank model is set up, the TopK and Score threshold settings are only effective in the Rerank step.
Full-Text Search
Definition: Full-Text Search involves indexing all the words in a document, enabling users to query any term and retrieve text chunks that contain these terms.

Settings for Full-Text Search
TopK: This setting is utilized to select text chunks that most closely match the user's query in terms of similarity. The system also dynamically adjusts the number of chunks based on the context window size of the chosen model. The default value for TopK is set at 3.
Rerank Model: After configuring the API key for the Rerank model on the "Model Provider" page, you can activate the "Rerank Model" in the search settings. The system will then perform a semantic re-ranking of the document results retrieved through full-text search, optimizing the order of these results. Once the Rerank model is configured, the TopK and any Score threshold settings will only be effective during the Rerank step.
Hybrid Search
Hybrid Search operates by concurrently executing Full-Text Search and Vector Search. It then applies a re-ranking step to choose the best results that match the user's query from both types of search results. To effectively use this feature, it is necessary to configure the Rerank Model API.

Settings for Hybrid Search
TopK: This setting is used for filtering text chunks that have the highest similarity to the user's query. The system will dynamically adjust the number of chunks based on the context window size of the model in use. The default value for TopK is set at 3.
Rerank Model: After configuring the Rerank model's API key on the "Model Supplier" page, you can enable the "Rerank Model" in the search settings. The system will perform a semantic re-ranking of the document results retrieved through hybrid search, thereby optimizing the order of these results. Once the Rerank model is set up, the TopK and any Score threshold settings are only applicable during the Rerank step.
Setting the Search Mode When Creating a Knowledge
To set the search mode when creating a knowledge base, navigate to the "Knowledge -> Create Knowledge" page. There, you can configure different search modes in the retrieval settings section.

Setting the Search Mode When Creating a Knowledge base
Modifying the Search Mode in Prompt Engineering
You can modify the search mode during application creation by navigating to the "Prompt Engineering -> Context -> Select Knowledge -> Settings" page. This allows for adjustments to different search modes within the prompt arrangement phase.

Modifying the Search Mode in Prompt Engineering
Rerank
Why is Rerank Necessary?
Hybrid Search combines the advantages of various search technologies to achieve better recall results. However, results from different search modes need to be merged and normalized (converting data into a uniform standard range or distribution for better comparison, analysis, and processing) before being collectively provided to the large model. This necessitates the introduction of a scoring system: Rerank Model.
The Rerank Model works by reordering the list of candidate documents based on their semantic match with the user's question, thus improving the results of semantic sorting. It does this by calculating a relevance score between the user's question and each candidate document, returning a list of documents sorted by relevance from high to low. Common Rerank models include Cohere rerank, bge-reranker, and others.

Hybrid Search + Rerank
In most cases, there is an initial search before rerank because calculating the relevance score between a query and millions of documents is highly inefficient. Therefore, rerank is typically placed at the end of the search process, making it very suitable for merging and sorting results from different search systems.
However, rerank is not only applicable to merging results from different search systems. Even in a single search mode, introducing a rerank step can effectively improve the recall of documents, such as adding semantic rerank after keyword search.
In practice, apart from normalizing results from multiple queries, we usually limit the number of text chunks passed to the large model before providing the relevant text chunks (i.e., TopK, which can be set in the rerank model parameters). This is done because the input window of the large model has size limitations (generally 4K, 8K, 16K, 128K Token counts), and you need to select an appropriate segmentation strategy and TopK value based on the size limitation of the chosen model's input window.
It should be noted that even if the model's context window is sufficiently large, too many recalled chunks may introduce content with lower relevance, thus degrading the quality of the answer. Therefore, the TopK parameter for rerank is not necessarily better when larger.
Rerank is not a substitute for search technology but an auxiliary tool to enhance existing search systems. Its greatest advantage is that it not only offers a simple and low-complexity method to improve search results but also allows users to integrate semantic relevance into existing search systems without the need for significant infrastructure modifications.
How to Obtain the Cohere Rerank Model?
Visit https://cohere.com/rerank, register on the page, and apply for usage rights for the Rerank model to obtain the API key.
Setting the Rerank Model in Knowledge Search Mode
Access the Rerank settings by navigating to “Knowledge -> Create Knowledge -> Retrieval Settings”. Besides setting Rerank during knowledge creation, you can also modify the Rerank configuration in the settings of an already created knowledge base, and change the Rerank configuration in the knowledge recall mode settings of application arrangement.

Setting the Rerank Model in Knowledge Search Mode
TopK: Used to set the number of relevant documents returned after Rerank.
Score Threshold: Used to set the minimum score for relevant documents to be returned after Rerank. After setting the Rerank model, the TopK and Score threshold settings are only effective in the Rerank step.
Setting the Rerank Model in Multi-path retrieval
Recall Mode Enable the Rerank model by setting it to Multi-path retrieval mode in the “Prompt Engineering -> Context -> Settings” page.
Explanation of Multi-path retrieval Mode: 🔗

Setting the Rerank Model in Multi-path retrieval
Retrieval
When users build knowledge base Q&A AI applications, if multiple knowledge bases are associated within the application, Dify supports two retrieval modes: N-to-1 retrieval and Multi-path retrieval.

Retrieval Settings
Retrieval Settings
N-to-1 Retrieval
Based on user intent and knowledge description, the Agent independently determines and selects the most matching single knowledge base for querying relevant text. This mode is suitable for applications with distinct knowledge and a smaller number of knowledge bases. N-to-1 retrieval relies on the model's inference capability to choose the most relevant knowledge base based on user intent. When inferring the knowledge, the knowledge serves as a tool for the Agent, chosen through intent inference; the tool description is essentially the knowledge description.
When users upload knowledge, the system automatically creates a summary description of each knowledge base. To achieve the best retrieval results in this mode, you can view the system-generated summary description under “Knowledge -> Settings -> Knowledge Description” and check if this content clearly summarizes the knowledge's content.
Here is the technical flowchart for N-to-1 retrieval:

N-to-1 Retrieval
Therefore, this mode's recall effectiveness can be impacted when there are too many knowledge bases or when the knowledge descriptions lack sufficient distinction. This mode is more suitable for applications with fewer knowledge bases.
Tip: OpenAI Function Call already supports multiple tool calls, and Dify plans to upgrade this mode to "N-to-M retrieval" in future versions.
Multi-path Retrieval
Based on user intent, this mode matches all knowledge bases simultaneously, queries relevant text chunks from multiple knowledge bases, and after a re-ranking step, selects the best results matching the user's question from the multi-path query results. Configuring the Rerank model API is required. In Multi-path retrieval mode, the search engine retrieves text content related to the user's query from all knowledge bases associated with the application, merges the results from multi-path recall, and re-ranks the retrieved documents semantically using the Rerank model.
In Multi-path retrieval mode, configuring the Rerank model is necessary. How to configure the Rerank model: 🔗
Here is the technical flowchart for Multi-path retrieval:

Multi-path retrieval
As Multi-path retrieval does not rely on the model's inferencing capability or knowledge descriptions, this mode can achieve higher quality recall results in multi-knowledge searches. Additionally, incorporating the Rerank step can effectively improve document recall. Therefore, when creating a knowledge base Q&A application associated with multiple knowledge bases, we recommend configuring the retrieval mode as Multi-path retrieval.
Knowledge Import
Most language models use outdated training data and have length limitations for the context of each request. For example, GPT-3.5 is trained on corpora from 2021 and has a limit of approximately 4k tokens per request. This means that developers who want their AI applications to be based on the latest and private context conversations must use techniques like embedding.
Dify' knowledge feature allows developers (and even non-technical users) to easily manage knowledge and automatically integrate them into AI applications. All you need to do is prepare text content, such as:
Long text content (TXT, Markdown, DOCX, HTML, JSONL, or even PDF files)
Structured data (CSV, Excel, etc.)
Additionally, we are gradually supporting syncing data from various data sources to knowledge, including:
GitHub
Databases
Webpages
...
Practice: If your company wants to build an AI customer service assistant based on existing knowledge bases and product documentation, you can upload the documents to a knowledge base in Dify and create a conversational application. This might have taken you several weeks in the past and been difficult to maintain continuously.
Knowledge and Documents
In Dify, knowledge bases are collections of documents. A knowledge base can be integrated as a whole into an application to be used as context. Documents can be uploaded by developers or operations staff, or synced from other data sources (typically corresponding to a file unit in the data source).
Steps to upload a document:
Upload your file, usually a long text file or a spreadsheet
Segment, clean, and preview
Dify submits it to the LLM provider for embedding as vector data and storage
Set metadata for the document
Ready to use in the application!
How to write a good knowledge description
When multiple knowledge bases are referenced in an application, AI uses the description of the knowledge and the user's question to determine which knowledge base to use to answer the user's question. Therefore, a well-written knowledge description can improve the accuracy of AI in selecting knowledge.
The key to writing a good knowledge description is to clearly describe the content and characteristics of the knowledge. It is recommended that the knowledge description begin with this: Useful only when the question you want to answer is about the following: specific description. Here is an example of a real estate knowledge description:
Useful only when the question you want to answer is about the following: global real estate market data from 2010 to 2020. This data includes information such as the average housing price, property sales volume, and housing types for each city. In addition, this knowledge base also includes some economic indicators such as GDP and unemployment rate, as well as some social indicators such as population and education level. These indicators can help analyze the trends and influencing factors of the real estate market. With this data, we can understand the development trends of the global real estate market, analyze the changes in housing prices in various cities, and understand the impact of economic and social factors on the real estate market.
Create a knowledge
Click on knowledge in the main navigation bar of Dify. On this page, you can see the existing knowledge bases. Click on "Create Knowledge" to enter the creation wizard.
If you have already prepared your files, you can start by uploading the files.
If you haven't prepared your documents yet, you can create an empty knowledge base first.
Uploading Documents By upload file
Select the file you want to upload.We support batch uploads
Preview the full text
Perform segmentation and cleaning
Wait for Dify to process the data for you; this step usually consumes tokens in the LLM provider
Text Preprocessing and Cleaning
Text Preprocessing and cleaning refers to Dify automatically segmenting and vectorizing your data documents so that user's questions (input) can match relevant paragraphs (Q to P), and generate results.
When uploading a knowledge base, you need to select a indexing mode to specify how data is matched. This affects the accuracy of AI replies.
In High Quality mode, OpenAI's embedding API is used for higher accuracy in user queries.
In Economic mode, offline vector engines, keyword indexing etc. are used to reduce costs at the expense of lower accuracy.
In Segmenting in Question & Answer format, instead of normal "Q to P" (question matches paragraphs), it uses "Q to Q" (question matches question) matching. After segmentation, Q&A pairs are generated for each passage. When users ask questions, the system finds the most similar question and returns the corresponding passage as the answer. This is more precise because it directly matches the user's question and retrieves the information they need.
Questions have complete syntax while keywords lack semantics and context. So Q to Q improves clarity and handles similar high-frequency questions better.

In Segmenting in Question & Answer format, the text is summarized into multiple QA pairs

The difference between Q to P and Q to Q indexing modes
Modify Documents
Modify Documents For technical reasons, if developers make the following changes to documents, Dify will create a new document for you, and the old document will be archived and deactivated:
Adjust segmentation and cleaning settings
Re-upload the file
Dify support customizing the segmented and cleaned text by adding, deleting, and editing paragraphs. You can dynamically adjust your segmentation to make your knowledge more accurate. Click Document --> paragraph --> Edit in the knowledge to modify paragraphs content and custom keywords. Click Document --> paragraph --> Add segment --> Add a segment to manually add new paragraph. Or click Document --> paragraph --> Add segment --> Batch add to batch add new paragraph.

Edit

add
Disabling and Archiving of Documents
Disable, cancel disable: The knowledge supports disabling documents or chunks that you temporarily do not want indexed. In the knowledge's document list, click the Disable button and the document will be disabled. You can also click the Disable button in the document details to disable the entire document or a segment. Disabled documents will not be indexed. To cancel the disable, click Enable on a disabled document.
Archive, Unarchive: Some unused old document data can be archived if you don't want to delete it. After archiving, the data can only be viewed or deleted, not edited. In the document list of the knowledge, click the Archive button to archive the document. Documents can also be archived in the document details page. Archived documents will not be indexed. Archived documents can also be unarchived by clicking the Unarchive button.
Maintain Knowledge via API
Head to Maintain Knowledge Via Api.
Knowledge Settings
Click Settings in the left navigation of the knowledge. You can change the following settings for the knowledge:
Knowledge name for identifying a knowledge base
Knowledge description to allow AI to better use the knowledge appropriately. If the description is empty, Dify's automatic indexing strategy will be used.
Permissions can be set to Only Me or All Team Members. Those without permissions cannot view and edit the knowledge.
Indexing mode: In High Quality mode, OpenAI's embedding interface will be called to process and provide higher accuracy when users query. In Economic mode, offline vector engines, keyword indexing, etc. will be used to reduce accuracy without consuming tokens.
Note: Upgrading the indexing mode from Economic to High Quality will incur additional token consumption. Downgrading from High Quality to Economic will not consume tokens.
Integrate into Applications
Once the knowledge base is ready, it needs to be integrated into the application. When the AI application processes will automatically use the associated knowledge content as a reference context.
Go to the application - Prompt Arrangement page
In the context options, select the knowledge you want to integrate
Save the settings to complete the integration
Q&A
Q: What should I do if the PDF upload is garbled?
A: If your PDF parsing appears garbled under certain formatted contents, you could consider converting the PDF to Markdown format, which currently offers higher accuracy, or you could reduce the use of images, tables, and other formatted content in the PDF. We are researching ways to optimize the experience of using PDFs.
Q: How does the consumption mechanism of context work? A: With a knowledge base added, each query will consume segmented content (currently embedding two chunks) + question + prompt + chat history combined. However, it will not exceed model limitations, such as 4096.
Q: Where does the embedded knowledge appear when asking questions? A: It will be embedded as context before the question.
Q: Is there any priority between the added knowledge and OpenAI's answers? A: The knowledge serves as context and is used together with questions for LLM to understand and answer; there is no priority relationship.
Q: Why can I hit in test but not in application? A: You can troubleshoot issues by following these steps:
Make sure you have added text on the prompt page and clicked on the save button in the top right corner.
Test whether it responds normally in the prompt debugging interface.
Try again in a new WebApp session window.
Optimize your data format and quality. For practice reference, visit https://github.com/langgenius/dify/issues/90 If none of these steps solve your problem, please join our community for help.
Q: Will APIs related to hit testing be opened up so that dify can access knowledge bases and implement dialogue generation using custom models? A: We plan to open up Webhooks later on; however, there are no current plans for this feature. You can achieve your requirements by connecting to any vector database.
Q: How do I add multiple knowledge bases? A: Due to short-term performance considerations, we currently only support one knowledge base. If you have multiple sets of data, you can upload them within the same knowledge base for use.
Sync from Notion
Dify knowledge supports importing from Notion and setting up Sync so that data is automatically synced to Dify after updates in Notion.
Authorization verification
When creating a knowledge base, select the data source, click Sync from Notion--Go to connect, and complete the authorization verification according to the prompt.
You can also: click Settings--Data Sources--Add a Data Source, click Notion Source Connect to complete authorization verification.

Connect Notion
Import Notion data
After completing authorization verification, go to the knowledge creation page, click Sync from Notion, and select the required authorization page to import.
Segmentation and cleaning
Next, select your segmentation settings and indexing method, save and process. Wait for Dify to process this data, usually this step requires token consumption in LLM providers. Dify not only supports importing ordinary page types but also summarizes and saves the page attributes under the database type.
Note: Images and files are not currently supported for import. Table data will be converted to text.
Sync Notion data
If your Notion content has been modified, you can click Sync directly on the Dify knowledge document list page to sync the data with one click(Please note that each time you click, the current content will be synchronized). This step requires token consumption.

Sync Notion data
(Community Edition) Notion Integration Configuration Guide
Notion integration is divided into two ways: internal integration and public integration . It can be configured in Dify on demand.
For the specific differences between the two integration methods, please refer to the official doc of Notion.
1. Use internal integration
Create an integration in your integration's settings page. By default, all integrations start with an internal integration; internal integrations will be associated with a workspace of your choice, so you need to be the workspace owner to create an integration.
Specific operation steps:
Click the " New integration " button, the type is Internal by default (cannot be modified), select the associated space, enter the name and upload the logo, and click "Submit" to create the integration successfully.

Once the integration is created, you can update its settings as needed under the Capabilities tab and click the "Show" button under Secrets and then copy the Secrets.

Copy it and back to the Dify source code , in the .env file configuration related environment variables, environment variables as follows:
NOTION_INTEGRATION_TYPE = internal or NOTION_INTEGRATION_TYPE = public
NOTION_INTERNAL_SECRET=you-internal-secret
2. Use public integration
You need to upgrade the internal integration to public integration , navigate to the integrated Distribution page, and toggle the switch to expose the integration.
To toggle the switch to public settings, you need to fill in additional information in the Organization Information form below, including your company name, website, and Retargeting URL, and click the "Submit" button.

After your integration has been successfully made public in your integration’s settings page, you will be able to access the integration’s secrets in the Secrets tab.

Back to the Dify source code , in the .env file configuration related environment variables , environment variables as follows:
NOTION_INTEGRATION_TYPE=public
NOTION_CLIENT_SECRET=you-client-secret
NOTION_CLIENT_ID=you-client-id
Once configured, you will be able to utilize Notion data import and sync functions in the knowledge section.
Maintain Knowledge Via Api
Authentication, invocation method and application Service API remain consistent. The difference is that a knowledge API token can operate on all knowledge bases.
Benefits of Using the Knowledge API
Sync your data systems to Dify knowledge to create powerful workflows.
Provide knowledge list and document list APIs as well as detail query interfaces, to facilitate building your own data management page.
Support both plain text and file uploads/updates documents, as well as batch additions and modifications, to simplify your sync process.
Reduce manual document handling and syncing time, improving visibility of Dify's software and services.
How to use
Please go to the knowledge page, you can switch tap to the API page in the navigation on the left side. On this page, you can view the API documentation provided by Dify and manage credentials for accessing the Knowledge API.

Knowledge API Document
Create Empty Knowledge
POST /datasets
Used only to create an empty dataset
Copy
curl --location --request POST 'https://api.dify.ai/v1/datasets' \

--header 'Authorization: Bearer {api_key}' \

--header 'Content-Type: application/json' \

--data-raw '{"name": "name"}'

List of Knowledge
Copy
curl --location --request GET 'https://api.dify.ai/v1/datasets?page=1&limit=20' \

--header 'Authorization: Bearer {api_key}'

Create A Document From Text
Copy
curl --location --request POST '<https://api.dify.ai/v1/datasets/<uuid:dataset_id>/document/create_by_text>' \\

--header 'Authorization: Bearer {api_key}' \\

--header 'Content-Type: application/json' \\

--data-raw '{

   "name": "Dify",

   "text": "Dify means Do it for you...",

   "indexing_technique": "high_quality",

   "process_rule": {

       "rules": {

               "pre_processing_rules": [{

                       "id": "remove_extra_spaces",

                       "enabled": true

               }, {

                       "id": "remove_urls_emails",

                       "enabled": true

               }],

               "segmentation": {

                       "separator": "###",

                       "max_tokens": 500

               }

       },

       "mode": "custom"

   }

}'

Create A Document From File
Copy
curl --location POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_file' \

--header 'Authorization: Bearer {api_key}' \

--form 'data="{

	"name": "Dify",

	"indexing_technique": "high_quality",

	"process_rule": {

		"rules": {

			"pre_processing_rules": [{

				"id": "remove_extra_spaces",

				"enabled": true

			}, {

				"id": "remove_urls_emails",

				"enabled": true

			}],

			"segmentation": {

				"separator": "###",

				"max_tokens": 500

			}

		},

		"mode": "custom"

	}

   }";

   type=text/plain' \

--form 'file=@"/path/to/file"'

Get Document Embedding Status
Copy
curl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{batch}/indexing-status' \

--header 'Authorization: Bearer {api_key}'
Delete Document
Copy
curl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}' \

--header 'Authorization: Bearer {api_key}'
Get Document List
Copy
curl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents' \

--header 'Authorization: Bearer {api_key}'

Add New Segment
Copy
curl 'https://api.dify.ai/v1/datasets/aac47674-31a8-4f12-aab2-9603964c4789/documents/2034e0c1-1b75-4532-849e-24e72666595b/segment' \

 --header 'Authorization: Bearer {api_key}' \

 --header 'Content-Type: application/json' \

 --data-raw $'"segments":[

 {"content":"Dify means Do it for you",

 "keywords":["Dify","Do"]

 }

 ]'

 --compressed

Error Message
document_indexing，document is in indexing status
provider_not_initialize， Embedding model is not configured
not_found，document not exist
dataset_name_duplicate ，have existing knowledge name
provider_quota_exceeded，The model quota has exceeded the limit
dataset_not_initialized，The knowledge has not been initialized
unsupported_file_type，Unsupported file type
support file type：txt, markdown, md, pdf, html, htm, xlsx, docx, csv
too_many_files，The number of files is too large, and only single file upload is temporarily supported
file_too_large，The file is too large, supporting files under 15M

External Data Tool
Previously, Knowledge Import allowed developers to directly upload long texts in various formats and structured data to build knowledge, enabling AI applications to converse based on the latest context uploaded by users. With this update, the external data tool empowers developers to use their own search capabilities or external data such as internal knowledge bases as the context for LLMs. This is achieved by extending APIs to fetch external data and embedding it into Prompts. Compared to uploading knowledge to the cloud, using external data tools offers significant advantages in ensuring the security of private data, customizing searches, and obtaining real-time data.
What does it do?
When end-users make a request to the conversational system, the platform backend triggers the external data tool (i.e., calling its own API), which queries external information related to the user's question, such as employee profiles, real-time records, etc. The tool then returns through the API the portions relevant to the current request. The platform backend will assemble the returned results into text as context injected into the Prompt, in order to produce replies that are more personalized and meet user needs more accurately.
Quick Start
Before using the external data tool, you need to prepare an API and an API Key for authentication. Head to External_data_tool.
Dify offers centralized API management; After adding API extension configurations in the settings interface, they can be directly utilized across various applications on Dify.

API-based Extension

Taking "Query Weather" as an example, enter the name, API endpoint, and API Key in the "Add New API-based Extension" dialog box. After saving, we can then call the API.

Weather Inquiry
On the prompt orchestration page, click the "+ Add" button to the right of "Tools," and in the "Add Tool" dialog that opens, fill in the name and variable name (the variable name will be referenced in the Prompt, so please use English), as well as select the API-based extension added in Step 2.

External_data_tool
In the prompt orchestration box, we can assemble the queried external data into the Prompt. For instance, if we want to query today's weather in London, we can add a variable named location, enter "London", and combine it with the external data tool's extension variable name weather_data. The debug output would be as follows:

Weather_search_tool
In the Prompt Log, we can also see the real-time data returned by the API:

Prompt Log
Annotation Reply
Feature Overview
The Annotation Reply feature offers tailored, high-quality replies for various applications, achieved through manual annotation.
Key Uses:
Specialized Replies for Specific Sectors: This is particularly valuable in customer service or knowledge bases within business, government, etc. It allows for precise answers to specific questions by annotating replies, such as setting "standard annotations" for some or marking others as "unanswerable."
Quick Adaptation for Prototypes: Utilizing Annotation Reply can significantly improve reply quality in the rapid development of prototype products, enhancing customer satisfaction.
How It Works:
The feature provides an alternative system for enhancing retrieval, skipping the generation phase of Large Language Models (LLMs) and avoiding the complications of Retrieval-Augmented Generation (RAG).
Once activated, you can annotate LLM dialogue replies. Annotations can either be high-quality answers taken directly from the LLM or your own edited annotations. These annotated contents are saved for future use.
When similar questions are asked again, the system identifies matching annotated questions.
If a match is found, the annotated answer is returned directly, bypassing LLM or RAG processes.
Without a match, the query follows the standard LLM or RAG process.
Deactivating Annotation Reply ceases matching replies from the annotations.

Annotation Reply Process
Activation
Navigate to “Build Apps -> Add Feature” to enable the Annotation Reply feature.

Start by setting the parameters for Annotation Reply. These include the Score threshold and the Embedding model.
Score Threshold: Sets the minimum similarity score for an annotation to be considered a match and recalled.
Embedding Model: Used for converting annotated text into vectors. Changing the model leads to re-creation of embeddings.
Select 'Save' for immediate application of these settings. The system then creates and stores embeddings for all existing annotations.

Adding Annotations in Debug Mode
Annotations can be added or modified directly on the model's replies within the debug and preview interface.

Edit and save these replies to ensure high quality.

When a user repeats a query, the system uses the relevant saved annotation for a direct reply.

Enabling Annotations in System Logs
Turn on the Annotation Reply feature under “Build Apps -> Logs and Annotations -> Annotations.”

Adjusting Backend Parameters for Annotations
Parameter Settings: These include the Score threshold and Embedding model, just as in the initial configuration.

Bulk Importing Annotated Q&As
Import Process: Use the provided template to format Q&A pairs for annotations, then upload them in bulk.

Bulk Exporting Annotated Q&As
Export Function: This feature allows for a one-time export of all annotated Q&A pairs stored in the system.

Reviewing Annotation Hit History
View the history of each annotation's use, including edits, queries, replies, sources, similarity scores, and timestamps. This information is valuable for ongoing improvements to your annotations.

Logs & Annotations
Please ensure that your application complies with local regulations when collecting user data. The common practice is to publish a privacy policy and obtain user consent.
The Logs feature is designed to observe and annotate the performance of Dify applications. Dify records logs for all interactions with the application, whether through the WebApp or API. If you are a Prompt Engineer or LLM operator, it will provide you with a visual experience of LLM application operations.
Using the Logs Console
You can find the Logs in the left navigation of the application. This page typically displays:
Interaction records between users and AI within the selected timeframe
The results of user input and AI output, which for conversational applications are usually a series of message flows
Ratings from users and operators, as well as improvement annotations from operators
The logs currently do not include interaction records from the Prompt debugging process.
Improvement Annotations
These annotations will be used for model fine-tuning in future versions of Dify to improve model accuracy and response style. The current preview version only supports annotations.
Clicking on a log entry will open the log details panel on the right side of the interface. In this panel, operators can annotate an interaction:
Give a thumbs up for well-performing messages
Give a thumbs down for poorly-performing messages
Mark improved responses for improvement, which represents the text you expect AI to reply with
Please note that if multiple administrators in the team annotate the same log entry, the last annotation will overwrite the previous ones.
Plugins
Plugins is an upcoming feature of Dify. You can incorporate plugins into your App orchestration and access AI applications with plugin capabilities through an API or WebApp. Dify is compatible with the ChatGPT Plugins standard and provides some native plugins.
Based on WebApp Template
If developers are developing new products from scratch or in the product prototype design phase, you can quickly launch AI sites using Dify. At the same time, Dify hopes that developers can fully freely create different forms of front-end applications. For this reason, we provide:
SDK for quick access to the Dify API in various languages
WebApp Template for WebApp development scaffolding for each type of application
The WebApp Templates are open source under the MIT license. You are free to modify and deploy them to achieve all the capabilities of Dify or as a reference code for implementing your own App.
You can find these Templates on GitHub:
Conversational app
Text generation app
The fastest way to use the WebApp Template is to click "Use this template" on GitHub, which is equivalent to forking a new repository. Then you need to configure the Dify App ID and API Key, like this:
Copy
export const APP_ID = ''

export const API_KEY = ''
More config in config/index.ts:
Copy
export const APP_INFO: AppInfo = {

 "title": 'Chat APP',

 "description": '',

 "copyright": '',

 "privacy_policy": '',

 "default_language": 'zh-Hans'

}



export const isShowPrompt = true

export const promptTemplate = ''
Each WebApp Template provides a README file containing deployment instructions. Usually, WebApp Templates contain a lightweight backend service to ensure that developers' API keys are not directly exposed to users.
These WebApp Templates can help you quickly build prototypes of AI applications and use all the capabilities of Dify. If you develop your own applications or new templates based on them, feel free to share with us.
Extension
In the process of creating AI applications, developers face constantly changing business needs and complex technical challenges. Effectively leveraging extension capabilities can not only enhance the flexibility and functionality of applications but also ensure the security and compliance of enterprise data. Dify offers the following two methods of extension:
API Based Extension
Code-based Extension
API Based Extension
Developers can extend module capabilities through the API extension module. Currently supported module extensions include:
moderation
external_data_tool
Before extending module capabilities, prepare an API and an API Key for authentication, which can also be automatically generated by Dify. In addition to developing the corresponding module capabilities, follow the specifications below so that Dify can invoke the API correctly.

Add API Extension
API Specifications
Dify will invoke your API according to the following specifications:
Copy
POST {Your-API-Endpoint}
Header
Header
Value
Desc
Content-Type
application/json
The request content is in JSON format.
Authorization
Bearer {api_key}
The API Key is transmitted as a token. You need to parse the api_key and verify if it matches the provided API Key to ensure API security.

Request Body
Copy
{

   "point":  string, // Extension point, different modules may contain multiple extension points

   "params": {

       ...  // Parameters passed to each module's extension point

   }

}
API Response
Copy
{

   ...  // For the content returned by the API, see the specific module's design specifications for different extension points.

}

Check
When configuring API-based Extension in Dify, Dify will send a request to the API Endpoint to verify the availability of the API. When the API Endpoint receives point=ping, the API should return result=pong, as follows:
Header
Copy
Content-Type: application/json

Authorization: Bearer {api_key}
Request Body
Copy
{

   "point": "ping"

}
Expected API response
Copy
{

   "result": "pong"

}
\
For Example
Here we take the external data tool as an example, where the scenario is to retrieve external weather information based on the region as context.
API Specifications
POST https://fake-domain.com/api/dify/receive
Header
Copy
Content-Type: application/json

Authorization: Bearer 123456
Request Body
Copy
{

   "point": "app.external_data_tool.query",

   "params": {

       "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",

       "tool_variable": "weather_retrieve",

       "inputs": {

           "location": "London"

       },

       "query": "How's the weather today?"

   }

}
API Response
Copy
{

   "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"

}
Code demo
The code is based on the Python FastAPI framework.
Install dependencies.
Copy
pip install 'fastapi[all]' uvicorn
Write code according to the interface specifications.
Copy
from fastapi import FastAPI, Body, HTTPException, Header

from pydantic import BaseModel



app = FastAPI()





class InputData(BaseModel):

   point: str

   params: dict





@app.post("/api/dify/receive")

async def dify_receive(data: InputData = Body(...), authorization: str = Header(None)):

   """

   Receive API query data from Dify.

   """

   expected_api_key = "123456"  # TODO Your API key of this API

   auth_scheme, _, api_key = authorization.partition(' ')



   if auth_scheme.lower() != "bearer" or api_key != expected_api_key:

       raise HTTPException(status_code=401, detail="Unauthorized")



   point = data.point



   # for debug

   print(f"point: {point}")



   if point == "ping":

       return {

           "result": "pong"

       }

   if point == "app.external_data_tool.query":

       return handle_app_external_data_tool_query(params=data.params)

   # elif point == "{point name}":

       # TODO other point implementation here



   raise HTTPException(status_code=400, detail="Not implemented")





def handle_app_external_data_tool_query(params: dict):

   app_id = params.get("app_id")

   tool_variable = params.get("tool_variable")

   inputs = params.get("inputs")

   query = params.get("query")



   # for debug

   print(f"app_id: {app_id}")

   print(f"tool_variable: {tool_variable}")

   print(f"inputs: {inputs}")

   print(f"query: {query}")



   # TODO your external data tool query implementation here, 

   #  return must be a dict with key "result", and the value is the query result

   if inputs.get("location") == "London":

       return {

           "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind "

                     "Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"

       }

   else:

       return {"result": "Unknown city"}
Launch the API service.
The default port is 8000. The complete address of the API is: http://127.0.0.1:8000/api/dify/receivewith the configured API Key '123456'.
Copy
uvicorn main:app --reload --host 0.0.0.0
Configure this API in Dify.

Select this API extension in the App.

When debugging the App, Dify will request the configured API and send the following content (example):
Copy
{

   "point": "app.external_data_tool.query",

   "params": {

       "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",

       "tool_variable": "weather_retrieve",

       "inputs": {

           "location": "London"

       },

       "query": "How's the weather today?"

   }

}
API Response：
Copy
{

   "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"

}
Local debugging
Since Dify's cloud version can't access internal network API services, you can use Ngrok to expose your local API service endpoint to the public internet for cloud-based debugging of local code. The steps are:
Visit the Ngrok official website at https://ngrok.com, register, and download the Ngrok file.

After downloading, go to the download directory. Unzip the package and run the initialization script as instructed:
Copy
$ unzip /path/to/ngrok.zip

$ ./ngrok config add-authtoken 你的Token
Check the port of your local API service.

Run the following command to start:
Copy
$ ./ngrok http [port number]
Upon successful startup, you'll see something like the following:

Find the 'Forwarding' address, like the sample domain https://177e-159-223-41-52.ngrok-free.app, and use it as your public domain.
For example, to expose your locally running service, replace the example URL http://127.0.0.1:8000/api/dify/receive with https://177e-159-223-41-52.ngrok-free.app/api/dify/receive.
Now, this API endpoint is accessible publicly. You can configure this endpoint in Dify for local debugging. For the configuration steps, consult the appropriate documentation or guide.
Deploy API extension with Cloudflare Workers
We recommend that you use Cloudflare Workers to deploy your API extension, because Cloudflare Workers can easily provide a public address and can be used for free.
Expose API Extension on public Internet using Cloudflare Workers
External_data_tool
When creating AI applications, developers can use API extensions to incorporate additional data from external tools into prompts as supplementary information for LLMs.
Please read API Based Extension to complete the development and integration of basic API service capabilities.
Extension Point
app.external_data_tool.query: Apply external data tools to query extension points.
This extension point takes the application variable content passed in by the end user and the input content (fixed parameters for conversational applications) as parameters to the API. Developers need to implement the query logic for the corresponding tool and return the query results as a string type.
Request Body
Copy
{

   "point": "app.external_data_tool.query", 

   "params": {

       "app_id": string,  

       "tool_variable": string,  

       "inputs": {  

           "var_1": "value_1",

           "var_2": "value_2",

           ...

       },

       "query": string | null  

   }

}
Example
Copy
{

   "point": "app.external_data_tool.query",

   "params": {

       "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",

       "tool_variable": "weather_retrieve",

       "inputs": {

           "location": "London"

       },

       "query": "How's the weather today?"

   }

}
API Response
Copy
{

   "result": string

}
Example
Copy
{

   "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"

}

Moderation Extension
This module is used to review the content input by end-users and the output from LLMs within the application, divided into two types of extension points.
Please read API Based Extension to complete the development and integration of basic API service capabilities.
Extension Point
app.moderation.input: End-user input content review extension point. It is used to review the content of variables passed in by end-users and the input content of dialogues in conversational applications.
app.moderation.output: LLM output content review extension point. It is used to review the content output by LLM. When the LLM output is streaming, the content will be requested by the API in chunks of 100 characters to avoid delays in review when the output content is lengthy.
app.moderation.input
Request Body
Copy
{

   "point": "app.moderation.input", 

       "app_id": string,  

       "inputs": {  

           "var_1": "value_1",

           "var_2": "value_2",

           ...

       },

       "query": string | null  

   }

}
Example
Copy
{

   "point": "app.moderation.input",

   "params": {

       "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",

       "inputs": {

           "var_1": "I will kill you.",

           "var_2": "I will fuck you."

       },

       "query": "Happy everydays."

   }

}
API Response
Copy
{

   "flagged": bool,  

   "action": string, 

   "preset_response": string,  

   "inputs": {  

       "var_1": "value_1",

       "var_2": "value_2",

       ...

   },

   "query": string | null  

}
Example
action=direct_output
Copy
{

   "flagged": true,

   "action": "direct_output",

   "preset_response": "Your content violates our usage policy."

}
action=overrided
Copy
{

   "flagged": true,

   "action": "overrided",

   "inputs": {

       "var_1": "I will *** you.",

       "var_2": "I will *** you."

   },

   "query": "Happy everydays."

}
app.moderation.output
Request Body
Copy
{

   "point": "app.moderation.output", 

   "params": {

       "app_id": string,  

       "text": string  

   }

}
Example
Copy
{


   "point": "app.moderation.output",


   "params": {


       "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",


       "text": "I will kill you."


   }


}
API Response
Copy
{

   "flagged": bool,  

   "action": string, 

   "preset_response": string,  

   "text": string 
Example
action=direct_output
Copy
{


   "flagged": true,


   "action": "direct_output",


   "preset_response": "Your content violates our usage policy."


}
action=overrided
Copy
{


   "flagged": true,


   "action": "overrided",


   "text": "I will *** you."


}
Code-based Extension
For developers deploying Dify locally who wish to implement extension capabilities, there is no need to rewrite an API service. Instead, they can use Code-based Extension, which allows for the expansion or enhancement of the program's capabilities in the form of code (i.e., plugin capabilities) on top of Dify's existing features, without disrupting the original code logic of Dify. It follows certain interfaces or specifications to ensure compatibility and pluggability with the main program. Currently, Dify has opened up two types of Code-based Extensions, which are:
Add a new type of external data tool
Expand sensitive content review policies
On the basis of the above functions, you can follow the specifications of the code-level interface to achieve the purpose of horizontal expansion.
Moderation
In our interactions with AI applications, we often have stringent requirements in terms of content security, user experience, and legal regulations. At this point, we need the "Sensitive Word Review" feature to create a better interactive environment for end-users. On the prompt orchestration page, click "Add Function" and locate the "Content Review" toolbox at the bottom:

Content moderation
Call the OpenAI Moderation API
OpenAI, along with most companies providing LLMs, includes content moderation features in their models to ensure that outputs do not contain controversial content, such as violence, sexual content, and illegal activities. Additionally, OpenAI has made this content moderation capability available, which you can refer to at https://platform.openai.com/docs/guides/moderation/overview.
Now you can also directly call the OpenAI Moderation API on Dify; you can review either input or output content simply by entering the corresponding "preset reply."

OpenAI Moderation
Keywords
Developers can customize the sensitive words they need to review, such as using "kill" as a keyword to perform an audit action when users input. The preset reply content should be "The content is violating usage policies." It can be anticipated that when a user inputs a text snippet containing "kill" at the terminal, it will trigger the sensitive word review tool and return the preset reply content.

Keywords
Moderation Extension
Different enterprises often have their own mechanisms for sensitive word moderation. When developing their own AI applications, such as an internal knowledge base ChatBot, enterprises need to moderate the query content input by employees for sensitive words. For this purpose, developers can write an API extension based on their enterprise's internal sensitive word moderation mechanisms, specifically referring to Moderation Extension, which can then be called on Dify to achieve a high degree of customization and privacy protection for sensitive word review.

Moderation Extension

Discovery
Template application
In Explore > Discovery, some commonly used template applications are provided. These apps cover translate, writing, programming and assistant.

If you want to use a template application, click the template's "Add to Workspace" button. In the workspace on the left, the app is available.

If you want to modify a template to create a new application, click the "Customize" button of the template.
Workspace
The workspace is the application's navigation. Click an application in the workspace to use the application directly.

Apps in the workspace include: your own apps and apps added to the workspace by other teams.
Billing
Know more about Dify's billing plans to support expanding your usage.
Workspace-based Billing
The Dify platform has "workspaces" and "apps". A workspace can contain multiple apps. Each app has capabilities like prompt orchestration, LLM invocation, knowledge RAG, logging & annotation, and standard API delivery. We recommend one team or organization use one workspace, because our system bills on a per-workspace basis (calculated from total resource consumption within a workspace). For example:
Copy


Workspace 1  

App 1(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API)

App 2(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API) 

App 3(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API)

...

Workspace 2
Plan Quotas and Features
We offer a free plan for all users to test your AI app ideas, including 200 OpenAI model message calls. After using up the free allowance, you need to obtain LLM API keys from different model providers, and add them under Settings --> Model Providers to enable normal model capabilities.Upgrading your workspace to a paid plan means unlocking paid resources for that workspace. For example: upgrading to Professional allows creating over 10 apps (up to 50) with up to 200MB total vector storage quota combined across projects in that workspace. Different version quotas and features are as follows:
Metric
Sandbox
Professional
Team
pricing
Free
$59/month
$159/month
Model Providers
OpenAI,Anthropic,Azure OpenAI,Llama2,Hugging Face,Replicate
OpenAI,Anthropic,Azure OpenAI, Llama2,Hugging Face,Replicate
OpenAI,Anthropic,Azure OpenAI, Llama2,Hugging Face,Replicate
Team Members
1
3
Unlimited
Apps
10
50
Unlimited
Vector Storage
5MB
200MB
1GB
Document Processing Priority
Standard
Priority
Priority
Logo Change
/
/
√
Message Requests
500 per day
Unlimited
Unlimited
RAG API Requests Quota Limits
/
√ Coming soon
√ Coming soon
Annotation Quota Limits
10
2000
5000
Agent Model
/
√ Coming soon
√ Coming soon
Logs History
30 days
Unlimited
Unlimited

Check out the pricing page to learn more.
**Vector storage:**Vector storage is used to store knowledge bases as vectors for LLMs to understand. Each 1MB can store about 1.2million characters of vectorized data(estimated using OpenAI Embeddings, varies across models). How much the data shrinks depends on complexity and repetition in the content.
**Annotation Quota Limits:**Manual editing and annotation of responses provides customizable high-quality question-answering abilities for apps.
**Message Requests:**The number of times the Dify API is called daily during application sessions (rather than LLM API resource usage). It includes all messages generated from your applications via API calls or during WebApp sessions. Note:Daily quotas are refreshed at midnight Pacific Standard Time.
**RAG API Requests:**Refers to the number of API calls invoking only the knowledge base processing capabilities of Dify.
Monitor Resource Usage
You can view capacity usage details on your workspace's Billing page.

monitor resource usage
FAQ
What happens if my resource usage exceeds the Free plan before I upgrade to a paid plan?
During Dify's Beta stage, excess quotas were provided for free to seed users' workspaces. After Dify's billing system goes live, your existing data will not be lost, but your workspace can no longer process additional text vectorization storage. You will need to upgrade to a suitable plan to continue using Dify.
What if neither the Professional nor Team plans meet my usage needs?
If you are a large enterprise requiring more advanced plans, please email us at business@dify.ai.
Under what circumstances do I need to pay when using the CE version?
When using the CE version, please follow our open source license terms. If you need commercial use, such as removing Dify's logo or requiring multiple workspaces, using Dify in a SaaS model, you will need to contact us at business@dify.ai for commercial licensing.

Quick Tool Integration
Here, we will use GoogleSearch as an example to demonstrate how to quickly integrate a tool.
1. Prepare the Tool Provider yaml
Introduction
This yaml declares a new tool provider, and includes information like the provider's name, icon, author, and other details that are fetched by the frontend for display.
Example
We need to create a google module (folder) under core/tools/provider/builtin, and create google.yaml. The name must be consistent with the module name.
Subsequently, all operations related to this tool will be carried out under this module.
Copy
identity: # Basic information of the tool provider

 author: Dify # Author

 name: google # Name, unique, no duplication with other providers

 label: # Label for frontend display

   en_US: Google # English label

   zh_Hans: Google # Chinese label

 description: # Description for frontend display

   en_US: Google # English description

   zh_Hans: Google # Chinese description

 icon: icon.svg # Icon, needs to be placed in the _assets folder of the current module

The identity field is mandatory, it contains the basic information of the tool provider, including author, name, label, description, icon, etc.
The icon needs to be placed in the _assets folder of the current module, you can refer to: api/core/tools/provider/builtin/google/_assets/icon.svg
Copy
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="25" viewBox="0 0 24 25" fill="none">


 <path d="M22.501 12.7332C22.501 11.8699 22.4296 11.2399 22.2748 10.5865H12.2153V14.4832H18.12C18.001 15.4515 17.3582 16.9099 15.9296 17.8898L15.9096 18.0203L19.0902 20.435L19.3106 20.4565C21.3343 18.6249 22.501 15.9298 22.501 12.7332Z" fill="#4285F4"/>


 <path d="M12.214 23C15.1068 23 17.5353 22.0666 19.3092 20.4567L15.9282 17.8899C15.0235 18.5083 13.8092 18.9399 12.214 18.9399C9.38069 18.9399 6.97596 17.1083 6.11874 14.5766L5.99309 14.5871L2.68583 17.0954L2.64258 17.2132C4.40446 20.6433 8.0235 23 12.214 23Z" fill="#34A853"/>


 <path d="M6.12046 14.5766C5.89428 13.9233 5.76337 13.2233 5.76337 12.5C5.76337 11.7766 5.89428 11.0766 6.10856 10.4233L6.10257 10.2841L2.75386 7.7355L2.64429 7.78658C1.91814 9.20993 1.50146 10.8083 1.50146 12.5C1.50146 14.1916 1.91814 15.7899 2.64429 17.2132L6.12046 14.5766Z" fill="#FBBC05"/>


 <path d="M12.2141 6.05997C14.2259 6.05997 15.583 6.91163 16.3569 7.62335L19.3807 4.73C17.5236 3.03834 15.1069 2 12.2141 2C8.02353 2 4.40447 4.35665 2.64258 7.78662L6.10686 10.4233C6.97598 7.89166 9.38073 6.05997 12.2141 6.05997Z" fill="#EB4335"/>


</svg>
2. Prepare Provider Credentials
Google, as a third-party tool, uses the API provided by SerpApi, which requires an API Key to use. This means that this tool needs a credential to use. For tools like wikipedia, there is no need to fill in the credential field, you can refer to: api/core/tools/provider/builtin/wikipedia/wikipedia.yaml
Copy
identity:

 author: Dify

 name: wikipedia

 label:

   en_US: Wikipedia

   zh_Hans: 维基百科

   pt_BR: Wikipedia

 description:

   en_US: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.

   zh_Hans: 维基百科是一个由全世界的志愿者创建和编辑的免费在线百科全书。

   pt_BR: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.

 icon: icon.svg

credentials_for_provider:
After configuring the credential field, the effect is as follows:
Copy
identity:

 author: Dify

 name: google

 label:

   en_US: Google

   zh_Hans: Google

 description:

   en_US: Google

   zh_Hans: Google

 icon: icon.svg

credentials_for_provider: # Credential field

 serpapi_api_key: # Credential field name

   type: secret-input # Credential field type

   required: true # Required or not

   label: # Credential field label

     en_US: SerpApi API key # English label

     zh_Hans: SerpApi API key # Chinese label

   placeholder: # Credential field placeholder

     en_US: Please input your SerpApi API key # English placeholder

     zh_Hans: 请输入你的 SerpApi API key # Chinese placeholder

   help: # Credential field help text

     en_US: Get your SerpApi API key from SerpApi # English help text

     zh_Hans: 从 SerpApi 获取您的 SerpApi API key # Chinese help text

   url: https://serpapi.com/manage-api-key # Credential field help link

type: Credential field type, currently can be either secret-input, text-input, or select , corresponding to password input box, text input box, and drop-down box, respectively. If set to secret-input, it will mask the input content on the frontend, and the backend will encrypt the input content.
3. Prepare Tool yaml
A provider can have multiple tools, each tool needs a yaml file to describe, this file contains the basic information, parameters, output, etc. of the tool.
Still taking GoogleSearch as an example, we need to create a tools module under the google module, and create tools/google_search.yaml, the content is as follows.
Copy
identity: # Basic information of the tool

 name: google_search # Tool name, unique, no duplication with other tools

 author: Dify # Author

 label: # Label for frontend display

   en_US: GoogleSearch # English label

   zh_Hans: 谷歌搜索 # Chinese label

description: # Description for frontend display

 human: # Introduction for frontend display, supports multiple languages

   en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.

   zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。

 llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query. # Introduction passed to LLM, in order to make LLM better understand this tool, we suggest to write as detailed information about this tool as possible here, so that LLM can understand and use this tool

parameters: # Parameter list

 - name: query # Parameter name

   type: string # Parameter type

   required: true # Required or not

   label: # Parameter label

     en_US: Query string # English label

     zh_Hans: 查询语句 # Chinese label

   human_description: # Introduction for frontend display, supports multiple languages

     en_US: used for searching

     zh_Hans: 用于搜索网页内容

   llm_description: key words for searching # Introduction passed to LLM, similarly, in order to make LLM better understand this parameter, we suggest to write as detailed information about this parameter as possible here, so that LLM can understand this parameter

   form: llm # Form type, llm means this parameter needs to be inferred by Agent, the frontend will not display this parameter

 - name: result_type

   type: select # Parameter type

   required: true

   options: # Drop-down box options

     - value: text

       label:

         en_US: text

         zh_Hans: 文本

     - value: link

       label:

         en_US: link

         zh_Hans: 链接

   default: link

   label:

     en_US: Result type

     zh_Hans: 结果类型

   human_description:

     en_US: used for selecting the result type, text or link

     zh_Hans: 用于选择结果类型，使用文本还是链接进行展示

   form: form # Form type, form means this parameter needs to be filled in by the user on the frontend before the conversation starts

The identity field is mandatory, it contains the basic information of the tool, including name, author, label, description, etc.
parameters Parameter list
name Parameter name, unique, no duplication with other parameters
type Parameter type, currently supports string, number, boolean, select four types, corresponding to string, number, boolean, drop-down box
required Required or not
In llm mode, if the parameter is required, the Agent is required to infer this parameter
In form mode, if the parameter is required, the user is required to fill in this parameter on the frontend before the conversation starts
options Parameter options
In llm mode, Dify will pass all options to LLM, LLM can infer based on these options
In form mode, when type is select, the frontend will display these options
default Default value
label Parameter label, for frontend display
human_description Introduction for frontend display, supports multiple languages
llm_description Introduction passed to LLM, in order to make LLM better understand this parameter, we suggest to write as detailed information about this parameter as possible here, so that LLM can understand this parameter
form Form type, currently supports llm, form two types, corresponding to Agent self-inference and frontend filling
4. Add Tool Logic
After completing the tool configuration, we can start writing the tool code that defines how it is invoked.
Create google_search.py under the google/tools module, the content is as follows.
Copy
from core.tools.tool.builtin_tool import BuiltinTool

from core.tools.entities.tool_entities import ToolInvokeMessage



from typing import Any, Dict, List, Union



class GoogleSearchTool(BuiltinTool):

   def _invoke(self, 

               user_id: str,

              tool_parameters: Dict[str, Any], 

       ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:

       """

           invoke tools

       """

       query = tool_parameters['query']

       result_type = tool_parameters['result_type']

       api_key = self.runtime.credentials['serpapi_api_key']

       # TODO: search with serpapi

       result = SerpAPI(api_key).run(query, result_type=result_type)



       if result_type == 'text':

           return self.create_text_message(text=result)

       return self.create_link_message(link=result)
Parameters
The overall logic of the tool is in the _invoke method, this method accepts two parameters: user_id and tool_parameters, which represent the user ID and tool parameters respectively
Return Data
When the tool returns, you can choose to return one message or multiple messages, here we return one message, using create_text_message and create_link_message can create a text message or a link message.
5. Add Provider Code
Finally, we need to create a provider class under the provider module to implement the provider's credential verification logic. If the credential verification fails, it will throw a ToolProviderCredentialValidationError exception.
Create google.py under the google module, the content is as follows.
Copy
from core.tools.entities.tool_entities import ToolInvokeMessage, ToolProviderType

from core.tools.tool.tool import Tool

from core.tools.provider.builtin_tool_provider import BuiltinToolProviderController

from core.tools.errors import ToolProviderCredentialValidationError



from core.tools.provider.builtin.google.tools.google_search import GoogleSearchTool



from typing import Any, Dict



class GoogleProvider(BuiltinToolProviderController):

   def _validate_credentials(self, credentials: Dict[str, Any]) -> None:

       try:

           # 1. Here you need to instantiate a GoogleSearchTool with GoogleSearchTool(), it will automatically load the yaml configuration of GoogleSearchTool, but at this time it does not have credential information inside

           # 2. Then you need to use the fork_tool_runtime method to pass the current credential information to GoogleSearchTool

           # 3. Finally, invoke it, the parameters need to be passed according to the parameter rules configured in the yaml of GoogleSearchTool

           GoogleSearchTool().fork_tool_runtime(

               meta={

                   "credentials": credentials,

               }

           ).invoke(

               user_id='',

               tool_parameters={

                   "query": "test",

                   "result_type": "link"

               },

           )

       except Exception as e:

           raise ToolProviderCredentialValidationError(str(e))
Completion
After the above steps are completed, we can see this tool on the frontend, and it can be used in the Agent.
Of course, because google_search needs a credential, before using it, you also need to input your credentials on the frontend.


Advanced Tool Integration
Before starting with this advanced guide, please make sure you have a basic understanding of the tool integration process in Dify. Check out Quick Integration for a quick run through.
Tool Interface
We have defined a series of helper methods in the Tool class to help developers quickly build more complex tools.
Message Return
Dify supports various message types such as text, link, image, and file BLOB. You can return different types of messages to the LLM and users through the following interfaces.
Please note, some parameters in the following interfaces will be introduced in later sections.
Image URL
You only need to pass the URL of the image, and Dify will automatically download the image and return it to the user.
Copy
   def create_image_message(self, image: str, save_as: str = '') -> ToolInvokeMessage:

       """

           create an image message



           :param image: the url of the image

           :return: the image message

       """
Link
If you need to return a link, you can use the following interface.
Copy
   def create_link_message(self, link: str, save_as: str = '') -> ToolInvokeMessage:

       """

           create a link message



           :param link: the url of the link

           :return: the link message

       """
Text
If you need to return a text message, you can use the following interface.
Copy
   def create_text_message(self, text: str, save_as: str = '') -> ToolInvokeMessage:

       """

           create a text message



           :param text: the text of the message

           :return: the text message

       """
File BLOB
If you need to return the raw data of a file, such as images, audio, video, PPT, Word, Excel, etc., you can use the following interface.
blob The raw data of the file, of bytes type
meta The metadata of the file, if you know the type of the file, it is best to pass a mime_type, otherwise Dify will use octet/stream as the default type
Copy
   def create_blob_message(self, blob: bytes, meta: dict = None, save_as: str = '') -> ToolInvokeMessage:

       """

           create a blob message



           :param blob: the blob

           :return: the blob message

       """
Shortcut Tools
In large model applications, we have two common needs:
First, summarize a long text in advance, and then pass the summary content to the LLM to prevent the original text from being too long for the LLM to handle
The content obtained by the tool is a link, and the web page information needs to be crawled before it can be returned to the LLM
To help developers quickly implement these two needs, we provide the following two shortcut tools.
Text Summary Tool
This tool takes in an user_id and the text to be summarized, and returns the summarized text. Dify will use the default model of the current workspace to summarize the long text.
Copy
   def summary(self, user_id: str, content: str) -> str:

       """

           summary the content



           :param user_id: the user id

           :param content: the content

           :return: the summary

       """
Web Page Crawling Tool
This tool takes in web page link to be crawled and a user_agent (which can be empty), and returns a string containing the information of the web page. The user_agent is an optional parameter that can be used to identify the tool. If not passed, Dify will use the default user_agent.
Copy
   def get_url(self, url: str, user_agent: str = None) -> str:

       """

           get url

       """ the crawled result
Variable Pool
We have introduced a variable pool in Tool to store variables, files, etc. generated during the tool's operation. These variables can be used by other tools during the tool's operation.
Next, we will use DallE3 and Vectorizer.AI as examples to introduce how to use the variable pool.
DallE3 is an image generation tool that can generate images based on text. Here, we will let DallE3 generate a logo for a coffee shop
Vectorizer.AI is a vector image conversion tool that can convert images into vector images, so that the images can be infinitely enlarged without distortion. Here, we will convert the PNG icon generated by DallE3 into a vector image, so that it can be truly used by designers.
DallE3
First, we use DallE3. After creating the image, we save the image to the variable pool. The code is as follows:
Copy
from typing import Any, Dict, List, Union

from core.tools.entities.tool_entities import ToolInvokeMessage

from core.tools.tool.builtin_tool import BuiltinTool



from base64 import b64decode



from openai import OpenAI



class DallE3Tool(BuiltinTool):

   def _invoke(self, 

               user_id: str, 

              tool_paramters: Dict[str, Any], 

       ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:

       """

           invoke tools

       """

       client = OpenAI(

           api_key=self.runtime.credentials['openai_api_key'],

       )



       # prompt

       prompt = tool_paramters.get('prompt', '')

       if not prompt:

           return self.create_text_message('Please input prompt')



       # call openapi dalle3

       response = client.images.generate(

           prompt=prompt, model='dall-e-3',

           size='1024x1024', n=1, style='vivid', quality='standard',

           response_format='b64_json'

       )



       result = []

       for image in response.data:

           # Save all images to the variable pool through the save_as parameter. The variable name is self.VARIABLE_KEY.IMAGE.value. If new images are generated later, they will overwrite the previous images.

           result.append(self.create_blob_message(blob=b64decode(image.b64_json), 

                                                  meta={ 'mime_type': 'image/png' },

                                                   save_as=self.VARIABLE_KEY.IMAGE.value))



       return result
Note that we used self.VARIABLE_KEY.IMAGE.value as the variable name of the image. In order for developers' tools to cooperate with each other, we defined this KEY. You can use it freely, or you can choose not to use this KEY. Passing a custom KEY is also acceptable.
Vectorizer.AI
Next, we use Vectorizer.AI to convert the PNG icon generated by DallE3 into a vector image. Let's go through the functions we defined here. The code is as follows:
Copy
from core.tools.tool.builtin_tool import BuiltinTool

from core.tools.entities.tool_entities import ToolInvokeMessage, ToolParamter

from core.tools.errors import ToolProviderCredentialValidationError



from typing import Any, Dict, List, Union

from httpx import post

from base64 import b64decode



class VectorizerTool(BuiltinTool):

   def _invoke(self, user_id: str, tool_paramters: Dict[str, Any]) \

       -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:

       """

       Tool invocation, the image variable name needs to be passed in from here, so that we can get the image from the variable pool

       """

       

   

   def get_runtime_parameters(self) -> List[ToolParamter]:

       """

       Override the tool parameter list, we can dynamically generate the parameter list based on the actual situation in the current variable pool, so that the LLM can generate the form based on the parameter list

       """

       

   

   def is_tool_avaliable(self) -> bool:

       """

       Whether the current tool is available, if there is no image in the current variable pool, then we don't need to display this tool, just return False here

       """    
Next, let's implement these three functions
Copy
from core.tools.tool.builtin_tool import BuiltinTool

from core.tools.entities.tool_entities import ToolInvokeMessage, ToolParamter

from core.tools.errors import ToolProviderCredentialValidationError



from typing import Any, Dict, List, Union

from httpx import post

from base64 import b64decode



class VectorizerTool(BuiltinTool):

   def _invoke(self, user_id: str, tool_paramters: Dict[str, Any]) \

       -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:

       """

           invoke tools

       """

       api_key_name = self.runtime.credentials.get('api_key_name', None)

       api_key_value = self.runtime.credentials.get('api_key_value', None)



       if not api_key_name or not api_key_value:

           raise ToolProviderCredentialValidationError('Please input api key name and value')



       # Get image_id, the definition of image_id can be found in get_runtime_parameters

       image_id = tool_paramters.get('image_id', '')

       if not image_id:

           return self.create_text_message('Please input image id')



       # Get the image generated by DallE from the variable pool

       image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE)

       if not image_binary:

           return self.create_text_message('Image not found, please request user to generate image firstly.')



       # Generate vector image

       response = post(

           'https://vectorizer.ai/api/v1/vectorize',

           files={ 'image': image_binary },

           data={ 'mode': 'test' },

           auth=(api_key_name, api_key_value), 

           timeout=30

       )



       if response.status_code != 200:

           raise Exception(response.text)

       

       return [

           self.create_text_message('the vectorized svg is saved as an image.'),

           self.create_blob_message(blob=response.content,

                                   meta={'mime_type': 'image/svg+xml'})

       ]

   

   def get_runtime_parameters(self) -> List[ToolParamter]:

       """

       override the runtime parameters

       """

       # Here, we override the tool parameter list, define the image_id, and set its option list to all images in the current variable pool. The configuration here is consistent with the configuration in yaml.

       return [

           ToolParamter.get_simple_instance(

               name='image_id',

               llm_description=f'the image id that you want to vectorize, \

                   and the image id should be specified in \

                       {[i.name for i in self.list_default_image_variables()]}',

               type=ToolParamter.ToolParameterType.SELECT,

               required=True,

               options=[i.name for i in self.list_default_image_variables()]

           )

       ]

   

   def is_tool_avaliable(self) -> bool:

       # Only when there are images in the variable pool, the LLM needs to use this tool

       return len(self.list_default_image_variables()) > 0
It's worth noting that we didn't actually use image_id here. We assumed that there must be an image in the default variable pool when calling this tool, so we directly used image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE) to get the image. In cases where the model's capabilities are weak, we recommend developers to do the same, which can effectively improve fault tolerance and avoid the model passing incorrect parameters.

Expose API Extension on public Internet using Cloudflare Workers
Getting Started
Since the Dify API Extension requires a publicly accessible internet address as an API Endpoint, we need to deploy our API extension to a public internet address. Here, we use Cloudflare Workers for deploying our API extension.
We clone the Example GitHub Repository, which contains a simple API extension. We can modify this as a base.
Copy
git clone https://github.com/crazywoola/dify-extension-workers.git

cp wrangler.toml.example wrangler.toml
Open the wrangler.toml file, and modify name and compatibility_date to your application's name and compatibility date.
An important configuration here is the TOKEN in vars, which you will need to provide when adding the API extension in Dify. For security reasons, it's recommended to use a random string as the Token. You should not write the Token directly in the source code but pass it via environment variables. Thus, do not commit your wrangler.toml to your code repository.
Copy
name = "dify-extension-example"

compatibility_date = "2023-01-01"



[vars]

TOKEN = "bananaiscool"
This API extension returns a random Breaking Bad quote. You can modify the logic of this API extension in src/index.ts. This example shows how to interact with a third-party API.
Copy
// ⬇️ implement your logic here ⬇️

// point === "app.external_data_tool.query"

// https://api.breakingbadquotes.xyz/v1/quotes

const count = params?.inputs?.count ?? 1;

const url = `https://api.breakingbadquotes.xyz/v1/quotes/${count}`;

const result = await fetch(url).then(res => res.text())

// ⬆️ implement your logic here ⬆️
This repository simplifies all configurations except for business logic. You can directly use npm commands to deploy your API extension.
Copy
npm run deploy
After successful deployment, you will get a public internet address, which you can add in Dify as an API Endpoint. Please note not to miss the endpoint path.

Adding API Endpoint in Dify

Adding API Tool in the App edit page
Other Logic TL;DR
About Bearer Auth
Copy
import { bearerAuth } from "hono/bearer-auth";



(c, next) => {

   const auth = bearerAuth({ token: c.env.TOKEN });

   return auth(c, next);

},
Our Bearer authentication logic is as shown above. We use the hono/bearer-auth package for Bearer authentication. You can use c.env.TOKEN in src/index.ts to get the Token.
About Parameter Validation
Copy
import { z } from "zod";

import { zValidator } from "@hono/zod-validator";



const schema = z.object({

 point: z.union([

   z.literal("ping"),

   z.literal("app.external_data_tool.query"),

 ]), // Restricts 'point' to two specific values

 params: z

   .object({

     app_id: z.string().optional(),

     tool_variable: z.string().optional(),

     inputs: z.record(z.any()).optional(),

     query: z.any().optional(),  // string or null

   })

   .optional(),

});
We use zod to define the types of parameters. You can use zValidator in src/index.ts for parameter validation. Get validated parameters through const { point, params } = c.req.valid("json");. Our point has only two values, so we use z.union for definition. params is an optional parameter, defined with z.optional. It includes a inputs parameter, a Record<string, any> type representing an object with string keys and any values. This type can represent any object. You can get the count parameter in src/index.ts using params?.inputs?.count.
Accessing Logs of Cloudflare Workers
Copy
wrangler tail
Reference Content
Cloudflare Workers
Cloudflare Workers CLI
Example GitHub Repository
Connecting with Different Models
Learn about the Different Models Supported by Dify.
Dify supports major model providers like OpenAI's GPT series and Anthropic's Claude series. Each model's capabilities and parameters differ, so select a model provider that suits your application's needs. Obtain the API key from the model provider's official website before using it in Dify.
Model Types in Dify
Dify classifies models into 4 types, each for different uses:
System Inference Models: Used in applications for tasks like chat, name generation, and suggesting follow-up questions.
Providers include OpenAI、Azure OpenAI Service、Anthropic、Hugging Face Hub、Replicate、Xinference、OpenLLM、iFLYTEK SPARK、WENXINYIYAN、TONGYI、Minimax、ZHIPU(ChatGLM) Ollama、LocalAI、.
Embedding Models: Employed for embedding segmented documents in knowledge and processing user queries in applications.
Providers include OpenAI, ZHIPU (ChatGLM), Jina AI(Jina Embeddings 2).
Rerank Models: Enhance search capabilities in LLMs.
Provider: Cohere.
Speech-to-Text Models: Convert spoken words to text in conversational applications.
Provider: OpenAI.
Dify plans to add more LLM providers as technology and user needs evolve.
Hosted Model Trial Service
Dify offers trial quotas for cloud service users to experiment with different models. Set up your model provider before the trial ends to ensure uninterrupted application use.
OpenAI Hosted Model Trial: Includes 200 invocations for models like GPT3.5-turbo, GPT3.5-turbo-16k, text-davinci-003 models.
Setting the Default Model
Dify automatically selects the default model based on usage. Configure this in Settings > Model Provider.

Model Integration Settings
Choose your model in Dify's Settings > Model Provider.

Model providers fall into two categories:
Proprietary Models: Developed by providers such as OpenAI and Anthropic.
Hosted Models: Offer third-party models, like Hugging Face and Replicate.
Integration methods differ between these categories.
Proprietary Model Providers: Dify connects to all models from an integrated provider. Set the provider's API key in Dify to integrate.
Dify uses PKCS1_OAEP encryption to protect your API keys. Each user (tenant) has a unique key pair for encryption, ensuring your API keys remain confidential.
Hosted Model Providers: Integrate third-party models individually.
Specific integration methods are not detailed here.
Hugging Face
Replicate
Xinference
OpenLLM
Using Models
Once configured, these models are ready for application use.

Hugging Face
Dify supports Text-Generation and Embeddings. Below are the corresponding Hugging Face model types:
Text-Generation：text-generation，text2text-generation
Embeddings：feature-extraction
The specific steps are as follows:
You need a Hugging Face account (registered address).
Set the API key of Hugging Face (obtain address).
Select a model to enter the Hugging Face model list page.

Dify supports accessing models on Hugging Face in two ways:
Hosted Inference API. This method uses the model officially deployed by Hugging Face. No fee is required. But the downside is that only a small number of models support this approach.
Inference Endpoint. This method uses resources such as AWS accessed by the Hugging Face to deploy the model and requires payment.
Models that access the Hosted Inference API
1 Select a model
Hosted inference API is supported only when there is an area containing Hosted inference API on the right side of the model details page. As shown in the figure below:

On the model details page, you can get the name of the model.

2 Using access models in Dify
Select Hosted Inference API for Endpoint Type in Settings > Model Provider > Hugging Face > Model Type. As shown below:

API Token is the API Key set at the beginning of the article. The model name is the model name obtained in the previous step.
Method 2: Inference Endpoint
1 Select the model to deploy
Inference Endpoint is only supported for models with the Inference Endpoints option under the Deploy button on the right side of the model details page. As shown below:

2 Deployment model
Click the Deploy button for the model and select the Inference Endpoint option. If you have not bound a bank card before, you will need to bind the card. Just follow the process. After binding the card, the following interface will appear: modify the configuration according to the requirements, and click Create Endpoint in the lower left corner to create an Inference Endpoint.

After the model is deployed, you can see the Endpoint URL.

3 Using access models in Dify
Select Inference Endpoints for Endpoint Type in Settings > Model Provider > Hugging face > Model Type. As shown below:

The API Token is the API Key set at the beginning of the article. The name of the Text-Generation model can be arbitrary, but the name of the Embeddings model needs to be consistent with Hugging Face. The Endpoint URL is the Endpoint URL obtained after the successful deployment of the model in the previous step.

Note: The "User name / Organization Name" for Embeddings needs to be filled in according to your deployment method on Hugging Face's Inference Endpoints, with either the ''User name'' or the "Organization Name".
Replicate
Dify supports accessing Language models and Embedding models on Replicate. Language models correspond to Dify's reasoning model, and Embedding models correspond to Dify's Embedding model.
Specific steps are as follows:
You need to have a Replicate account (registered address).
Get API Key (get address).
Pick a model. Select the model under Language models and Embedding models .
Add models in Dify's Settings > Model Provider > Replicate.

The API key is the API Key set in step 2. Model Name and Model Version can be found on the model details page:

Xinference
Xorbits inference is a powerful and versatile library designed to serve language, speech recognition, and multimodal models, and can even be used on laptops. It supports various models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, etc. And Dify supports connecting to Xinference deployed large language model inference and embedding capabilities locally.
Deploy Xinference
Please note that you usually do not need to manually find the IP address of the Docker container to access the service, because Docker offers a port mapping feature. This allows you to map the container ports to local machine ports, enabling access via your local address. For example, if you used the -p 80:80 parameter when running the container, you can access the service inside the container by visiting http://localhost:80 or http://127.0.0.1:80.
If you do need to use the container's IP address directly, the steps above will assist you in obtaining this information.
Starting Xinference
There are two ways to deploy Xinference, namely local deployment and distributed deployment, here we take local deployment as an example.
First, install Xinference via PyPI:
Copy
$ pip install "xinference[all]"
Start Xinference locally:
Copy
$ xinference-local


2023-08-20 19:21:05,265 xinference   10148 INFO     Xinference successfully started. Endpoint: http://127.0.0.1:9997


2023-08-20 19:21:05,266 xinference.core.supervisor 10148 INFO     Worker 127.0.0.1:37822 has been added successfully


2023-08-20 19:21:05,267 xinference.deploy.worker 10148 INFO     Xinference worker successfully started.
Xinference will start a worker locally by default, with the endpoint: http://127.0.0.1:9997, and the default port is 9997. By default, access is limited to the local machine only, but it can be configured with -H 0.0.0.0 to allow access from any non-local client. To modify the host or port, you can refer to xinference's help information: xinference-local --help.
If you use the Dify Docker deployment method, you need to pay attention to the network configuration to ensure that the Dify container can access the endpoint of Xinference. The Dify container cannot access localhost inside, and you need to use the host IP address.
Create and deploy the model
Visit http://127.0.0.1:9997, select the model and specification you need to deploy, as shown below:


As different models have different compatibility on different hardware platforms, please refer to Xinference built-in models to ensure the created model supports the current hardware platform.
Obtain the model UID
Copy model ID from Running Models page, such as: 2c886330-8849-11ee-9518-43b0b8f40bea
After the model is deployed, connect the deployed model in Dify.
In Settings > Model Providers > Xinference, enter:
Model name: vicuna-v1.3
Server URL: http://<Machine_IP>:9997 Replace with your machine IP address
Model UID: 2c886330-8849-11ee-9518-43b0b8f40bea
Click "Save" to use the model in the dify application.
Dify also supports using Xinference builtin models as Embedding models, just select the Embeddings type in the configuration box.
For more information about Xinference, please refer to: Xorbits Inference
OpenLLM
With OpenLLM, you can run inference with any open-source large-language models, deploy to the cloud or on-premises, and build powerful AI apps. And Dify supports connecting to OpenLLM deployed large language model's inference capabilities locally.
Deploy OpenLLM Model
Starting OpenLLM
Each OpenLLM Server can deploy one model, and you can deploy it in the following way:
Copy
docker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt
Note: Using the facebook/opt-1.3b model here is only for demonstration, and the effect may not be good. Please choose the appropriate model according to the actual situation. For more models, please refer to: Supported Model List.
After the model is deployed, use the connected model in Dify.
Fill in under Settings > Model Providers > OpenLLM:
Model Name: facebook/opt-1.3b
Server URL: http://<Machine_IP>:3333 Replace with your machine IP address
Click "Save" and the model can be used in the application.
This instruction is only for quick connection as an example. For more features and information on using OpenLLM, please refer to: OpenLLM

LocalAI
LocalAI is a drop-in replacement REST API that's compatible with OpenAI API specifications for local inferencing. It allows you to run LLMs (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families that are compatible with the ggml format. Does not require GPU.
Dify allows integration with LocalAI for local deployment of large language model inference and embedding capabilities.
Deploying LocalAI
Starting LocalAI
You can refer to the official Getting Started guide for deployment, or quickly integrate following the steps below:
(These steps are derived from LocalAI Data query example)
First, clone the LocalAI code repository and navigate to the specified directory.
Copy
$ git clone https://github.com/go-skynet/LocalAI


$ cd LocalAI/examples/langchain-chroma
Download example LLM and Embedding models.
Copy
$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert


$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j
Here, we choose two smaller models that are compatible across all platforms. ggml-gpt4all-j serves as the default LLM model, and all-MiniLM-L6-v2 serves as the default Embedding model, for quick local deployment.
Configure the .env file.
Copy
$ mv .env.example .env
NOTE: Ensure that the THREADS variable value in .env doesn't exceed the number of CPU cores on your machine.
Start LocalAI.
Copy
# start with docker-compose


$ docker-compose up -d --build





# tail the logs & wait until the build completes


$ docker logs -f langchain-chroma-api-1


7:16AM INF Starting LocalAI using 4 threads, with models path: /models


7:16AM INF LocalAI version: v1.24.1 (9cc8d9086580bd2a96f5c96a6b873242879c70bc)
The LocalAI request API endpoint will be available at http://127.0.0.1:8080.
And it provides two models, namely:
LLM Model: ggml-gpt4all-j
External access name: gpt-3.5-turbo (This name is customizable and can be configured in models/gpt-3.5-turbo.yaml).
Embedding Model: all-MiniLM-L6-v2
External access name: text-embedding-ada-002 (This name is customizable and can be configured in models/embeddings.yaml).
If you use the Dify Docker deployment method, you need to pay attention to the network configuration to ensure that the Dify container can access the endpoint of LocalAI. The Dify container cannot access localhost inside, and you need to use the host IP address.
Integrate the models into Dify.
Go to Settings > Model Providers > LocalAI and fill in:
Model 1: ggml-gpt4all-j
Model Type: Text Generation
Model Name: gpt-3.5-turbo
Server URL: http://127.0.0.1:8080
If Dify is deployed via docker, fill in the host domain: http://<your-LocalAI-endpoint-domain>:8080, which can be a LAN IP address, like: http://192.168.1.100:8080
Click "Save" to use the model in the application.
Model 2: all-MiniLM-L6-v2
Model Type: Embeddings
Model Name: text-embedding-ada-002
Server URL: http://127.0.0.1:8080
If Dify is deployed via docker, fill in the host domain: http://<your-LocalAI-endpoint-domain>:8080, which can be a LAN IP address, like: http://192.168.1.100:8080
Click "Save" to use the model in the application.
For more information about LocalAI, please refer to: https://github.com/go-skynet/LocalAI

Ollama

ollama
Ollama is a local inference framework client that allows one-click deployment of LLMs such as Llama 2, Mistral, Llava, etc. Dify supports integrating LLM and Text Embedding capabilities of large language models deployed with Ollama.
Quick Integration
Download and Launch Ollama
Download Ollama
Visit https://ollama.ai/download to download the Ollama client for your system.
Run Ollama and Chat with Llava
Copy
ollama run llava
After successful launch, Ollama starts an API service on local port 11434, which can be accessed at http://localhost:11434.
For other models, visit Ollama Models for more details.
Integrate Ollama in Dify
In Settings > Model Providers > Ollama, fill in:


Model Name: llava
Base URL: http://<your-ollama-endpoint-domain>:11434
Enter the base URL where the Ollama service is accessible.
If Dify is deployed using docker, consider using the local network IP address, e.g., http://192.168.1.100:11434 or the docker host machine IP address, e.g., http://172.17.0.1:11434.
For local source code deployment, use http://localhost:11434.
Model Type: Chat
Model Context Length: 4096
The maximum context length of the model. If unsure, use the default value of 4096.
Maximum Token Limit: 4096
The maximum number of tokens returned by the model. If there are no specific requirements for the model, this can be consistent with the model context length.
Support for Vision: Yes
Check this option if the model supports image understanding (multimodal), like llava.
Click "Save" to use the model in the application after verifying that there are no errors.
The integration method for Embedding models is similar to LLM, just change the model type to Text Embedding.
Use Ollama Models


Enter Prompt Eng. page of the App that needs to be configured, select the llava model under the Ollama provider, and use it after configuring the model parameters.
FAQ
⚠️ If you are using docker to deploy Dify and Ollama, you may encounter the following error:
Copy
httpconnectionpool(host=127.0.0.1, port=11434): max retries exceeded with url:/cpi/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8562812c20>: fail to establish a new connection:[Errno 111] Connection refused'))



httpconnectionpool(host=localhost, port=11434): max retries exceeded with url:/cpi/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8562812c20>: fail to establish a new connection:[Errno 111] Connection refused'))
This error occurs because the Ollama service is not accessible from the docker container. localhost usually refers to the container itself, not the host machine or other containers. To resolve this issue, you need to expose the Ollama service to the network.
Setting environment variables on Mac
If Ollama is run as a macOS application, environment variables should be set using launchctl:
For each environment variable, call launchctl setenv.
Copy
launchctl setenv OLLAMA_HOST "0.0.0.0"
Restart Ollama application.
Setting environment variables on Linux
If Ollama is run as a systemd service, environment variables should be set using systemctl:
Edit the systemd service by calling systemctl edit ollama.service. This will open an editor.
For each environment variable, add a line Environment under section [Service]:
Copy
[Service]


Environment="OLLAMA_HOST=0.0.0.0"
Save and exit.
Reload systemd and restart Ollama:
Copy
systemctl daemon-reload


systemctl restart ollama
Setting environment variables on Windows
On windows, Ollama inherits your user and system environment variables.
First Quit Ollama by clicking on it in the task bar
Edit system environment variables from the control panel
Edit or create New variable(s) for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.
Click OK/Apply to save
Run ollama from a new terminal window
How can I expose Ollama on my network?
Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the OLLAMA_HOST environment variable.
More Information
For more information on Ollama, please refer to:
Ollama
Ollama FAQ
Vector Database Migrate Tool
When you want to switch to another vector database, you can deactivate or delete the original vector database after switching.
How to use
Step:
If you are starting from local source code, modify the environment variable in the .env file to the vector database you want to migrate to.
For example:
Copy
VECTOR_STORE=qdrant
If you are starting from docker compose, modify the environment variable in the docker-compose.yaml file to the vector database you want to migrate to, both api and worker are all needed.
For example:
Copy
# The type of vector store to use. Supported values are `weaviate`, `qdrant`, `milvus`.

VECTOR_STORE: qdrant
run the below command in your terminal or docker container
Copy
flask vdb-migrarte
Connecting with Different Tools
Learn about the Different Tools Supported by Dify.
Dify supports various tools to enhance your application's capabilities. Each tool has unique features and parameters, so select a tool that suits your application's needs. Obtain the API key from the tool provider's official website before using it in Dify.
Tools Integration Guides
StableDiffusion: A tool for generating images based on text prompts.
SearXNG: A free internet metasearch engine which aggregates results from various search services and databases.
Stable Diffusion
Stable Diffusion is a tool for generating images based on text prompts, Dify has implemented the interface to access the Stable Diffusion WebUI API, so you can use it directly in Dify. followings are steps to integrate Stable Diffusion in Dify.
1. Make sure you have a machine with a GPU
Stable Diffusion requires a machine with a GPU to generate images. but it's not necessary, you can just use CPU to generate images, but it will be slow.
2. Launch Stable Diffusion WebUI
Launch the Stable Diffusion WebUI on your local machine or server.
2.1. Clone the Stable Diffusion WebUI repository
Clone the Stable Diffusion WebUI repository from the official repository
Copy
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
2.2. Launch it locally
After cloning the repository, you should change directory to the cloned repository and run the following command to launch the Stable Diffusion WebUI.
Windows
Copy
cd stable-diffusion-webui

./webui.bat --api --listen
Linux
Copy
cd stable-diffusion-webui

./webui.sh --api --listen
2.3. Prepare Models
Now you can access the Stable Diffusion WebUI on your browser according to the address shown in the terminal, but the models are not available yet. You need to download the models HuggingFace or other sources and put them in the models directory of the Stable Diffusion WebUI.
For example, we use pastel-mix as the model, use git lfs to download the model and put it in the models directory in stable-diffusion-webui.
Copy
git clone https://huggingface.co/JamesFlare/pastel-mix
2.4 Get Model Name
Now you can see pastel-mix in the model list, but we still need to get the model name, visit http://your_id:port/sdapi/v1/sd-models, you will see the model name like below.
Copy
[

   {

       "title": "pastel-mix/pastelmix-better-vae-fp32.ckpt [943a810f75]",

       "model_name": "pastel-mix_pastelmix-better-vae-fp32",

       "hash": "943a810f75",

       "sha256": "943a810f7538b32f9d81dc5adea3792c07219964c8a8734565931fcec90d762d",

       "filename": "/home/takatost/stable-diffusion-webui/models/Stable-diffusion/pastel-mix/pastelmix-better-vae-fp32.ckpt",

       "config": null

   },

]
The model_name is what we need, in this case, it's pastel-mix_pastelmix-better-vae-fp32.
3. Integrate Stable Diffusion in Dify
Fill in the Authentication and Model Configuration in Tools > StableDiffusion > To Authorize with the information you get from the previous steps.
4. Finish
Just try use it in Dify!
SearXNG
SearXNG is a free internet metasearch engine which aggregates results from various search services and databases. Users are neither tracked nor profiled. Dify has implemented the interface to access the SearXNG, so you can use it directly in Dify. followings are steps to integrate SearXNG in Dify.
1. Install SearXNG using Docker
Copy
docker run --rm \

            -d -p 8080:8080 \

            -v "${PWD}/searxng:/etc/searxng" \

            -e "BASE_URL=http://0.0.0.0:8080/" \

            -e "INSTANCE_NAME=searxng" \

            searxng/searxng
If you intend to install SearXNG using alternative methods. Please refer to this page.
2. Change settings.yml
When you install SearxNG, the default output format is the HTML format. You need to activate the json format. Add the following line to the settings.yml file. The settings.yml file is located at ${PWD}/searxng/settings.yml, as demonstrated in the previous example.
Copy
 # remove format to deny access, use lower case.

 # formats: [html, csv, json, rss]

 formats:

   - html

   - json    # <-- add this line
3. Integrate SearXNG in Dify
Fill in the base url http://x.x.x.x:8080 in Tools > SearXNG > To Authorize page to active it.
4. Finish
Have a fun!

Contributing
So you're looking to contribute to Dify - that's awesome, we can't wait to see what you do. As a startup with limited headcount and funding, we have grand ambitions to design the most intuitive workflow for building and managing LLM applications. Any help from the community counts, truly.
We need to be nimble and ship fast given where we are, but we also want to make sure that contributors like you get as smooth an experience at contributing as possible. We've assembled this contribution guide for that purpose, aiming at getting you familiarized with the codebase & how we work with contributors, so you could quickly jump to the fun part.
This guide, like Dify itself, is a constant work in progress. We highly appreciate your understanding if at times it lags behind the actual project, and welcome any feedback for us to improve.
In terms of licensing, please take a minute to read our short License and Contributor Agreement. The community also adheres to the code of conduct.
Before you jump in
Find an existing issue, or open a new one. We categorize issues into 2 types:
Feature requests:
If you're opening a new feature request, we'd like you to explain what the proposed feature achieves, and include as much context as possible. @perzeusss has made a solid Feature Request Copilot that helps you draft out your needs. Feel free to give it a try.
If you want to pick one up from the existing issues, simply drop a comment below it saying so.
A team member working in the related direction will be looped in. If all looks good, they will give the go-ahead for you to start coding. We ask that you hold off working on the feature until then, so none of your work goes to waste should we propose changes.
Depending on whichever area the proposed feature falls under, you might talk to different team members. Here's rundown of the areas each our team members are working on at the moment:
Member
Scope
@yeuoly
Architecting Agents
@jyong
RAG pipeline design
@GarfieldDai
Building workflow orchestrations
@iamjoel & @zxhlyh
Making our frontend a breeze to use
@guchenhe & @crazywoola
Developer experience, points of contact for anything
@takatost
Overall product direction and architecture

How we prioritize:
Feature Type
Priority
High-Priority Features as being labeled by a team member
High Priority
Popular feature requests from our community feedback board
Medium Priority
Non-core features and minor enhancements
Low Priority
Valuable but not immediate
Future-Feature

Anything else (e.g. bug report, performance optimization, typo correction):
Start coding right away.
How we prioritize:
Issue Type
Priority
Bugs in core functions (cannot login, applications not working, security loopholes)
Critical
Non-critical bugs, performance boosts
Medium Priority
Minor fixes (typos, confusing but working UI)
Low Priority

Installing
Here are the steps to set up Dify for development:
1. Fork this repository
2. Clone the repo
Clone the forked repository from your terminal:
Copy
git clone git@github.com:<github_username>/dify.git
3. Verify dependencies
Dify requires the following dependencies to build, make sure they're installed on your system:
Docker
Docker Compose
Node.js v18.x (LTS)
npm version 8.x.x or Yarn
Python version 3.10.x
4. Installations
Dify is composed of a backend and a frontend. Navigate to the backend directory by cd api/, then follow the Backend README to install it. In a separate terminal, navigate to the frontend directory by cd web/, then follow the Frontend README to install.
Check the installation FAQ for a list of common issues and steps to troubleshoot.
5. Visit dify in your browser
To validate your set up, head over to http://localhost:3000 (the default, or your self-configured URL and port) in your browser. You should now see Dify up and running.
Developing
If you are adding a model provider, this guide is for you.
To help you quickly navigate where your contribution fits, a brief, annotated outline of Dify's backend & frontend is as follows:
Backend
Dify’s backend is written in Python using Flask. It uses SQLAlchemy for ORM and Celery for task queueing. Authorization logic goes via Flask-login.
Copy
[api/]

├── constants             // Constant settings used throughout code base.

├── controllers           // API route definitions and request handling logic.           

├── core                  // Core application orchestration, model integrations, and tools.

├── docker                // Docker & containerization related configurations.

├── events                // Event handling and processing

├── extensions            // Extensions with 3rd party frameworks/platforms.

├── fields                // field definitions for serialization/marshalling.

├── libs                  // Reusable libraries and helpers.

├── migrations            // Scripts for database migration.

├── models                // Database models & schema definitions.

├── services              // Specifies business logic.

├── storage               // Private key storage.      

├── tasks                 // Handling of async tasks and background jobs.

└── tests
Frontend
The website is bootstrapped on Next.js boilerplate in Typescript and uses Tailwind CSS for styling. React-i18next is used for internationalization.
Copy
[web/]

├── app                   // layouts, pages, and components

│   ├── (commonLayout)    // common layout used throughout the app

│   ├── (shareLayout)     // layouts specifically shared across token-specific sessions 

│   ├── activate          // activate page

│   ├── components        // shared by pages and layouts

│   ├── install           // install page

│   ├── signin            // signin page

│   └── styles            // globally shared styles

├── assets                // Static assets

├── bin                   // scripts ran at build step

├── config                // adjustable settings and options 

├── context               // shared contexts used by different portions of the app

├── dictionaries          // Language-specific translate files 

├── docker                // container configurations

├── hooks                 // Reusable hooks

├── i18n                  // Internationalization configuration

├── models                // describes data models & shapes of API responses

├── public                // meta assets like favicon

├── service               // specifies shapes of API actions

├── test                  

├── types                 // descriptions of function params and return values

└── utils                 // Shared utility functions
Submitting your PR
At last, time to open a pull request (PR) to our repo. For major features, we first merge them into the deploy/dev branch for testing, before they go into the main branch. If you run into issues like merge conflicts or don't know how to open a pull request, check out GitHub's pull request tutorial.
And that's it! Once your PR is merged, you will be featured as a contributor in our README.
Getting Help
If you ever get stuck or got a burning question while contributing, simply shoot your queries our way via the related GitHub issue, or hop onto our Discord for a quick chat.

Support
If you still have questions or suggestions about using the product while reading this documentation, please try the following ways to seek support. Our team and community will do their best to help you.
Community Support
Please do not share your Dify account information or other sensitive information with the community. Our support staff will not ask for your account information.
Submit an Issue on GitHub
Join the Discord community
Email support@dify.ai
Contact Us
For matters other than product support.
Email hello@dify.ai

Open-Source License
Dify Open Source License
The Dify project is licensed under the Apache License 2.0, with the following additional conditions:
Dify is permitted to be used for commercialization, such as using Dify as a "backend-as-a-service" for your other applications, or delivering it to enterprises as an application development platform. However, when the following conditions are met, you must contact the producer to obtain a commercial license:
Multi-tenant SaaS service: Unless explicitly authorized by Dify in writing, you may not use the Dify.AI source code to operate a multi-tenant SaaS service that is similar to the Dify.AI service edition.
LOGO and copyright information: In the process of using Dify, you may not remove or modify the LOGO or copyright information in the Dify console.
Please contact business@dify.ai by email to inquire about licensing matters.
As a contributor, you should agree that your contributed code:
The producer can adjust the open-source agreement to be more strict or relaxed.
Can be used for commercial purposes, such as Dify's cloud business.
Apart from this, all other rights and restrictions follow the Apache License 2.0. If you need more detailed information, you can refer to the full version of Apache License 2.0.
The interactive design of this product is protected by appearance patent.
© 2023 LangGenius, Inc.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at
Copy
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

Data Security
Thank you for your interest in the Dify product. Dify takes your data security very seriously. Please refer to our [Privacy Policy].
What can be disclosed is that Dify's cloud service is located on US Azure, and only a very small number of authorized personnel can access user data after approval. In addition, our code is open source on GitHub. If you have concerns about the security of cloud services, you can use the self-deployed version.
As our product is still in its early stages, there may be some areas where we are not yet perfect, but we do plan to obtain SOC2 and ISO27001 certifications.
If you have any questions regarding commercialization, please contact business@dify.ai.
In the self-deployed version of Dify, there is only one instance where the Dify server is called - to check for updates via the API functionality. This must be triggered by an administrator in the backend. There are no other remote server technologies used, so you can use it safely.
If you still have concerns, you can protect your data through measures such as setting up firewalls.

